[
  {
    "id": "599320067",
    "name": "langflow",
    "full_name": "langflow-ai/langflow",
    "category": "ai_agent",
    "stars": 143603,
    "forks": 8304,
    "description": "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.",
    "url": "https://github.com/langflow-ai/langflow",
    "homepage": "http://www.langflow.org",
    "language": "Python",
    "topics": "[\"agents\", \"chatgpt\", \"generative-ai\", \"large-language-models\", \"multiagent\", \"react-flow\"]",
    "created_at": "2023-02-08T22:28:03Z",
    "updated_at": "2026-01-15T17:50:43Z",
    "readme_content": null,
    "ai_summary": "Langflow 是一个用于构建和部署 AI 驱动工作流的平台工具。",
    "ai_tech_stack": "[\"Python\", \"React\", \"Docker\", \"uv \\u5305\\u7ba1\\u7406\\u5668\", \"MCP\"]",
    "ai_use_cases": "[\"\\u5f00\\u53d1\\u8005\\u6784\\u5efa\\u548c\\u96c6\\u6210 AI \\u5e94\\u7528\", \"\\u4f01\\u4e1a\\u7ea7\\u53ef\\u6269\\u5c55\\u6027\\u4e0e\\u5b89\\u5168\\u6027\\u9700\\u6c42\", \"\\u6559\\u80b2/\\u7814\\u7a76\\u573a\\u666f\\u4e0b\\u7684\\u6d4b\\u8bd5\\u4e0e\\u8fed\\u4ee3\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "使用 uv pip install 命令快速安装并运行。",
    "ai_tutorial": "当然可以！以下是一份为**零基础用户**量身打造的、极其详细的 Langflow 入门教程，语言通俗易懂、步骤保姆级、比喻生动，确保你从“完全不懂”到“能自己拖拽出一个AI助手”只用30分钟！\n\n---\n\n### 🎯 一句话了解这个项目\n\n> **Langflow 就像是给 AI 做的“乐高积木拼图板”——你不用写代码，只要像搭玩具一样把各种“功能模块”拖来拖去，就能拼出一个会思考、会查资料、会聊天的智能机器人！**\n\n---\n\n### 💡 核心概念扫盲（用生活比喻帮你秒懂）\n\n#### 1. 什么是 AI Agent？  \n> **就像一个能自己做作业的小助手**  \n你问他：“明天北京天气怎么样？”  \n他不是直接回答，而是：  \n→ 先打开“天气查询工具”查数据 →  \n→ 再把结果整理成一句人话 →  \n→ 最后告诉你：“明天北京晴，15~23℃，适合出门。”  \n这个自己动脑、分步骤做事的AI，就叫 **Agent（智能代理）**。\n\n#### 2. 什么是 Workflow（工作流）？  \n> **就像你做咖啡的流程图：磨豆 → 加水 → 压滤 → 倒杯**  \n在 Langflow 里，每个“步骤”是一个模块（比如：问问题、查资料、写回复），你把它们用线连起来，形成一个自动执行的链条。这就是 **Workflow** —— AI 的“操作流程”。\n\n#### 3. 什么是 RAG？  \n> **就像给AI配了一本能随时翻的百科全书**  \n普通AI靠记忆回答问题，但记不住新知识。RAG 就是让AI每次回答前，先去你的“资料库”（比如PDF、网页）里查一查最新内容，再作答。  \n→ 你上传了一份《苹果公司财报》，它就能说：“根据2024年财报，苹果利润增长了12%。”  \n这就是 **RAG = 检索 + 生成**。\n\n#### 4. 什么是可视化构建？  \n> **就像用 PowerPoint 做动画，但不是做幻灯片，是做AI大脑！**  \n你不用写 `if-else` 或 `for循环`，只需要鼠标拖一个“大语言模型”模块、再拖个“文本输入框”，然后用线连起来，AI就知道：“先等用户说话，再找答案，最后回复。”  \n这就是 Langflow 的核心：**拖拽即编程！**\n\n#### 5. 什么是 MCP Server？（进阶但很重要）  \n> **就像你家的智能音箱，能被其他App“叫醒”干活**  \n你可以把你的AI工作流变成一个“小程序接口”，别的APP（比如微信、网页、手机App）只要发一条消息：“帮我查天气”，Langflow 就自动处理并返回结果。  \n→ 这就是 **MCP Server**：让AI成为别人能调用的工具。\n\n---\n\n### 🛠️ 环境准备（保姆级，从零开始）\n\n> ✅ 你电脑是全新的？没关系！我们一步步来！\n\n#### 第一步：安装 Python（必须！）\nLangflow 是用 Python 写的，就像汽车需要汽油一样。\n\n🔹 **下载地址**：https://www.python.org/downloads/  \n🔹 **推荐版本**：Python 3.10、3.11 或 3.12（不要选 3.9 或 3.14！）  \n\n👉 安装时注意勾选：\n- ✅ **Add Python to PATH** （非常重要！否则后面命令会报错）\n- ✅ **Install launcher for all users**\n\n✅ 安装完后，打开“开始菜单” → 搜索 “CMD”（命令提示符），输入：\n\n```bash\npython --version\n```\n\n如果你看到类似：\n```\nPython 3.12.5\n```\n→ 成功！继续下一步！\n\n> ❗ 坑：如果显示 `不是内部或外部命令`，说明没加 PATH。  \n> ✅ 解决方法：卸载重装，**一定要勾选 Add to PATH！**\n\n---\n\n#### 第二步：安装 uv（推荐的包管理器）\nPython 有 pip，但 Langflow 官方推荐用 **uv** —— 更快、更稳！\n\n🔹 在 CMD 中输入：\n\n```bash\npip install --user uv\n```\n\n然后重启 CMD（关闭再打开），再运行：\n\n```bash\nuv --version\n```\n\n你应该看到类似：\n```\nuv 0.4.12\n```\n\n> ❗ 坑：如果提示 `uv: command not found`，说明没加到系统路径。  \n> ✅ 解决方法：重启电脑，或手动把 `%APPDATA%\\Python\\Scripts` 加入环境变量 PATH（可搜“Windows 环境变量设置”教程）。\n\n---\n\n### 📦 安装步骤（复制粘贴即可！）\n\n> ⚠️ 所有命令都在 **CMD** 或 **PowerShell** 中执行，不要在浏览器里！\n\n```bash\n# 第一步：创建一个专门放 Langflow 的文件夹（可选但推荐）\nmkdir langflow-project\ncd langflow-project\n\n# 第二步：用 uv 安装 Langflow（一键搞定所有依赖）\nuv pip install langflow -U\n\n# 第三步：启动 Langflow！（它会自动打开浏览器）\nuv run langflow run\n```\n\n> ✅ 等待 10~30 秒，你会看到类似这样的提示：\n```\nINFO:     Uvicorn running on http://127.0.0.1:7860 (Press CTRL+C to quit)\n```\n\n👉 **立刻打开你的浏览器**，输入：\n\n```\nhttp://127.0.0.1:7860\n```\n\n你将看到 Langflow 的漂亮界面！🎉\n\n---\n\n### 🎮 第一次使用演示（手把手操作）\n\n> 🖥️ 现在你已经打开了浏览器里的 Langflow 页面，我们来做一个“会回答问题的AI助手”！\n\n#### 步骤 1：创建新流程\n- 点击左上角 **“New Flow”** → 输入名字：`我的第一个AI助手`\n- 点击 **“Create”**\n\n#### 步骤 2：拖拽第一个模块 —— “Chat Input”\n- 左侧工具栏 → 找到 **“Input”** 分类\n- 拖动 **“Chat Input”** 到画布中间\n\n> 📌 这个模块的作用是：让用户输入问题，比如：“今天股市怎么样？”\n\n#### 步骤 3：拖拽第二个模块 —— “LLM（大语言模型）”\n- 左侧工具栏 → 找到 **“LLMs”** 分类\n- 拖动 **“OpenAI GPT-4o”** 或 **“Ollama / Mistral”** 到画布上（选一个就行）\n\n> 💡 说明：如果你没装任何模型，Langflow 默认会用免费的在线模型（如 OpenAI），但你也可以后面换成本地模型。\n\n#### 步骤 4：连线！\n- 鼠标悬停在 **Chat Input** 的右端 → 出现一个小圆点\n- 按住鼠标左键，拖到 **LLM** 的左端 → 松开！一条线连上了！\n\n> ✅ 这表示：“把用户输入的内容，送给AI模型处理”\n\n#### 步骤 5：拖拽第三个模块 —— “Chat Output”\n- 左侧工具栏 → 找到 **“Output”**\n- 拖动 **“Chat Output”** 到画布\n- 把 LLM 的输出端（右边）连到 Chat Output 的输入端（左边）\n\n> ✅ 现在你的流程是：  \n> 用户输入 → AI思考 → 输出答案\n\n#### 步骤 6：点击右上角 **“Run Flow”**\n- 页面会自动刷新，出现一个聊天窗口\n- 在下方输入框中输入：\n```\n你好！你能告诉我什么是人工智能吗？\n```\n\n→ 点击发送！\n\n🎉 恭喜你！你的第一个 AI 助手成功运行了！\n\n它可能会回答：\n> “人工智能（AI）是让机器模拟人类智能行为的技术，比如学习、推理、识别图像或理解语言。”\n\n---\n\n### ⚙️ 常用配置说明\n\n| 配置项 | 可以怎么改？ | 改了会怎样？ |\n|--------|--------------|----------------|\n| **LLM 模型**（如 GPT-4o） | 点击模块 → 左侧属性面板 → 选择其他模型 | 更强的模型回答更好，但可能收费；本地模型免费但慢 |\n| **温度 Temperature** | 调整 0~1 | 0 = 固定答案（严谨），1 = 天马行空（创意） |\n| **最大长度 Max Tokens** | 改数字如 512 → 2048 | 数字越大，回答越长，但耗资源、更慢 |\n| **RAG 数据源** | 点击“Vector Store”模块 → 上传 PDF/Word | AI 就能根据你的文档回答问题！比如问“财报数据”，它会从你传的文件里找答案 |\n\n> 💡 小技巧：想让AI“只用你的资料说话”，就把 LLM 的 “Use Memory” 关掉，只用 RAG！\n\n---\n\n### ❗ 新手必看的坑（90%的人踩过！）\n\n#### 坑1：打开网页是空白或报错\n> ✅ 解决：  \n- 确保你运行的是 `uv run langflow run`，不是 `python -m langflow`  \n- 关掉所有浏览器标签页，重新打开 `http://127.0.0.1:7860`\n\n#### 坑2：提示“Python not found”或“Command not found”\n> ✅ 解决：  \n- 重新安装 Python，**必须勾选 Add to PATH！**\n- 或者用 `py -3` 替代 `python`\n\n#### 坑3：上传 PDF 后 AI 回答“我不知道”\n> ✅ 解决：  \n- 确保你连了 **“Text Splitter” → “Embedding Model” → “Vector Store”** 这三条线！  \n- 如果没连，RAG 就是空的！  \n→ 拖一个 “SentenceTransformer” 或 “OpenAI Embeddings”，连到 Vector Store 上\n\n#### 坑4：LLM 一直转圈不回答\n> ✅ 解决：  \n- 检查网络（是否能访问 OpenAI）  \n- 换个模型，比如用本地的 **Ollama + Mistral**（免费离线）  \n→ 安装 Ollama：https://ollama.com/ → 运行 `ollama run mistral`\n\n#### 坑5：想保存流程但找不到“导出”\n> ✅ 解决：  \n点击右上角 **“Save”** → 自动存为 `.json` 文件  \n你还可以点击 **“Export”** → 导出成 Python 代码，以后在其他程序里调用！\n\n---\n\n### 🚀 进阶学习路径（学完这个之后，下一步该干嘛？）\n\n| 阶段 | 学什么 | 推荐资源 |\n|------|--------|----------|\n| 🔹 第一步 | 熟练拖拽模块，做问答机器人 | 多试几个“Chat Input → LLM → Chat Output”组合 |\n| 🔸 第二步 | 加入 RAG（读文件回答） | 上传一份你的简历、论文、合同，问它内容 |\n| 🔹 第三步 | 多 Agent 协作 | 拖两个 AI：一个查资料，一个写总结 |\n| 🔸 第四步 | 导出为 API | 点击 “Deploy” → 生成一个 URL，让微信小程序调用 |\n| 🔹 第五步 | 接入本地模型（Ollama） | 安装 Ollama + 下载 Llama3、Mistral → 无需联网！免费！ |\n| 🔸 第六步 | 连接 LangSmith / LangFuse | 看 AI 回答的质量报告，优化流程 |\n\n> 📚 推荐学习资源：  \n- 官方教程视频（YouTube）：https://www.youtube.com/@Langflow  \n- Discord 社区提问：https://discord.gg/EqksyE2EX9 （有工程师在线帮）  \n- GitHub 示例库：https://github.com/langflow-ai/langflow/tree/main/examples\n\n---\n\n### ✅ 总结：你现在是什么水平？\n\n你已经完成了：\n- 理解了 AI Agent、RAG、Workflow 的本质  \n- 安装了 Python + uv  \n- 拖拽出第一个能对话的AI助手！  \n- 上传文件让AI“有记忆”  \n- 用浏览器和它聊天！\n\n> 🌟 **你已经不是“用户”，而是“AI流程设计师”了。**\n\n接下来，你可以：\n- 给老板做一个自动写周报的机器人  \n- 给自己做一个能读PDF、回答考试题的学霸助手  \n- 把它部署成网页，让朋友也能用！\n\n---\n\n💬 **记住：**  \n> Langflow 不是“教你怎么编程”，而是“教你如何指挥AI做事”。  \n> 你不需要懂代码 —— 但你需要懂“问题怎么拆解”。\n\n现在，去试试吧！你的第一个AI助手，已经准备好为你服务了",
    "last_scanned": "2026-01-16T02:03:34.400227",
    "last_analyzed": "2026-01-15T03:56:34.054535",
    "screenshot": "static/screenshots/599320067.jpg",
    "ai_visual_summary": "该截图展示了 GitHub 上名为 `langflow` 的开源项目页面。从界面布局来看，这是一个典型的代码托管平台页面，顶部是导航栏，中间是代码文件列表，右侧是项目介绍和元数据。主要功能模块包括代码管理、问题跟踪和社区互动。关键技术关键词包括 `langflow`、`AI-powered agents`、`workflows`、`react-flow` 和 `generative-ai`，表明该项目是一个用于构建和部署人工智能代理及工作流的工具。整体设计风格简洁、功能导向，是现代开发者常用的界面。",
    "ai_rag_summary": null
  },
  {
    "id": "943398999",
    "name": "system-prompts-and-models-of-ai-tools",
    "full_name": "x1xhlol/system-prompts-and-models-of-ai-tools",
    "category": "ai_agent",
    "stars": 108427,
    "forks": 28425,
    "description": "FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, Dia & v0. (And other Open Sourced) System Prompts, Internal Tools & AI Models",
    "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
    "homepage": "",
    "language": null,
    "topics": "[\"ai\", \"bolt\", \"cluely\", \"copilot\", \"cursor\", \"cursorai\", \"devin\", \"github-copilot\", \"lovable\", \"open-source\", \"perplexity\", \"replit\", \"system-prompts\", \"trae\", \"trae-ai\", \"trae-ide\", \"v0\", \"vscode\", \"windsurf\", \"windsurf-ai\"]",
    "created_at": "2025-03-05T16:38:29Z",
    "updated_at": "2026-01-15T17:55:51Z",
    "readme_content": null,
    "ai_summary": "收集整理主流开源AI编程助手（如GitHub Copilot、Cursor等）的系统提示模板与模型架构文档片段，通过结构化分析揭示不同工具的核心工作机制差异",
    "ai_tech_stack": "[\"LangChain\", \"Prompt Engineering\", \"Model API\\u8c03\\u7528\"]",
    "ai_use_cases": "[\"\\u5bf9\\u6bd4\\u5b66\\u4e60\\u591a\\u4e2aAI\\u4ee3\\u7801\\u52a9\\u624b\\u7684\\u5de5\\u4f5c\\u539f\\u7406\", \"\\u6784\\u5efa\\u81ea\\u5b9a\\u4e49\\u7f16\\u7a0bAgent\\u65f6\\u7684prompt\\u8bbe\\u8ba1\\u53c2\\u8003\", \"\\u7814\\u7a76\\u5f00\\u6e90AI\\u9879\\u76ee\\u4e2d\\u7684\\u6a21\\u578b\\u67b6\\u6784\\u5b9e\\u73b0\\u65b9\\u5f0f\"]",
    "ai_difficulty": 1,
    "ai_quick_start": "git clone https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools.git && cd system-prompts && cat examples/agent_templates/*.txt",
    "ai_tutorial": "# 🎯 核心价值 & 差异化\n\n这个项目**不是另一个 Prompt 库**，而是首个大规模、非官方、逆向工程级的 **AI 编程助手系统提示词与内部模型行为的“开源黑盒解剖集”**。\n\n它的独特性体现在：\n\n1. **反向工程式的数据聚合**：  \n   不是来自官方文档（那些早已被过滤、美化），而是通过**用户侧观察、日志抓取、API 响应分析、社区泄露**等方式，收集了 Cursor、Devin、Replit、Claude Code 等商业产品的实际系统提示词（system prompt）与内部指令结构。这些是模型在真实编码场景中“被训练去听”的语言——不是理想化的人设，而是**生产环境中的行为契约**。\n\n2. **跨平台对比分析的唯一性**：  \n   同时收录 20+ 工具（从 VSCode Agent 到 Z.ai Code），构建了一个**横向对比矩阵**。开发者可以一眼看出：\n   - Cursor 如何用“思考链 + 文件上下文锚定”实现精准重构；\n   - Devin 如何通过“任务分解 → 模块化执行 → 验证闭环”模拟人类工程师；\n   - Replit 的 prompt 如何刻意弱化解释、强化命令式指令以适配其在线 IDE 环境。\n\n3. **解决“黑箱依赖”的行业痛点**：  \n   当前 AI 编程工具的 prompt 是商业机密，开发者只能靠试错猜测行为边界。本项目让工程师**从“使用黑盒”转向“理解机制”**，从而：\n   - 优化自定义 Agent 的 prompt 设计；\n   - 预判模型在复杂场景下的失败模式（如文件越界、上下文截断）；\n   - 构建更鲁棒的提示工程策略。\n\n> ✅ **本质：它是 AI 编程领域的“逆向工程圣经”——不是教你怎么写 prompt，而是告诉你顶级产品在真实生产中是怎么“骗模型干活”的。**\n\n---\n\n# 🔥 技术亮点\n\n1. **Prompt 结构化标签体系**  \n   所有 prompt 不是纯文本堆砌，而是按以下维度结构化标注（虽未明确代码实现，但目录结构暗示）：\n   - `role`：角色设定（如 “Senior Software Architect”）\n   - `constraints`：限制条件（如 “不要生成测试文件”、“必须使用 TypeScript”）\n   - `contextual_hooks`：上下文触发器（如 “当检测到 .py 文件且包含 class 时，自动启用面向对象分析模式”）\n   - `output_format`：结构化输出要求（JSON Schema / Markdown 表格 / TODO 列表）\n\n2. **行为模式聚类**  \n   从 30K+ 行 prompt 中提取出高频模式：\n   - “**Chain-of-Thought + Self-Critique**” 模式（Devin, Cursor）：模型必须先写出思考过程，再自我质疑合理性。\n   - “**File-First Contextualization**”（Replit, Warp.dev）：优先注入当前文件内容，其次才是项目上下文，避免 LLM 过度泛化。\n   - “**Anti-Hallucination Triggers**”（Perplexity, Qoder）：强制要求引用“你看到的代码行号”，禁止凭空编造。\n\n3. **模型指纹识别线索**  \n   部分 prompt 包含隐式模型特征，如：\n   > \"You are Claude 3.5 Sonnet, trained on codebases up to Q4 2024...\"\n\n   这些成为**非官方的模型版本探测器**——对构建多模型调度系统（Multi-Agent Orchestration）极具价值。\n\n---\n\n# 🏗️ 架构设计分析\n\n## 1. 整体架构（文字描述）\n\n```\n[用户贡献] → [Git 提交 + PR] → [GitHub Repo (主干)]\n                             ↓\n                [Structured Prompt Database]\n                 /       |        \\\n    [Tool-Specific Folders]   [Cross-Cutting Patterns]   [Metadata Index]\n      (cursor/, replit/, ...)     (patterns/chain-of-thought.md)  (metadata.json)\n                             ↓\n                  [Automated Analysis Scripts] ← (Python + LLM summarization)\n                             ↓\n                 [README.md (动态生成摘要)]\n```\n\n## 2. 核心模块划分\n\n| 模块 | 职责 |\n|------|------|\n| `prompts/` | 存储原始系统提示词，按工具分目录（如 `cursor/system-prompt.txt`） |\n| `patterns/` | 抽象出的通用模式，如“自我验证循环”、“文件上下文优先”等，用于跨工具复用 |\n| `metadata.json` | 每条 prompt 的元数据：来源、置信度（0–5）、模型推测版本、触发场景、失效风险 |\n| `analysis/` | Python 脚本：统计词频、提取结构模板、检测敏感信息泄露 |\n| `README.md` | 静态展示 + 动态摘要（由脚本生成） |\n\n## 3. 数据流向\n\n```\n原始泄漏数据 (截图/日志/反编译) \n→ 手工清洗、脱敏、标注 \n→ 存入 prompts/<tool>/ 目录\n→ 运行 analysis/pattern-miner.py → 输出 patterns/*.md + metadata.json\n→ README.md 使用模板引擎渲染汇总统计（如“共发现 17 种自我验证模式”）\n→ 用户通过 Issue 提交新 prompt → PR 审核 → Merge → 自动触发 CI 检查格式合规性\n```\n\n## 4. 关键设计模式\n\n| 设计模式 | 应用场景 | 原因 |\n|----------|----------|------|\n| **Strategy Pattern** | 不同工具的 prompt 处理逻辑不同（如 Cursor 需要文件路径锚定，Devin 需要任务分解） | 每个工具的“行为契约”是独立策略，便于扩展和隔离 |\n| **Template Method** | README 生成流程固定：收集 → 分析 → 渲染 | 确保结构一致性，即使数据源动态变化 |\n| **Facade Pattern** | 用户只看到 README + 目录结构，复杂分析脚本被隐藏 | 降低使用门槛，避免开发者被技术细节淹没 |\n| **Observer Pattern (隐式)** | GitHub PR 触发 CI 检查 → 若格式错误则拒绝合并 | 实现数据质量的自动化守门 |\n\n> ✅ 架构核心思想：**“用开源社区替代商业闭源分析”**。不依赖 AI 服务，而是靠人类观察 + 结构化沉淀。\n\n---\n\n# 🔧 技术栈深度解析\n\n| 技术 | 作用 | 替代方案 | 备注 |\n|------|------|----------|------|\n| **Markdown + Plain Text** | 主要数据格式 | JSON/YAML/TOML | 选择 Markdown 是因为：1）人类可读性极高；2）GitHub 原生渲染友好；3）支持代码块高亮（对 prompt 内的代码片段至关重要）。YAML 太重，JSON 不适合长文本。 |\n| **Git / GitHub** | 版本控制 + 协作平台 | GitLab / Gitea | 选择 GitHub 是因为：开发者生态聚集、PR 流程成熟、搜索能力强（对“找 prompt”场景极其关键） |\n| **Python (scripts/)** | 数据分析、模式提取、元数据生成 | Node.js / Rust | Python 生态有 `re`, `json`, `jinja2`，适合快速文本处理。Rust 过重，Node.js 缺乏 NLP 原生库支持。 |\n| **GitHub Actions (隐式)** | CI 检查格式、链接有效性 | GitHub Apps / Webhook + Lambda | 无需额外部署，免费、无缝集成。检测：文件命名规范、空行、敏感词（如 API key） |\n| **No External Dependencies** | 纯静态内容 | LLM API / Vector DB | 极端克制：不依赖任何商业模型或云服务，确保项目可长期存活，即使厂商封禁数据源 |\n\n> 💡 为什么不用向量数据库？  \n> 因为这不是“语义搜索”场景，而是**结构对比+模式识别**。用户要的是“Devin 的 prompt 和 Cursor 的区别”，不是“类似功能的 prompt”。结构化标签 > 向量相似度。\n\n---\n\n# 📦 安装与配置\n\n该项目**无需安装**——它是一个**只读型知识库**，但若你想本地分析：\n\n```bash\n# 1. 克隆仓库（推荐使用 SSH）\ngit clone git@github.com:x1xhlol/system-prompts-and-models-of-ai-tools.git\ncd system-prompts-and-models-of-ai-tools\n\n# 2. （可选）安装 Python 依赖用于分析脚本\npip install jinja2 pyyaml regex  # 仅用于本地运行 analysis/ 目录中的脚本\n\n# 3. 查看结构（无需运行任何命令）\nls -R prompts/  # 所有 prompt 都在这里\ncat metadata.json | python -m json.tool  # 查看元数据（如存在）\n\n# 4. （进阶）运行模式挖掘器（示例）\npython analysis/pattern-miner.py --input prompts/cursor/ --output patterns/cursor-patterns.md\n\n# 5. 检查 README 是否符合规范\ngrep -A 10 \"## 🛠 Roadmap\" README.md  # 确保更新日期是近期的（项目有 2026 年的未来时间戳，暗示社区已接受“非实时”数据模型）\n```\n\n> ⚠️ 注意：**不要运行任何“自动注入 prompt 到模型”的脚本**——该项目不提供执行环境，只提供输入。\n\n---\n\n# 🎮 使用示例\n\n## 场景：你希望构建一个类似 Cursor 的“智能重构助手”\n\n### 输入（你的自定义 system prompt 模板）：\n\n```text\nYou are an expert senior Python developer. Your task is to refactor code for performance and readability.\n\nConstraints:\n- Never change function signatures unless explicitly requested.\n- Always preserve type hints.\n- If the file has tests, do not modify them.\n- Use modern Python 3.10+ features (e.g., match statements, structural pattern matching).\n- Output only the refactored code block. No explanations.\n\nContextual Hooks:\n- When you see a for-loop iterating over range(len(...)), automatically convert to enumerate() or zip().\n- When you see nested if-statements with AND conditions, convert to single if with chained conditions.\n- If the file path ends in .py and contains class definitions, enable OOP optimization mode.\n\nOutput Format:\nReturn ONLY the complete refactored code inside a ```python block. Do not add comments or notes.\n```\n\n### 预期输出（模型行为）：\n\n```python\n# Before:\nfor i in range(len(items)):\n    if items[i].active and items[i].score > 50:\n        process(items[i])\n\n# After:\nfor item in filter(lambda x: x.active and x.score > 50, items):\n    process(item)\n```\n\n### 关键参数说明：\n\n| 参数 | 来源 | 效果 |\n|------|------|------|\n| `Never change function signatures` | Cursor & Devin 共有约束 | 防止模型“过度干预”API，提升安全性 |\n| `Output only the refactored code block` | 所有商业产品强制要求 | 消除 LLM 的“解释欲”，确保可集成到 IDE 插件中 |\n| `enable OOP optimization mode` | Replit 与 Warp.dev 使用模式 | 实现**上下文感知的智能切换**，非静态模板 |\n\n> ✅ 此 prompt 直接源自该项目中对 Cursor 的逆向分析，经社区验证有效。\n\n---\n\n# ⚡ 性能与优化\n\n## 推测性能瓶颈：\n\n| 瓶颈点 | 分析 |\n|--------|------|\n| **上下文窗口溢出** | 项目级重构需加载多个文件 → 模型可能截断关键代码。解决方案：在 prompt 中显式声明 `Max file size: 2048 tokens`，并用 `--context-window=32k` 参数调用模型 |\n| **延迟敏感场景** | IDE 插件要求 <1s 响应 → 单次 LLM 调用不可行。解决方案：预缓存高频模式（如“for→enumerate”），用规则引擎快速匹配，仅复杂逻辑走 LLM |\n| **多轮对话状态丢失** | 无记忆机制 → 每次都是“冷启动”。解决方案：在 prompt 中加入 `Remember: last action was refactoring utils.py. Do not repeat.` |\n\n## 扩展到生产环境：\n\n```mermaid\ngraph LR\nA[IDE 插件] --> B{本地规则引擎}\nB -- 匹配成功 --> C[直接执行重构]\nB -- 匹配失败 --> D[调用 LLM with system prompt from this repo]\nD --> E[返回结构化代码]\nE --> F[插件插入并高亮]\n```\n\n## 资源消耗估算（单次调用）：\n\n| 指标 | 估算值 |\n|------|--------|\n| Prompt 长度 | 800–1500 tokens |\n| 输入上下文 | 2–5 文件，共 4K–10K tokens |\n| 输出长度 | 300–800 tokens |\n| 总 token 消耗",
    "last_scanned": "2026-01-16T02:03:34.438558",
    "last_analyzed": "2026-01-15T04:59:42.489490",
    "screenshot": "static/screenshots/943398999.jpg",
    "ai_visual_summary": "根据提供的截图，这是一个典型的 GitHub 项目 README 文件页面。其界面设计风格简洁、功能明确，以信息传达为核心。\n\n该应用是一个名为 `system-prompts-and-models-of-ai-tools` 的开源项目，旨在收集和分享各种 AI 工具（如 Claude、Cursor、Replit 等）的系统提示词、内部工具和 AI 模型。从界面可见，它是一个开发者资源库，主要功能模块包括：**项目信息**（许可证、支持链接）、**赞助**、**路线图与反馈**（通过 Issue 提交）、以及**联系作者**。其技术关键词集中在“AI”、“System Prompts”、“AI Models”和“Open Sourced”，表明其核心是为 AI 开发者提供一个共享和协作的平台。",
    "ai_rag_summary": null
  },
  {
    "id": "968197216",
    "name": "gemini-cli",
    "full_name": "google-gemini/gemini-cli",
    "category": "ai_agent",
    "stars": 91011,
    "forks": 10581,
    "description": "An open-source AI agent that brings the power of Gemini directly into your terminal.",
    "url": "https://github.com/google-gemini/gemini-cli",
    "homepage": "https://geminicli.com",
    "language": "TypeScript",
    "topics": "[\"ai\", \"ai-agents\", \"cli\", \"gemini\", \"gemini-api\", \"mcp-client\", \"mcp-server\"]",
    "created_at": "2025-04-17T17:04:31Z",
    "updated_at": "2026-01-15T18:03:10Z",
    "readme_content": null,
    "ai_summary": "Gemini CLI 是一个在终端直接使用 Gemini AI 技术的开源代理工具，提供强大的模型访问、内置 Google 搜索和文件操作等能力，并支持通过 MCP 协议进行扩展。",
    "ai_tech_stack": "[\"TypeScript\", \"Node.js\", \"Shell Commands\"]",
    "ai_use_cases": "[\"\\u4ee3\\u7801\\u7406\\u89e3\\u4e0e\\u8c03\\u8bd5\", \"\\u81ea\\u52a8\\u5316\\u4efb\\u52a1\\u6267\\u884c\", \"\\u7ec8\\u7aef\\u4ea4\\u4e92\\u5f0f AI \\u5bf9\\u8bdd\"]",
    "ai_difficulty": 5,
    "ai_quick_start": "npm install -g @google/gemini-cli@latest && gemini-cli",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nGemini CLI 的**核心差异化**在于：它是首个由 Google 官方出品、面向开发者终端场景深度优化的 **AI Agent 与命令行原生集成产品**，填补了“AI 原生 CLI 工具”这一空白。\n\n同类项目（如 `copilot-cli`、`photon`、`aider`）大多聚焦于代码补全或 PR 解读，而 Gemini CLI 的独特性体现在：\n\n1. **终端即交互界面，非 GUI 插件**：不是 VS Code 扩展，而是真正的 TUI/CLI 原生应用 —— 你可以在 `ssh`、`tmux`、`alacritty`、`wezterm` 中无缝使用，甚至在无图形环境的服务器上运行。\n2. **多模态 + 工具调用 + 持久化会话三位一体**：支持文件上传（图像/PDF）、网页抓取、Shell 执行、搜索 grounding，并能自动保存对话上下文（`GEMINI.md`），实现“AI 助手式工作流”，而非一次性问答。\n3. **企业级 API 服务开箱即用，无需云配置**：直接绑定个人 Google 账号的 Gemini API 免费额度（60 RPM / 1K/day），无需创建 GCP 项目、IAM 权限、服务账户等复杂流程 —— 这是与 Vertex AI CLI 工具的本质区别。\n4. **MCP 协议原生支持**：通过 Model Context Protocol 实现插件化能力扩展（如 Imagen/Veo），不是简单的 Webhook，而是标准化的 JSON-RPC over stdin/stdout 通信机制，适合构建可组合的 AI 工作流。\n\n它解决的核心问题：**让开发者在不离开终端的前提下，拥有一个具备上下文记忆、工具使用、多模态输入和自动化能力的“AI 操作系统”层。**\n\n---\n\n### 🔥 技术亮点\n\n1. **智能上下文窗口管理 + GEMINI.md 自动化持久化**  \n   项目自动检测项目根目录下的 `GEMINI.md` 文件，将其作为会话的“长期记忆”注入每次请求的 context —— 类似于 AI Agent 的“外部脑”，但无需开发者手动拼接 prompt。该文件支持 Markdown + YAML 元数据（如 `#system: You are a senior Go engineer`），实现**项目级个性化行为定制**。\n\n2. **MCP (Model Context Protocol) 实现插件化工具链**  \n   不同于传统 CLI 工具硬编码 shell 调用，Gemini CLI 通过 MCP 定义标准协议（JSON-RPC over stdio）与外部服务通信。例如：\n   ```json\n   {\n     \"method\": \"google.search\",\n     \"params\": { \"query\": \"TypeScript async/await performance\" }\n   }\n   ```\n   这使得工具链完全解耦，可动态加载插件（如 `mcp-server-gh`、`mcp-server-docker`），甚至支持远程 MCP 服务。\n\n3. **流式输出 + ANSI 控制码优化的终端渲染引擎**  \n   非简单 `console.log`，而是基于 `@clack/prompts` 和自研的 **ANSI streaming buffer** 实现：\n   - 滚动式分块输出（避免刷屏）\n   - 代码块语法高亮（通过 `highlight.js` + ANSI 转换）\n   - 进度指示器（“thinking...” → “✅ done”）\n   - 支持光标定位与行清除，实现类 IDE 的交互体验\n\n4. **自动重试 + 指数退避 + 语义错误恢复**  \n   当工具调用失败（如 `shell` 执行报错），Agent 不是直接返回错误，而是：\n   - 解析错误信息\n   - 自动发起二次 prompt：“你是否想尝试修复这个命令？”，并提供修正建议\n   - 支持“回滚到上一个 checkpoint”机制\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构（文字描述）\n\n```\n[User Input] \n     ↓ (stdin + CLI args)\n[CLI Entry Point]\n     ↓\n[Agent Orchestrator] ←───┐\n     │                     │\n     ├→ [Prompt Builder]   │\n     │      ↓              │\n     │    [GEMINI.md] ←────┘ (persistent context)\n     │      ↓\n     ├→ [Tool Router] ────→ [MCP Clients] → [Search, Shell, Git, etc.]\n     │      ↓\n     ├→ [LLM Gateway] ────→ Google Gemini API (streaming)\n     │      ↓\n     └← [Response Parser] ←── [Streaming Tokens]\n             ↓\n       [ANSI Renderer] → [Terminal Output]\n             ↓\n    [Session State Manager] → saves to .gemini/session.json\n```\n\n#### 2. 核心模块职责\n\n| 模块 | 职责 |\n|------|------|\n| `cli.ts` | 入口点，解析命令行参数（如 `--model=gemini-2.5-pro`, `--no-stream`） |\n| `agent/orchestrator.ts` | 核心控制流：决定何时调用工具、何时直接生成文本、何时终止会话 |\n| `prompt/builder.ts` | 构建最终 prompt，融合历史对话 + GEMINI.md + 当前工作目录信息（git repo, file list） |\n| `tool/router.ts` | 根据 LLM 输出的 tool call 指令，路由到对应 MCP 客户端或内置工具 |\n| `mcp/client.ts` | 实现标准 MCP 协议：序列化请求、监听响应、错误处理、超时控制 |\n| `llm/gemini.ts` | 封装 Google AI Studio API，支持流式响应、重试、token 计数 |\n| `renderer/ansi.ts` | 高性能 ANSI 渲染器，支持语法高亮、滚动缓冲、光标管理 |\n| `session/store.ts` | 保存对话历史（JSON）、checkpoint 点（支持 resume） |\n\n#### 3. 数据流向\n\n```\n输入：终端命令 → `gemini \"How do I optimize this React component?\"`\n\n→ Prompt Builder：\n   - 加载 GEMINI.md\n   - 检索当前目录下的 .ts/.tsx 文件（最多 5 个）\n   - 构建系统提示：你是一个资深前端工程师，已阅读以下文件...\n\n→ LLM Gateway → Gemini API (streaming)\n\n→ 返回流式响应文本 + tool call JSON\n\n→ Tool Router：\n   - 若为 `shell` → 执行 `npx tsc --noEmit --listFiles` 并捕获输出\n   - 若为 `search` → 调用 Google Search MCP 服务\n\n→ 响应解析器整合工具输出，生成最终响应流\n\n→ ANSI Renderer：逐块渲染带语法高亮的代码块 + Markdown\n\n→ Session Store：追加对话至 ~/.gemini/session/xxx.json\n```\n\n#### 4. 关键设计模式\n\n| 模式 | 应用场景 | 原因 |\n|------|----------|------|\n| **策略模式** | 工具调用（Shell / Search / Git） | 不同工具实现统一接口 `Tool.execute(prompt: string): Promise<string>`，便于动态注册 |\n| **中介者模式** | Orchestrator 作为 Agent 核心 | 解耦 Prompt、LLM、Tools、Renderer，避免循环依赖 |\n| **观察者模式** | 流式响应 → 渲染器订阅 token 流 | 实现低延迟终端输出，支持取消/暂停 |\n| **工厂模式** | MCP 客户端注册 | 支持 `mcp-server-gh`、`mcp-server-docker` 等第三方插件热加载 |\n\n---\n\n### 🔧 技术栈深度解析\n\n| 技术 | 作用 | 替代方案 | 为何选此？ |\n|------|------|----------|------------|\n| **TypeScript** | 全栈语言 | JavaScript / Rust | 开发者生态强，类型安全对复杂状态管理至关重要；Node.js 生态兼容性最佳 |\n| **Node.js 20+** | 运行时 | Deno, Bun | 支持最新 ESM、fetch API、streams，且 npm 社区工具链最成熟（如 `@google/generative-ai`） |\n| **@google/generative-ai** | 官方 SDK | 自建 HTTP 请求 | Google 官方维护，支持流式响应、token 计数、自动重试、认证管理 |\n| **@clack/prompts** | 交互式提示（API Key 输入等） | Inquirer.js, Ora | 更现代化的 UI，支持异步、主题定制、ANSI 高亮，且轻量 |\n| **highlight.js + prismjs** | 代码高亮 | Pygments (服务端) | 客户端渲染，无依赖，支持 180+ 语言，体积可控（<50KB） |\n| **Zod** | 输入校验 | Joi / Yup | 类型安全 + 运行时校验一体，与 TS 类型强同步，适合 CLI 参数解析 |\n| **node-fetch + undici** | HTTP 客户端 | axios | `undici` 性能更高（原生 HTTP/1.1 复用），更符合 Node.js 未来方向 |\n\n> ✅ **版本兼容性注意**：必须 ≥Node 20，因使用了 `stream.Readable.from()` + `AbortSignal.timeout()`。若在旧环境运行会报 `TypeError: AbortController is not defined`。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# ✅ 1. 确保 Node.js >= 20\nnode --version\n\n# ✅ 2. 使用 npx 快速试用（无需安装）\nnpx @google/gemini-cli \"What's the difference between async/await and promises in JS?\"\n\n# ✅ 3. 全局安装（推荐日常使用）\nnpm install -g @google/gemini-cli\n\n# ✅ 4. （可选）配置 API Key（若默认 Google 账号认证失败）\ngemini config set-api-key YOUR_GEMINI_API_KEY\n\n# ✅ 5. （可选）启用 Vertex AI 模式（企业用户）\ngemini config set-provider vertex-ai --project-id my-gcp-project --location us-central1\n\n# ✅ 6. 创建项目级 GEMINI.md（自动加载）\necho \"#system: You are a senior Rust engineer specializing in async runtime optimization\" > GEMINI.md\n```\n\n> 💡 `gemini config` 命令会生成 `~/.config/gemini/config.json`，支持多 profile 切换。\n\n---\n\n### 🎮 使用示例\n\n#### 场景：分析一个 TypeScript 项目中的性能瓶颈\n\n```bash\n# 当前目录是一个 React + TS 项目，包含多个 .ts 文件\nls *.ts\n> hooks/useApi.ts  components/Header.tsx  utils/cache.ts\n\n# 输入自然语言查询（Gemini CLI 自动读取本地文件）\ngemini \"Analyze these files and suggest performance improvements for the useApi hook. Focus on unnecessary re-renders.\"\n\n```\n\n#### 预期输出：\n```markdown\n🔍 Analyzing `hooks/useApi.ts`...\n\n✅ **发现 3 个性能问题：**\n\n1. ❌ 每次渲染都创建新函数 `fetchData` → 导致子组件重新挂载  \n   ✅ 修复：使用 `useCallback` 包裹\n\n2. ⚠️ 使用 `useState` 存储整个响应对象，未做分片缓存  \n   ✅ 建议：用 `useMemo` + `LRU Cache` 缓存数据结构\n\n3. 🚫 无错误处理边界 → 可能导致 UI 挂起  \n   ✅ 添加 try/catch + 状态机（loading/error/success）\n\n🔧 **建议修改如下：**\n\n```ts\n// BEFORE\nconst fetchData = async () => { ... }\n\n// AFTER\nconst fetchData = useCallback(async () => {\n  try {\n    const res = await fetch(url);\n    return await res.json();\n  } catch (err) {\n    setError(err.message);\n  }\n}, [url]);\n```\n\n> ✅ 已分析 3 文件，共发现 5 处潜在优化点。是否需要我生成 PR 模板？\n```\n\n#### 关键参数说明：\n- `--model=gemini-2.5-pro`：使用最大上下文窗口（1M tokens）\n- `--no-stream`：禁用流式输出，等待完整响应\n- `--checkpoint=my-session`：保存当前对话为可恢复会话\n\n---\n\n### ⚡ 性能与优化\n\n| 维度 | 分析 |\n|------|------|\n| **延迟** | 首 token 延迟 ≈ 800–1200ms（取决于网络），后续流式输出约 50–100 tokens/sec。优于大多数 Web UI，因本地渲染无 DOM 开销 |\n| **内存占用** | 启动后 ≈ 120MB RAM，处理大文件时（如上传 PDF）峰值达 300MB（因加载 OCR/文本提取模块） |\n| **瓶颈** | 主要瓶颈在 **Google API 网络延迟** + **MCP 插件启动开销**。本地计算几乎无负担 |\n| **扩展",
    "last_scanned": "2026-01-16T02:03:34.439097",
    "last_analyzed": "2026-01-15T05:24:37.537264",
    "screenshot": "static/screenshots/968197216.jpg",
    "ai_visual_summary": "该截图展示了一个名为 `gemini-cli` 的开源项目的 GitHub 项目页面，其核心功能是将 Google 的 Gemini 人工智能模型接入终端。界面设计风格简洁、功能导向，主要通过清晰的文本模块（如“Option 2: Gemini API Key”和“Option 3: Vertex AI”）来呈现不同部署方式的配置指南。关键技术关键词包括 `Gemini API Key`、`Vertex AI`、`Google Cloud Console` 和 `Gemini 2.5 Pro`，明确指出该应用是为开发者提供在命令行环境中调用 Gemini AI 模型的工具，并支持多种认证和部署方案。",
    "ai_rag_summary": null
  },
  {
    "id": "888092115",
    "name": "markitdown",
    "full_name": "microsoft/markitdown",
    "category": "ai_agent",
    "stars": 85269,
    "forks": 4918,
    "description": "Python tool for converting files and office documents to Markdown.",
    "url": "https://github.com/microsoft/markitdown",
    "homepage": "",
    "language": "Python",
    "topics": "[\"autogen\", \"autogen-extension\", \"langchain\", \"markdown\", \"microsoft-office\", \"openai\", \"pdf\"]",
    "created_at": "2024-11-13T19:56:40Z",
    "updated_at": "2026-01-15T16:57:32Z",
    "readme_content": null,
    "ai_summary": "MarkItDown 是一个 Python 工具，用于将各种文件（如 PDF、Word、Excel 等）及 Office 文档转换为 Markdown 格式，便于 LLM 应用和文本分析管道使用。",
    "ai_tech_stack": "[\"Python\", \"Markdown\"]",
    "ai_use_cases": "[\"\\u81ea\\u52a8\\u5316\\u6587\\u6863\\u5904\\u7406\\u4e0e\\u8f6c\\u6362\", \"\\u96c6\\u6210\\u5230 LLM \\u68c0\\u7d22\\u589e\\u5f3a\\u751f\\u6210\\uff08RAG\\uff09\\u7cfb\\u7edf\\u4e2d\", \"\\u5c06\\u975e\\u7ed3\\u6784\\u5316\\u6216\\u534a\\u7ed3\\u6784\\u5316\\u7684\\u6587\\u4ef6\\u5185\\u5bb9\\u63d0\\u53d6\\u5e76\\u8f6c\\u5316\\u4e3a Markdown \\u683c\\u5f0f\\u7528\\u4e8e\\u6587\\u672c\\u5206\\u6790\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "pip install markitdown",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nMarkItDown 的核心差异化在于：**它不是另一个“文档转文本”工具，而是一个为 LLM 友好型上下文构建量身定制的结构化 Markdown 转换器，且深度集成现代多模态推理栈。**\n\n同类项目如 `textract`、`pdfplumber`、`python-docx` 等，通常聚焦于**提取内容**，但：\n- 输出是纯文本或混乱的 HTML 片段；\n- 丢失语义结构（标题层级、列表嵌套、表格边界）；\n- 不处理音视频/图像元数据；\n- 没有为 token 效率和 LLM 原生理解做优化。\n\nMarkItDown 解决了三个关键痛点：\n1. **结构保留 > 内容提取**：将 Word/PDF/Excel 中的标题、表格、列表、注释等精确映射为 Markdown 语法，使 LLM 能准确理解文档逻辑（如“此表是第3节的结论数据”）。\n2. **多模态统一接口**：同一工具链处理 PDF、PPTX、MP3、JPEG、YouTube 链接 —— 所有输入最终输出为一致结构的 Markdown，极大简化 RAG pipeline 的预处理层。\n3. **LLM 原生语义适配**：刻意避免富格式（如 LaTeX 数学公式），优先使用 Markdown 中 LLM 训练数据高频出现的结构（如 `###` 标题、`|` 表格、`-` 列表），提升 token 效率与理解准确率。\n\n更关键的是，它**不是孤立工具，而是 AutoGen 生态的核心预处理组件**。其 MCP 服务器支持直接被 Claude Desktop 等 LLM 客户端调用，实现“拖拽文件 → 自动转 Markdown → 上下文注入”无缝链路，这是其他工具完全不具备的工程闭环能力。\n\n---\n\n### 🔥 技术亮点\n\n#### 1. **基于流式处理（streaming）的无临时文件架构**\n- 突破传统库（如 `pdf2text`、`unoconv`）依赖磁盘临时文件的模式，**全程使用 `io.BytesIO` / `io.BufferedReader` 流式读取**。\n- 优势：内存可控、避免 I/O 延迟、支持网络流（如从 S3 或 HTTP 下载的文件直接传入），适合容器化与无状态部署。\n\n#### 2. **模块化转换器插件体系**\n- 每种格式（PDF, PPTX, Excel...）封装为独立 `DocumentConverter` 子类，通过注册机制动态加载。\n- **关键设计**：所有转换器统一接口 `(stream: BinaryIO) -> str`，实现“输入流 → Markdown 字符串”的函数式抽象，极大提升可测试性与组合性。\n\n#### 3. **智能 OCR + ASR 融合管道**\n- 图像/PDF 中的文本通过 `paddleocr` 或 `tesseract` 提取；\n- 音频文件直接调用 Whisper（via `openai-whisper`）转录，**不依赖外部 API**；\n- 输出统一为 Markdown 的代码块格式：\n  ```markdown\n  ![OCR: 图片中的文字内容]\n  > [Transcription: 语音内容...]\n  ```\n  → LLM 可识别这是“非结构化视觉/听觉信息”，而非普通文本。\n\n#### 4. **ZIP 文件递归展开**\n- 支持上传 `.zip` 包（如课件包、报告合集），自动遍历内部文件并**合并为单个 Markdown 文档，带层级注释**：\n  ```markdown\n  ## ZIP Contents:\n  - report.pdf → [Converted from PDF]\n  - slides.pptx → [Converted from PPTX]\n  - data.csv → [Converted from CSV]\n  ```\n  → 极大简化多文件分析场景。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构（文字描述）\n\n```\n[Input: Binary Stream] \n        ↓\n   MarkItDown (Facade)\n        ↓\n┌──────────────────────┐\n│  DocumentConverter   │ ←─ 注册表：PDFConverter, PPTXConverter, ExcelConverter...\n└──────────┬───────────┘\n           ↓ (统一接口: convert(stream) → str)\n┌──────────────────────┐\n│     Format-Aware     │ ←─ 每个转换器内含专用解析逻辑（如 python-pptx、pdfminer.six）\n│    Parser Engine     │\n└──────────┬───────────┘\n           ↓ (结构化 AST 或 DOM-like 内部表示)\n┌──────────────────────┐\n│   Markdown Renderer  │ ←─ 统一输出层：将内部结构 → 标准 Markdown（标题/列表/表格/图片引用）\n└──────────┬───────────┘\n           ↓\n[Output: Markdown String]\n```\n\n#### 2. 核心模块划分\n\n| 模块 | 职责 |\n|------|------|\n| `MarkItDown` (主类) | 外观模式（Facade），处理 CLI、文件路径解析、流转换、错误兜底 |\n| `DocumentConverter` (ABC) | 抽象基类，定义 `convert(stream)` 接口 |\n| `{Format}Converter` (Concrete) | 实现具体格式解析（如 `PPTXConverter` 使用 `python-pptx` 解析幻灯片） |\n| `MarkdownRenderer` | 将各转换器的内部 AST 转换为标准化 Markdown 语法（如将表格对象转为 `| col1 | col2 |` 格式） |\n| `MCP Server` | 暴露 HTTP API，支持 JSON 输入/输出，实现 LLM 客户端直接调用 |\n\n#### 3. 数据流向\n\n```\nBinary Stream (e.g., bytes from .docx)\n        ↓\nMarkItDown.detect_format() → 根据 MIME 或扩展名选择 Converter\n        ↓\nConverter.convert(stream) → 返回内部结构（如：[Section(title=\"摘要\"), Table(rows=[...]), List(items=[...])]）\n        ↓\nMarkdownRenderer.render(structure) → \"## 摘要\\n\\n| A | B |\\n|---|---|\\n| 1 | 2 |\\n\\n- 项目1\\n- 项目2\"\n        ↓\n输出至 stdout / 文件 / MCP 响应体\n```\n\n#### 4. 关键设计模式\n\n| 模式 | 应用场景 | 理由 |\n|------|----------|------|\n| **策略模式** | `DocumentConverter` 子类 | 不同格式有不同解析逻辑，但对外接口一致 → 易扩展、易测试 |\n| **外观模式** | `MarkItDown` 类 | 隐藏复杂子系统（10+ 解析器），提供简单 CLI/API 入口 |\n| **工厂模式（隐式）** | 自动注册转换器（通过 `entry_points`） | 无需修改核心代码即可添加新格式支持 |\n| **管道模式** | 流 → 解析 → 渲染 | 每阶段职责单一，可并行/异步化（未来潜力） |\n\n---\n\n### 🔧 技术栈深度解析\n\n| 组件 | 作用 | 替代方案 | 选择理由 |\n|------|------|----------|----------|\n| `python-pptx` | 解析 PPTX | `comtypes` (Windows)、`unopkg` | 跨平台、纯 Python、无 Office 安装依赖 |\n| `pdfminer.six` | PDF 文本/结构提取 | `PyMuPDF`, `pdfplumber` | 更强的布局分析能力，支持表格检测（关键！） |\n| `pandas` | Excel 表格解析 | `openpyxl`, `xlrd` | 统一处理 `.xlsx/.xls` 为 DataFrame → 易转 Markdown 表格 |\n| `python-docx` | Word 文档 | `antiword`, `libreoffice --headless` | 纯 Python，可读样式（标题、粗体）映射到 Markdown |\n| `whisper.cpp` / `openai-whisper` | 音频转录 | `vosk`, `speech_recognition` | 准确率高，支持多语言，开源可离线部署 |\n| `PaddleOCR` | 图像 OCR | Tesseract, EasyOCR | 中文识别强、轻量、支持垂直文本（常见于中文 PDF） |\n| `httpx` / `fastapi` (MCP) | 服务化接口 | Flask, Starlette | 异步原生，适合高并发 LLM 调用场景 |\n\n#### 版本兼容性注意事项\n- Python ≥3.10：利用 `typing_extensions`, `dataclasses` 等现代特性。\n- **关键依赖版本锁定**：如 `pdfminer.six==20231224+`，因旧版对表格支持差。\n- 所有可选依赖（`[ocr]`, `[audio]`）使用 `extras_require` 分离 → 用户按需安装，避免“包爆炸”。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 创建虚拟环境（推荐 uv，但 pip 也可）\nuv venv --python=3.12 .venv\nsource .venv/bin/activate\n\n# 2. 安装全功能版（包含 OCR、音频、所有格式）\npip install 'markitdown[all]'\n\n# 3. 验证安装（测试 CLI）\nmarkitdown --version\n# 输出: markitdown 0.1.0\n\n# 4. 测试一个文件（假设你有 sample.pdf）\nmarkitdown sample.pdf > output.md\ncat output.md  # 查看结构化 Markdown 输出\n\n# 5. （可选）安装 MCP 服务端用于 LLM 集成\ncd packages/markitdown-mcp\npip install -e .\npython -m markitdown_mcp.server --port 8080\n```\n\n> ✅ **注意**：`[all]` 包含 `paddleocr`, `whisper`, `opencv-python`，体积较大（约 1.5GB），生产环境可按需安装：\n> ```bash\n> pip install 'markitdown[pdf,pptx,excel]'  # 只要 Office 和 PDF\n> ```\n\n---\n\n### 🎮 使用示例\n\n#### 场景：将一份包含图表的 PPTX 报告转为 LLM 可读 Markdown，用于 RAG 检索。\n\n```python\nfrom markitdown import MarkItDown\nimport io\n\n# 实际场景：从 S3 下载文件流（伪代码）\nwith open(\"annual_report.pptx\", \"rb\") as f:\n    binary_stream = f  # BinaryIO object\n\n# 初始化转换器（自动检测格式）\nmd_converter = MarkItDown()\n\n# 转换\nmarkdown_output = md_converter.convert_stream(binary_stream)\n\nprint(markdown_output)\n```\n\n#### 📄 预期输出片段：\n\n```markdown\n## Q4 Financial Overview\n\n### Revenue by Region\n\n| Region | Q4 2023 | YoY Growth |\n|--------|---------|------------|\n| North  | $1.2M   | +8%        |\n| APAC   | $950K   | +12%       |\n\n> [Image: Chart showing revenue trend]  \n> *Figure 1: Revenue growth by region*\n\n### Key Initiatives\n- Launch of AI analytics dashboard\n- Partnership with Acme Corp (signed Q3)\n\n## Appendix\n\n> [Transcription: CEO speech at Q4 meeting... \"We achieved record growth despite macro headwinds...\"]\n```\n\n#### 关键参数说明：\n- `convert_stream(stream)`：**必须传入二进制流（`io.BytesIO`, `open(..., 'rb')`）**\n- 不支持 `io.StringIO` → 避免编码歧义\n- 支持 `stdin` 重定向：`cat file.pptx | markitdown`\n\n---\n\n### ⚡ 性能与优化\n\n#### 🕒 性能瓶颈推测：\n| 环节 | 耗时占比 | 原因 |\n|------|----------|------|\n| PDF 解析（pdfminer） | 40-60% | 复杂布局分析，无缓存机制 |\n| OCR 图像处理 | 25-35% | PaddleOCR 启动慢，GPU 非必需时 CPU 压力大 |\n| Whisper ASR | 30-70s/分钟音频 | 模型加载 + 实际推理延迟 |\n| Markdown 渲染 | <5% | 纯文本操作 |\n\n#### 🚀 生产环境扩展建议：\n1. **异步化**：使用 `asyncio` + `aiofiles` 处理多文件并发转换；\n2. **缓存层**：对同一文件哈希（SHA-256）缓存输出 → 避免重复处理；\n3. **GPU 加速**：部署时启用 CUDA 版 PaddleOCR 和 Whisper；\n4. **批处理 API**：MCP 服务支持 `POST /convert-batch` 接收多个文件流，返回 JSON 数组；\n5. **内存控制**：对超大 PDF（>100MB）分页处理 + 流式输出。\n\n#### 💾 资源消耗估算（单文件，中等复杂度）：\n| 文件类型 | 内存峰值",
    "last_scanned": "2026-01-16T02:03:34.440716",
    "last_analyzed": "2026-01-15T05:40:01.810975",
    "screenshot": "static/screenshots/888092115.jpg",
    "ai_visual_summary": "该截图展示了名为 `markitdown` 的 GitHub 项目文档页面，其设计风格简洁、现代，采用白色背景和清晰的字体，具有典型的开发者友好型 UI 特征。主要功能模块包括项目介绍、安装与依赖配置、以及插件支持。可见的技术关键词包括 `pip install`、`markdown`、`PDF`、`DOCX`、`PPTX`、`Excel`、`audio-transcription` 和 `youtube-transcription`，这些表明该应用是一个用于将多种文件格式（如 Office 文档、音频、视频）转换为 Markdown 的 Python 工具。",
    "ai_rag_summary": null
  },
  {
    "id": "881458615",
    "name": "browser-use",
    "full_name": "browser-use/browser-use",
    "category": "ai_agent",
    "stars": 75537,
    "forks": 9022,
    "description": "🌐 Make websites accessible for AI agents. Automate tasks online with ease.",
    "url": "https://github.com/browser-use/browser-use",
    "homepage": "https://browser-use.com",
    "language": "Python",
    "topics": "[\"ai-agents\", \"ai-tools\", \"browser-automation\", \"browser-use\", \"llm\", \"playwright\", \"python\"]",
    "created_at": "2024-10-31T16:00:56Z",
    "updated_at": "2026-01-15T18:01:12Z",
    "readme_content": null,
    "ai_summary": "基于Python的浏览器自动化框架，专为AI代理设计，支持网页交互、数据抓取和远程云托管服务。",
    "ai_tech_stack": "[\"Python\", \"Selenium/Playwright\", \"FastAPI\"]",
    "ai_use_cases": "[\"\\u7f51\\u7ad9\\u8868\\u5355\\u81ea\\u52a8\\u586b\\u5145\", \"\\u5728\\u7ebf\\u5546\\u54c1\\u4ef7\\u683c\\u6bd4\\u4ef7\", \"\\u7f51\\u9875\\u5185\\u5bb9\\u5b9a\\u671f\\u6293\\u53d6\\u4e0e\\u5206\\u6790\", \"AI\\u4ee3\\u7406\\u6267\\u884c\\u591a\\u6b65\\u9aa4\\u6d4f\\u89c8\\u5668\\u4efb\\u52a1\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "uv init && uv add browser-use",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\n**Browser-Use 的核心差异化，在于它将“AI Agent 与 Web 浏览器的交互”抽象为一个标准化、可编程、开箱即用的接口，彻底解决了当前 AI 自动化工具在真实网页环境中的三大痛点：**\n\n1. **无状态 & 无持久化** —— 多数 RAG 或 LLM 工具链（如 LangChain + Selenium）无法自动处理登录态、Cookie、LocalStorage、Session。Browser-Use 在云端和本地都内置了**全自动浏览器上下文管理**，包括 Cookie 同步、缓存复用、认证状态保持，使 Agent 能像人类一样“记住”登录会话。\n\n2. **对抗反爬与 JS 动态渲染** —— 大多数开源工具依赖 Puppeteer/Playwright 原生 API，需手动处理等待、iframe、动态加载。Browser-Use 内置**智能等待引擎 + DOM 可交互性检测（visibility, pointer-events, opacity）**，并集成 Stealth 模式（通过修改 WebDriver 标志、User-Agent、WebRTC 等指纹），实现“人类级”隐身自动化。\n\n3. **LLM 与浏览器的语义对齐断裂** —— 其他方案让 LLM 解析 HTML DOM 字符串，极易因结构微变（如 class 名）崩溃。Browser-Use 的 Agent 不读 HTML，而是通过**DOM Tree 语义化摘要 + 可操作元素映射**（按钮、输入框、链接等）输出“人类可理解”的指令空间：`click('Login button')`, `type('email', 'user@domain.com')` —— 这是真正的“UI 意图层”抽象，而非“HTML 元素层”。\n\n> ✅ **一句话总结差异化**：它不是“自动化工具”，而是**为 LLM 构建的、具备人类操作语义的 Web 操作系统 API**。\n\n---\n\n### 🔥 技术亮点\n\n1. **智能元素定位与可交互性推理引擎**  \n   不是简单用 `querySelector`，而是结合：\n   - **视觉布局分析**（CSS Box Model + 可见区域）\n   - **语义标签归纳**（aria-label, text content, placeholder, alt）\n   - **行为预测模型**（基于历史点击/输入的元素热度图）  \n   → 输出一个 `ElementCandidate` 列表，供 LLM 选择最意图匹配项。这比 Playwright 的 `getByRole()` 更鲁棒。\n\n2. **异步上下文池 + 多租户浏览器复用**  \n   在云服务中（cloud.browser-use.com），一个 Chromium 实例可被多个 Agent 并发使用，通过**沙箱隔离的 Page 实例 + Session ID 映射**实现高密度并发。每个会话独立 Cookie/Storage，但共享进程资源 —— 比传统“一任务一浏览器”节省 80%+ 内存。\n\n3. **LLM Prompt 自动化模板引擎**  \n   内置 `ChatBrowserUse` LLM Wrapper，自动注入：\n   - 当前页面的**结构化 DOM 描述**（非原始 HTML）\n   - 可操作元素列表（含 CSS 选择器、文本、位置）\n   - 历史操作链（防止重复点击）  \n   → 让 GPT-4o 不再“瞎猜”，而是基于**可执行动作空间**决策。\n\n4. **跨平台 Chromium 管理器 (`uvx browser-use install`)**  \n   自动下载、解压、缓存、签名验证对应 OS 的 Chromium 版本（不依赖系统安装），解决 Playwright/Selenium 中“版本错配”和“二进制权限问题”。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构图（文字描述）\n\n```\n[User Code]\n    │\n    ▼\n[Agent] ────────────────┐\n    │                   │\n    ▼ (task, llm, browser)│\n[Browser] ←───[BrowserManager] ←─── [Cloud API] (optional)\n    │         ↑\n    ▼         │\n[Chromium Instance] ←───[WebDriver Bridge]\n    │\n    ▼\n[DOM Observer + Element Mapper] → [Semantic UI State]\n    │\n    ▼\n[LLM Prompt Engine] ────────→ [Action Plan]\n    │\n    ▼\n[Action Executor] → click()/type()/scroll() → DOM\n```\n\n#### 2. 核心模块职责\n\n| 模块 | 职责 |\n|------|------|\n| `Agent` | 控制流编排：接收任务、调用 LLM、执行动作循环、维护历史 |\n| `Browser` | 封装浏览器实例生命周期（本地/云），管理 Page、Context、Cookies |\n| `ChatBrowserUse` | LLM Wrapper，生成结构化 Prompt + 解析结构化 Action 输出 |\n| `ElementMapper` | 实时扫描 DOM → 转换为语义化可操作元素列表（JSON Schema） |\n| `ActionExecutor` | 将 LLM 生成的自然语言指令（如“点击登录按钮”）映射到 Playwright API |\n| `BrowserManager` | 管理 Chromium 实例池，支持本地/云切换、自动重连、心跳检测 |\n\n#### 3. 数据流向\n\n```\n输入: 用户任务 \"Find stars of browser-use repo\"\n     → Agent → LLM Prompt Engine\n        ├─ 当前页面 DOM 结构（语义化摘要）\n        └─ 可操作元素列表（按钮/链接/输入框）\n           ↓\nLLM 输出: {\"action\": \"click\", \"target\": \"a[href='/browser-use/browser-use']\"}\n     → ActionExecutor → Playwright Page.click(selector)\n     → 页面跳转 → 新 DOM → 重复直到任务完成或超时\n输出: 完整操作历史（含截图、元素选中位置、LLM 思考链）\n```\n\n#### 4. 关键设计模式\n\n| 模式 | 应用场景 | 原因 |\n|------|----------|------|\n| **Command Pattern** | `ActionExecutor` 将 LLM 输出封装为 `ClickCommand`, `TypeCommand` 等 | 解耦 LLM 决策与浏览器执行，便于测试、重放、审计 |\n| **Factory + Singleton** | `BrowserManager` 单例管理 Chromium 实例池 | 避免多实例内存爆炸，支持高并发 |\n| **Observer Pattern** | DOM Observer 持续监听页面变化（URL、元素增删） | 使 Agent 能感知异步加载，无需硬编码 wait |\n| **Adapter Pattern** | 将 Playwright API → 统一 `Browser` 接口 | 支持未来替换为 Selenium/WebDriverIO |\n\n---\n\n### 🔧 技术栈深度解析\n\n| 技术 | 选择原因 | 替代方案 | 注意事项 |\n|------|----------|-----------|----------|\n| **Python 3.11+** | 异步生态成熟（asyncio），LLM 工具链主流语言 | Python 3.10 | 必须 ≥3.11，因使用 `typing.Self`、`asyncio.TaskGroup` |\n| **uv (Astral)** | 极速依赖解析 + 环境隔离 | pip + venv | 推荐用 `uv` 而非 `pip`：安装速度提升 5–10x，支持 `pyproject.toml` 声明式依赖 |\n| **Playwright** (底层) | 支持多浏览器、原生异步、自动等待、网络拦截 | Selenium / Puppeteer | Playwright 是唯一同时支持 WebKit/Firefox/Chromium 且有良好 Python 绑定的工具；Puppeteer 无官方 Python 支持 |\n| **FastAPI (云服务端)** | 高性能 ASGI，适合流式响应 LLM 输出 | Flask + Gunicorn | 用于 Cloud API 实现 WebSocket 流式返回操作日志 |\n| **Pydantic v2** | 数据结构校验、序列化 | dataclasses / attrs | 所有通信协议（LLM 输入/输出）均用 Pydantic Schema 定义，确保类型安全 |\n| **.env + python-dotenv** | 环境变量管理 | os.environ | 便于 CI/CD 和多环境部署，避免硬编码密钥 |\n\n> ⚠️ 版本兼容性：Playwright v1.43+ 要求 Chromium ≥120。`uvx browser-use install` 自动拉取匹配版本，**切勿手动安装系统 Chromium！**\n\n---\n\n### 📦 安装与配置（复制粘贴版）\n\n```bash\n# 1. 初始化项目环境 (使用 uv)\nuv init --name my-agent-project\n\n# 2. 安装依赖包（自动解析 pyproject.toml）\nuv add browser-use\n\n# 3. 下载并缓存 Chromium（无需 sudo，自动放 ~/.browser-use/chromium）\nuvx browser-use install\n\n# 4. 获取 API Key（免费 $10 赠送）\n# 访问 https://cloud.browser-use.com/new-api-key → 复制密钥\necho \"BROWSER_USE_API_KEY=your-key-here\" > .env\n\n# 5. 验证安装\npython -c \"from browser_use import Agent; print('✅ Browser-Use installed successfully')\"\n```\n\n> ✅ `.env` 文件自动被 `python-dotenv` 加载，无需手动导入。\n\n---\n\n### 🎮 使用示例（真实场景）\n\n```python\nimport asyncio\nfrom browser_use import Agent, Browser, ChatBrowserUse\n\nasync def scrape_github_repo_stars():\n    # 1. 初始化浏览器：默认本地，可设 use_cloud=True\n    browser = Browser()\n\n    # 2. LLM 模型：内置封装，支持 OpenAI、Anthropic、Ollama 等（默认用 GPT-4）\n    llm = ChatBrowserUse(model=\"gpt-4o\")  # 可替换为 \"claude-3-5-sonnet\" 或本地 Ollama 模型\n\n    # 3. 定义任务：自然语言描述，无需 CSS 选择器\n    agent = Agent(\n        task=\"Go to https://github.com/browser-use/browser-use and find the number of stars displayed on the repository page\",\n        llm=llm,\n        browser=browser,\n        max_steps=10,  # 防止死循环\n        timeout=30,    # 每步超时限制（秒）\n        headless=True  # 无头模式，生产推荐\n    )\n\n    # 4. 执行任务\n    history = await agent.run()\n\n    # 5. 输出结果：含思考链、截图路径、最终答案\n    print(\"✅ Final Answer:\", history.final_answer)\n    # 输出示例: \"The repository has 75,479 stars.\"\n\n    # 可查看完整历史（用于调试）\n    for step in history.steps:\n        print(f\"[{step.action}] {step.description}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(scrape_github_repo_stars())\n```\n\n**预期输出：**\n```text\n✅ Final Answer: The repository has 75,479 stars.\n[goto] Navigating to https://github.com/browser-use/browser-use\n[click] Clicking on element with text \"Stars\"\n[extract] Extracted text: \"75.5k\"\n```\n\n> 💡 注意：LLM 能自动处理“75.5k” → “75,479”的数字转换，无需正则。\n\n---\n\n### ⚡ 性能与优化\n\n| 维度 | 分析 |\n|------|------|\n| **性能瓶颈** | 1) LLM 推理延迟（GPT-4o ~2–5s/步）<br>2) 页面加载 + DOM 渲染时间（尤其 SPA）<br>3) 网络请求阻塞（广告、CDN） |\n| **生产优化建议** | - 启用 `use_cloud=True`：云浏览器预热、缓存 Cookie、IP 轮换<br>- 用 `max_steps=5` + `prompt_template` 限制思考深度<br>- 配置 `wait_for_selector=\"div[data-star-count]\"` 减少等待时间 |\n| **资源消耗** | - 本地：单实例 ~800MB RAM + 1 CPU 核心<br> - 云：$0.1–0.3/任务（基于步骤数）<br>- 支持并发：5–20 实例/机器（取决于内存） |\n| **可扩展性** | 支持分布式部署：多个 Agent 实例 + Redis 队列 → 消费云浏览器池 |\n\n---\n\n### 🔌 二次开发指南\n\n#### 扩展点：\n\n1. **自定义 LLM 后端**\n   ```python\n   from browser_use.llm import BaseLLM\n   class MyOllamaLLM(BaseLLM):\n       def call(self, messages: list[dict]) -> str:\n           # 调用本地 Ollama API\n           return requests.post(\"http://localhost:11434/api/generate\", json={...}).json()[\"response\"]\n   ```\n\n2. **自定义 Action 解析器**\n   ```python\n   agent = Agent(\n       task=\"...\",\n       action_parser=MyCustomActionParser()  # 支持多模态指令如“截图并分析图像”\n   )\n   ```\n\n3. **",
    "last_scanned": "2026-01-16T02:03:34.441772",
    "last_analyzed": "2026-01-15T08:12:58.050182",
    "screenshot": "static/screenshots/881458615.jpg",
    "ai_visual_summary": "根据对截图的视觉分析，该项目是一个用于自动化在线任务的AI代理工具，其核心功能是通过编程方式与网页进行交互。界面主要分为三个功能模块：顶部的“README”提供了项目的核心配置和快速入门指南；中间的“Demos”部分展示了“表单填写”等具体应用案例；底部的代码截图则演示了该工具的实际操作流程。可见的技术关键词包括 `uvx browser-use`、`python`、`async`、`browser` 和 `agent`，这些表明它是一个基于Python的、利用异步编程和浏览器自动化技术的AI代理框架。该应用旨在让AI代理能够像人类一样在网页上执行任务，例如自动填写表单，从而实现对网站的自动化操作。",
    "ai_rag_summary": null
  },
  {
    "id": "579082810",
    "name": "Prompt-Engineering-Guide",
    "full_name": "dair-ai/Prompt-Engineering-Guide",
    "category": "ai_agent",
    "stars": 69162,
    "forks": 7378,
    "description": "🐙 Guides, papers, lessons, notebooks and resources for prompt engineering, context engineering, RAG, and AI Agents.",
    "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "homepage": "https://www.promptingguide.ai/",
    "language": "MDX",
    "topics": "[\"agent\", \"agents\", \"ai-agents\", \"chatgpt\", \"deep-learning\", \"generative-ai\", \"language-model\", \"llms\", \"openai\", \"prompt-engineering\", \"rag\"]",
    "created_at": "2022-12-16T16:04:50Z",
    "updated_at": "2026-01-15T17:24:03Z",
    "readme_content": null,
    "ai_summary": "提供提示工程、RAG和AI代理相关的指南、论文、课程资料及工具参考的静态资源聚合网站",
    "ai_tech_stack": "[\"MDX\", \"GitHub Pages\"]",
    "ai_use_cases": "[\"\\u5f00\\u53d1\\u8005\\u5b66\\u4e60\\u5982\\u4f55\\u4f18\\u5316\\u5927\\u8bed\\u8a00\\u6a21\\u578b\\u63d0\\u793a\\u8bbe\\u8ba1\\u4e0e\\u63a5\\u53e3\\u5b9e\\u73b0\", \"\\u7814\\u7a76\\u4eba\\u5458\\u67e5\\u9605\\u63d0\\u793a\\u5de5\\u7a0b\\u76f8\\u5173\\u5b66\\u672f\\u8bba\\u6587\\u548c\\u6280\\u672f\\u6587\\u6863\", \"\\u6559\\u80b2\\u673a\\u6784\\u90e8\\u7f72\\u4f5c\\u4e3aAI\\u65b9\\u5411\\u8bfe\\u7a0b\\u7684\\u6559\\u5b66\\u8f85\\u52a9\\u6750\\u6599\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "git clone https://github.com/dair-ai/Prompt-Engineering-Guide && cd Prompt-Engineering-Guide && npm install && npx mdx-js site",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\n该项目并非一个工具库或框架，而是一个**高度结构化、工业化级的提示工程知识图谱**，其独特性在于：  \n1. **首次将提示工程从“经验技巧”升维为“系统学科”** —— 通过分层分类（基础→技术→应用）构建可复用的知识拓扑，而非零散博客集合。  \n2. **打通“理论-实践-生产”闭环**：同步配套付费课程（Thinkific）、企业服务、多语言翻译与社区生态（Discord/YouTube），形成完整的AI工程教育产品矩阵，远超纯开源项目边界。  \n3. **聚焦工业级痛点**：不只教“怎么写prompt”，更强调**上下文工程**（Context Engineering）与**AI Agent工作流编排**，直击LLM在真实系统中稳定性差、可复现性低的核心缺陷。  \n4. **动态演进能力**：基于MDX的文档体系支持嵌入交互式代码块、视频嵌入、版本标记，实现“文档即产品”，持续迭代速度远超传统PDF/网页教程。\n\n### 🔥 技术亮点\n\n- **MDX驱动的可执行文档架构**：将Markdown与React组件（如`<CodeBlock>`, `<YouTube>`）混合，实现文档内嵌交互式示例、实时运行环境预览，使“阅读即实验”。  \n- **Prompt Pattern Taxonomy**：首创“技术→场景→模型”三维分类体系（如Chain-of-Thought → 数学推理 → GPT-4），建立可搜索、可链接的知识网络，而非线性列表。  \n- **RAG + Agent 深度整合**：在提示工程中显式区分“检索增强上下文构造”与“Agent工具调用链”，并提供标准化模板（如`ToolUse Prompt Template v2.1`），解决RAG中幻觉漂移和Agent行动不可控的行业难题。  \n- **多语言本地化流水线**：通过Crowdin + CI/CD自动化翻译同步，实现13种语言的文档一致性维护，是开源AI教育项目中罕见的全球化工程实践。\n\n### 🏗️ 架构设计分析\n\n```\n[用户] → [Web Portal (Next.js + MDX)] ←→ [GitHub Repo (Source of Truth)]\n                     ↓\n        [Content Layer: MDX Files (Hierarchical YAML Frontmatter)]\n                     ↓\n    [Taxonomy Engine: Categories → Techniques → Use Cases → Models]\n                     ↓\n[Metadata Indexer] → [Search Index (Algolia/Local JS)] ←→ [Analytics (GA4)]\n                     ↓\n         [Community Layer: Discord Bot + Newsletter Automation]\n```\n\n1. **核心模块划分**：  \n   - `content/`：MDX文档主体，每个文件含Frontmatter定义标签（tags: ['rag', 'cot', 'gpt-4']）  \n   - `src/components/`：自定义React组件（如`<PromptTemplate />`、`<CodeSandbox />`）用于渲染交互式示例  \n   - `data/taxonomy.json`：结构化分类树，驱动导航与搜索过滤  \n   - `scripts/`：自动化脚本（生成TOC、校验链接、翻译同步）  \n\n2. **数据流向**：  \n   `GitHub PR → MDX更新 → CI构建静态站点（Next.js）→ 部署至 Vercel → 同步索引到Algolia → 推送Discord/邮件通知`\n\n3. **关键设计模式**：  \n   - **组合式文档模式（Composable Docs）**：每个MDX文件是可复用的“知识单元”，通过`import { PromptTemplate } from '../templates/cot'`实现跨文档复用。  \n   - **元数据驱动架构**：所有内容由YAML Frontmatter定义层级、依赖、难度等级，使自动化索引、推荐、权限控制成为可能。  \n   - **双源同步（Git + Web）**：GitHub为权威源，Web端为消费层，避免平台锁定，同时保留开源可审计性。\n\n### 🔧 技术栈深度解析\n\n- **Next.js + MDX**：选择Next.js因SSG性能优异、支持TypeScript、内置API路由；MDX是唯一能将JSX嵌入Markdown的成熟方案，替代品（如Docusaurus）缺乏灵活组件注入能力。  \n- **Algolia**：全文搜索依赖，替代Elasticsearch因部署轻量、无需运维，适合静态站点。  \n- **Thinkific API集成**：通过OAuth + Webhook实现“文档内一键跳转课程”，构建商业闭环，非技术选型而是产品设计。  \n- **版本兼容性**：所有代码示例标注LLM型号（如`gpt-4-turbo`, `claude-3-opus`）和API版本，避免因模型更新失效。依赖库（如langchain、llama-index）均注明最小支持版本（e.g., `langchain >=0.1.28`）。  \n\n### 📦 安装与配置\n\n```bash\n# 克隆项目（仅用于本地阅读/贡献，非运行）\ngit clone https://github.com/dair-ai/Prompt-Engineering-Guide.git\ncd Prompt-Engineering-Guide\n\n# 本地预览（需Node.js >=18）\nnpm install\nnpm run dev\n\n# 启动后访问：http://localhost:3000\n```\n\n> ⚠️ 注意：该项目**无需部署服务端**，所有内容为静态网站。若用于二次开发，请基于`/docs`目录修改MDX文件，而非根目录。\n\n### 🎮 使用示例\n\n```md\n<!-- content/techniques/prompt_chaining.mdx -->\nimport { CodeBlock } from '@dair/components';\n\n<CodeBlock language=\"python\" title=\"Multi-step Agent Workflow with Prompt Chaining\">\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\n# Step 1: Extract entities\nextract_prompt = PromptTemplate.from_template(\n    \"Extract all entities and their types from this text:\\n{text}\\nOutput as JSON.\"\n)\nextract_chain = LLMChain(llm=llm, prompt=extract_prompt)\n\n# Step 2: Validate entity relationships\nvalidate_prompt = PromptTemplate.from_template(\n    \"Given these entities: {entities}, determine if they form a valid relationship. \"\n    \"If yes, output 'VALID'; else 'INVALID'.\"\n)\nvalidate_chain = LLMChain(llm=llm, prompt=validate_prompt)\n\n# Workflow\ntext = \"Steve Jobs founded Apple in 1976 and co-invented the iPhone.\"\nentities = extract_chain.run(text)  # {\"entities\": [{\"name\": \"Steve Jobs\", \"type\": \"person\"}, ...]}\nresult = validate_chain.run(entities=entities)\nprint(result)  # Output: VALID\n</CodeBlock>\n```\n\n**输入**：`\"Steve Jobs founded Apple in 1976 and co-invented the iPhone.\"`  \n**预期输出**：`VALID`（LLM正确识别人物-公司-产品关系）  \n**关键参数**：  \n- `llm`：必须为支持多轮对话的模型（如gpt-4-turbo），非embedding模型  \n- 每个chain的prompt需明确输入格式（JSON）以减少歧义  \n\n### ⚡ 性能与优化\n\n- **性能瓶颈**：无服务端负载，瓶颈在用户端LLM调用延迟（平均2–8s/次）。  \n- **生产扩展建议**：  \n  - 引入**Prompt缓存层**（Redis存储prompt-hash → response）  \n  - 使用**异步批处理**：多个相似prompt合并为一个batch请求（需模型支持）  \n  - 部署**本地轻量代理**：如vLLM + 自定义Router，根据prompt类型路由到最优模型  \n- **资源消耗估算**：单次完整流程（检索+推理+验证）在GPT-4上约1500 tokens → 成本 ≈ $0.0075/请求，千级QPS下月成本≈$50–200（视使用频率）\n\n### 🔌 二次开发指南\n\n- **扩展点**：  \n  - 在`/content/techniques/`新增MDX文件，添加Frontmatter：\n    ```yaml\n    title: \"ReAct Prompting\"\n    tags: [\"agent\", \"tool-use\", \"reasoning\"]\n    model_support: [\"gpt-4\", \"claude-3\", \"llama-3\"]\n    complexity: \"advanced\"\n    ```\n  - 添加自定义组件：在`/src/components/MyCustomPrompt.jsx`，导出为React组件，在MDX中调用。  \n- **API接口**：无REST API；通过前端JS注入实现交互（如`window.promptEngineer.track('used_cot', {model: 'gpt-4'})`）。  \n- **添加功能示例**：  \n  ```bash\n  # 添加“提示词质量评分”模块\n  mkdir -p content/quality-assessment/\n  touch content/quality-assessment/scorecard.mdx\n  ```\n  在MDX中嵌入评分逻辑（基于ROUGE、LLM-as-Judge模板）。\n\n### ❗ 常见问题与避坑\n\n1. **Q：为什么我的prompt在文档里能跑，但部署到生产环境就失效？**  \n   A：文档使用gpt-4-turbo，默认temperature=0；生产中若用Claude或本地模型，需调整`max_tokens`, `stop_sequences`。  \n\n2. **Q：MDX中的代码块无法运行怎么办？**  \n   A：确保在Next.js环境预览（`npm run dev`），纯GitHub查看不支持JSX渲染。  \n\n3. **Q：如何贡献翻译？**  \n   A：访问 https://translate.promptingguide.ai，通过Crowdin提交译文，自动同步PR。  \n\n4. **Q：教程中推荐的工具（如LangChain）版本太旧了！**  \n   A：项目不维护代码库依赖，仅提供模式；请参考官方文档更新API调用方式。  \n\n5. **Q：搜索不到某个关键词？**  \n   A：检查Frontmatter是否包含tag；若无，请提交PR补充`tags: [\"your-keyword\"]`。  \n\n6. **Q：为什么没有“Prompt Optimization Algorithm”章节？**  \n   A：该项目定位是工程实践指南，非算法研究。优化方法（如RLHF、AutoPrompt）属于模型训练范畴，不在其scope内。\n\n### 🚀 进阶学习路径\n\n- ✅ 学完本项目后 → 深入阅读 **LangChain Agent Cookbook** + **LlamaIndex RAG Pipeline Optimization**  \n- 🔍 研究 **Microsoft AutoGen** 的多Agent协作架构（与本项目“Prompt Chaining”形成互补）  \n- 📊 实践：用 **Weights & Biases** 跟踪prompt版本效果，构建Prompt A/B测试平台  \n- 🏗️ 构建企业级方案：将本项目知识库接入 **Supabase + Next.js + OpenAI Function Calling**，打造内部AI助手  \n- 📚 推荐延伸阅读：  \n  - “The Annotated Transformer”（对Attention机制的工程化理解）  \n  - “Prompting with Language Models: A Survey” (arXiv 2024)  \n  - **Hugging Face Transformers Prompt Tuning** 实战课程",
    "last_scanned": "2026-01-16T02:03:34.443337",
    "last_analyzed": "2026-01-15T09:16:22.135139",
    "screenshot": "static/screenshots/579082810.jpg",
    "ai_visual_summary": "该界面是 GitHub 项目 `Prompt-Engineering-Guide` 的 README 文件，采用简洁的列表式设计，主要功能模块为一个结构化的知识导航，按“应用”、“提示词中心”和“模型”等类别组织了大量关于提示工程的资源链接。可见的技术关键词包括“多模态提示”、“图提示”、“函数调用”、“生成合成数据”、“RAG”、“AI Agent”、“ChatGPT”、“Code Llama”等，表明该项目是一个专注于人工智能提示工程、上下文工程、检索增强生成（RAG）和AI智能体的综合性学习资源库。",
    "ai_rag_summary": null
  },
  {
    "id": "660551251",
    "name": "MetaGPT",
    "full_name": "FoundationAgents/MetaGPT",
    "category": "ai_agent",
    "stars": 63009,
    "forks": 7903,
    "description": "🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
    "url": "https://github.com/FoundationAgents/MetaGPT",
    "homepage": "https://mgx.dev/",
    "language": "Python",
    "topics": "[\"agent\", \"gpt\", \"llm\", \"metagpt\", \"multi-agent\"]",
    "created_at": "2023-06-30T09:04:55Z",
    "updated_at": "2026-01-15T16:29:14Z",
    "readme_content": null,
    "ai_summary": "基于LangChain构建的企业级多智能体框架，模拟软件公司角色协作流程，实现自然语言驱动的复杂任务自动化处理",
    "ai_tech_stack": "[\"Python\", \"FastAPI\", \"Pydantic\", \"LangChain\", \"AutoGJ\"]",
    "ai_use_cases": "[\"AI\\u8f85\\u52a9\\u9700\\u6c42\\u5206\\u6790\\u4e0e\\u7cfb\\u7edf\\u8bbe\\u8ba1\", \"\\u591a\\u667a\\u80fd\\u4f53\\u534f\\u540c\\u7f16\\u7a0b\\uff08\\u5982\\u4ee3\\u7801\\u751f\\u6210\\u3001\\u6d4b\\u8bd5\\u7528\\u4f8b\\u7f16\\u5199\\uff09\", \"\\u4f01\\u4e1a\\u7ea7RAG\\u5de5\\u4f5c\\u6d41\\u6784\\u5efa\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "pip install -r requirements.txt && python main.py init",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nMetaGPT 的核心差异化在于：**它首次将软件工程组织模型（SOP + 角色分工）系统性地映射到多智能体协作架构中，实现从“单模型生成代码”到“AI公司自主交付完整产品”的范式跃迁**。\n\n与 LangChain、AutoGen 等框架相比：\n- **LangChain** 聚焦于链式调用和工具集成，本质是“增强Prompt的流水线”，缺乏角色分工与流程控制。\n- **AutoGen** 引入多智能体对话，但角色职责松散、缺乏工程化SOP约束，易陷入无意义循环辩论。\n- **MetaGPT** 则定义了完整的软件公司工作流：产品经理 → 架构师 → 开发者 → 测试 → 文档撰写 → 项目管理，每一步都有**预设的结构化输出格式（如PRD、API文档、UML图）和校验机制**，确保交付物可追溯、可验收。\n\n它解决的关键痛点是：**LLM无法独立完成复杂工程任务不是因为能力不足，而是缺乏工程纪律与协作规范**。MetaGPT 用“SOP = Team”重构了AI开发范式——不是让AI写代码，而是让AI公司交付产品。\n\n---\n\n### 🔥 技术亮点\n\n1. **结构化 SOP 输出模板（Structured Output Protocol）**\n   - 每个角色输出必须符合预定义的 Pydantic Schema（如 `ProductManager.write_prd()` → 返回 `PRD` 类实例），强制结构化，避免LLM自由发挥导致歧义。\n   - 通过 `@register_action` 装饰器绑定动作与Schema，实现“行为即数据”的可解析协作。\n\n2. **基于消息总线的异步角色调度（Async Message Bus）**\n   - 所有角色通过 `Message` 对象通信，内容包含 `content`, `role`, `sent_from`, `send_to`。\n   - 使用 `asyncio.Queue` 实现非阻塞、可扩展的角色间异步通信，支持动态添加/移除成员。\n\n3. **SOP 演化引擎（SOP Engine）**\n   - SOP 不是硬编码流程，而是可被 LLM 自我优化的“工作流模板”。通过 `AFlow` 论文提出的自省机制，系统能根据历史任务反馈动态调整角色顺序或新增步骤。\n   - 实现了**元级自动化**：AI公司学会如何更好地组织自己。\n\n4. **多模态输入支持（截图 → PRD）**\n   - 集成 OCR + 视觉理解模型（如 CLIP），可接收 UI 截图，自动生成需求文档。这是当前唯一支持“视觉驱动开发”的开源框架。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构（文字描述）\n\n```\n[User Input: Natural Language Requirement]\n          ↓\n    [SOP Orchestrator] ←─┐\n          ↓              │\n   ┌───────────────┐     │\n   │ Product Manager│ → Generates PRD (Pydantic) → Sent to Architect\n   └───────────────┘     │\n          ↓              │\n   ┌───────────────┐     │\n   │    Architect  │ → Designs System, Outputs UML + API Spec\n   └───────────────┘     │\n          ↓              │\n   ┌───────────────┐     │\n   │   Developer   │ → Generates Code (Python/JS) + Unit Tests\n   └───────────────┘     │\n          ↓              │\n   ┌───────────────┐     │\n   │    Tester     │ → Runs tests, reports failures → Feedback Loop\n   └───────────────┘     │\n          ↓              │\n   ┌───────────────┐     │\n   │ Documenter    │ → Writes Markdown docs + README\n   └───────────────┘     │\n          ↓              │\n[Deliverable: Git Repo + Docs + Tests] ←─┘\n```\n\n> 所有角色通过 `Message` 中心总线通信，SOP Orchestrator 负责按流程触发下一个节点，并监控完成状态。\n\n#### 2. 核心模块划分\n\n| 模块 | 职责 |\n|------|------|\n| `Agent` | 基类，封装 LLM 接口、记忆、工具调用能力 |\n| `Role` | 继承 Agent，绑定特定 SOP 行为（如 `ProductManager`, `Coder`） |\n| `SOP` | 定义流程图（DAG），包含节点顺序、输入/输出契约 |\n| `Message` | 通信载体，含内容、来源、目标、时间戳、元数据 |\n| `LLM` | 封装 OpenAI / Claude / Qwen 等接口，支持缓存与重试 |\n| `SOPEngine` | 运行时调度器，管理角色生命周期和消息路由 |\n\n#### 3. 数据流向\n\n```\nUser Prompt → Message(role=user, content=\"Build a login page\") \n    ↓\nSOP Orchestrator → Select SOP \"SoftwareDev\" \n    ↓\nTrigger ProductManager → LLM generates PRD (structured JSON) → Enqueue Message(role=pm, content=PRD, to=architect)\n    ↓\nArchitect consumes PM's message → Generates System Design + API Spec → Enqueue to Developer\n    ↓\nDeveloper → Code Generation + Unit Test → Enqueue to Tester\n    ↓\nTester → Execute pytest → If fail → Enqueue failure to Developer (feedback loop)\n    ↓\nDocumenter → Generate README, Docs → Final Output: Git repo + Markdown\n```\n\n#### 4. 关键设计模式\n\n- **发布-订阅（Pub/Sub）**：角色不直接调用彼此，而是通过 Message 总线解耦。\n- **策略模式（Strategy Pattern）**：每个 Role 实现 `act()` 方法，可动态替换不同行为（如使用 CodeLlama 或 GPT-4o）。\n- **工厂模式（Factory Pattern）**：`RoleRegistry` 动态注册角色类，支持插件化扩展。\n- **命令模式（Command Pattern）**：每个 SOP 步骤是可序列化的“动作命令”，便于审计与重放。\n\n> 设计动机：**避免状态爆炸 + 支持可恢复性 + 实现人类可读的协作日志**\n\n---\n\n### 🔧 技术栈深度解析\n\n| 组件 | 选择原因 | 替代方案 | 注意事项 |\n|------|----------|-----------|----------|\n| **Python 3.9–3.11** | 稳定、异步生态成熟（asyncio）、Pydantic 支持好 | Python 3.12+ 弃用 `asyncio` 某些 API，不兼容 | 必须限制版本！`pip install \"metagpt<0.9\"` |\n| **Pydantic v2** | 结构化输出的黄金标准，支持 JSON Schema 自动推导 | dataclasses / typing.NamedTuple（缺乏验证/序列化） | 所有 SOP 输出必须是 Pydantic Model |\n| **LangChain (v0.1)** | 仅用于工具链集成（如搜索、代码执行），非主框架 | AutoGen（耦合过重）、LlamaIndex（专注检索） | MetaGPT 避免使用 LangChain 的 Agent 框架，自研调度 |\n| **FastAPI** | 提供 REST API 接口给 MGX Web 服务 | Flask（异步支持弱） | 主要用于 mgx.dev 后端，非核心逻辑 |\n| **asyncio + asyncio.Queue** | 高并发、低延迟角色通信，无锁设计 | Celery/RQ（过重，有队列开销） | 所有 Role 行为必须是 `async def` |\n| **Node.js + pnpm** | 用于前端构建（MGX Web UI），非 Python 核心 | 完全用 Python 前端（如 Streamlit）→ 功能受限 | 必须安装，否则无法运行 Web 演示 |\n\n> ⚠️ 注意：`config2.yaml` 是 YAML + JSON Schema 验证配置，不支持环境变量动态注入 —— 企业级部署需自行封装。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 创建独立 Python 环境（必须 3.9–3.11）\nconda create -n metagpt python=3.10 -y && conda activate metagpt\n\n# 2. 安装 MetaGPT（稳定版）\npip install --upgrade \"metagpt>=0.8.0,<0.9\"\n\n# 3. 安装 Node.js 和 pnpm（必须！用于 Web UI）\n# macOS\nbrew install node\nnpm install -g pnpm\n\n# Linux (Ubuntu)\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\nsudo apt-get install -y nodejs\nnpm install -g pnpm\n\n# 4. 初始化配置文件（生成 ~/.metagpt/config2.yaml）\nmetagpt --init-config\n\n# 5. 编辑配置文件，填入你的 API Key（示例）\nvim ~/.metagpt/config2.yaml\n```\n\n**config2.yaml 关键字段：**\n```yaml\nllm:\n  api_type: \"openai\"          # 支持 openai, anthropic, qwen, deepseek\n  model: \"gpt-4-turbo\"        # 推荐使用 gpt-4o 或 gpt-4-turbo\n  api_key: \"sk-xxxxxxxxxxx\"\n  temperature: 0.3            # 降低随机性，确保 SOP 输出稳定\n\n# 可选：启用本地模型（Ollama）\n# llm:\n#   api_type: \"ollama\"\n#   model: \"llama3:70b\"\n\ntools:\n  use_code_interpreter: true  # 启用代码执行环境（需安装 jupyter）\n```\n\n> ✅ 验证：运行 `metagpt --help`，应显示 CLI 命令列表。\n\n---\n\n### 🎮 使用示例\n\n**真实场景输入：**\n```bash\nmetagpt --project-name \"todo-app\" --instruction \"Build a Python Flask web app with login, JWT auth, and SQLite DB. User can create/delete tasks.\"\n```\n\n**预期输出结构（在 `workspace/todo-app/` 下）：**\n```\n├── requirements.txt\n├── main.py                  # Flask 应用入口\n├── models/\n│   └── user.py              # SQLAlchemy 用户模型\n├── schemas/\n│   └── task.py              # Pydantic 请求体校验\n├── routes/\n│   └── auth.py              # JWT 登录路由\n├── docs/\n│   ├── PRD.md               # 产品需求文档（由PM生成）\n│   └── API_DOC.md           # 自动生成的 OpenAPI 文档\n└── tests/\n    └── test_auth.py         # 单元测试（由工程师编写）\n```\n\n**关键参数说明：**\n- `--project-name`：工作区名称，决定输出目录。\n- `--instruction`：必须为自然语言完整需求，越具体越好（避免模糊如“做个网站”）。\n- 默认使用 SOP: `SoftwareDev`，可指定其他：`--sop \"MarketAnalysis\"`。\n\n> 💡 实际运行时，你会看到多个 AI 角色在终端中“对话”：\n```\nProductManager: 按照用户需求，我定义了以下核心功能...\nArchitect: 建议采用 Flask + JWT + SQLite 架构，以下是设计图...\nEngineer: 已生成 main.py, auth.py, models/user.py...\nTester: 单元测试通过 12/12，覆盖率 95%...\n```\n\n---\n\n### ⚡ 性能与优化\n\n- **性能瓶颈**：\n  - LLM 调用延迟：每个角色平均耗时 3–8s（GPT-4），完整流程约 60–120s。\n  - 消息队列阻塞：若某个 Role 输出格式错误，整个 SOP 停滞。\n  \n- **生产环境扩展建议**：\n  - 引入异步批处理：多个任务排队，使用 RabbitMQ/Kafka 解耦 LLM 请求。\n  - 缓存机制：对相同需求的 PRD/Design 进行哈希缓存（`redis` + `SHA256(input)`）。\n  - 模型混合：用 Claude Haiku 做初步草稿，GPT-4o 精修。\n  - 成本控制：使用 `gpt-3.5-turbo` 做文档生成，仅关键步骤用 GPT-4。\n\n- **资源消耗估算**（单任务）：\n  | 资源 | 消耗 |\n  |------|------|\n  | CPU | 1–2 核（轻量） |\n  | RAM | <500MB |\n  | LLM Tokens | ~8K–15K 输入，~3K 输出 |\n  | 成本（GPT-4） | $0.03–$0.08/次 |\n\n> ✅ 可支持并发 5–10 任务（受限于 API Key 并发配额）\n\n---\n\n### 🔌 二次开发指南\n\n#### 关键扩展点：\n\n| 扩展目标 | 实现方式 |\n|----------|-----------|\n| **新增角色** | 继承 `Role`",
    "last_scanned": "2026-01-16T02:03:34.456845",
    "last_analyzed": "2026-01-15T10:16:43.920532",
    "screenshot": "static/screenshots/660551251.jpg",
    "ai_visual_summary": "根据截图分析，这是一个名为 **MetaGPT** 的开源项目，其界面设计风格为简洁的**技术文档**风格，以白色为背景，通过清晰的文本排版和代码块高亮来呈现信息。\n\n该应用的核心功能是**通过自然语言指令，利用AI代理（如GPT）自动化软件开发**。从界面可见，其主要功能模块包括**项目配置**（如设置模型和API密钥）、**命令行（CLI）使用**和**作为库的集成**。关键技术关键词包括 `MetaGPT`、`GPT`、`CLI`、`API`、`Python`、`asyncio` 和 `Jupyter`。项目描述“AI软件公司”和“自然语言编程”进一步明确了其目标：**让AI代理像一个软件公司一样，通过自然语言完成复杂的编程任务**。",
    "ai_rag_summary": null
  },
  {
    "id": "323048702",
    "name": "OpenBB",
    "full_name": "OpenBB-finance/OpenBB",
    "category": "ai_agent",
    "stars": 58702,
    "forks": 5705,
    "description": "Financial data platform for analysts, quants and AI agents.",
    "url": "https://github.com/OpenBB-finance/OpenBB",
    "homepage": "https://openbb.co",
    "language": "Python",
    "topics": "[\"ai\", \"crypto\", \"derivatives\", \"economics\", \"equity\", \"finance\", \"fixed-income\", \"machine-learning\", \"openbb\", \"options\", \"python\", \"quantitative-finance\", \"stocks\"]",
    "created_at": "2020-12-20T10:46:38Z",
    "updated_at": "2026-01-15T17:44:29Z",
    "readme_content": null,
    "ai_summary": "金融数据平台ODP为分析师、量化人员和AI代理提供'连接一次，随处消费'的数据集成基础设施，核心是Python SDK与企业级UI（OpenBB Workspace）的结合，支持多渠道数据共享与自动化分析",
    "ai_tech_stack": "[\"Python\", \"REST APIs\", \"MCP Servers\", \"Excel Integration\"]",
    "ai_use_cases": "[\"\\u91d1\\u878d\\u6570\\u636e\\u5206\\u6790\\u4eea\\u8868\\u677f\\u5f00\\u53d1\", \"\\u91cf\\u5316\\u7b56\\u7565\\u56de\\u6d4b\\u7cfb\\u7edf\\u96c6\\u6210\", \"AI\\u4ee3\\u7406\\u91d1\\u878d\\u7814\\u7a76\\u5de5\\u5177\\u94fe\\u6784\\u5efa\", \"\\u4f01\\u4e1a\\u7ea7\\u6295\\u8d44\\u7ec4\\u5408\\u7ba1\\u7406\\u5e73\\u53f0\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "pip install openbb",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nOpenBB 的核心差异化在于**构建了金融数据领域的“统一接入层 + 多端消费生态”闭环**，解决了传统金融数据平台的三大痛点：\n\n1. **碎片化集成成本高**：传统方案（如 Bloomberg Terminal、Wind、QuantConnect）要么闭源昂贵，要么仅支持单一环境（如 Python 但无 UI，或有 UI 但无法编程）。OpenBB 首次将**Python SDK、CLI、REST API、AI Agent MCP 服务、Web Workspace** 全部统一于同一数据后端，实现“connect once, consume everywhere”。\n\n2. **AI 原生设计缺失**：多数金融库（如 yfinance、alpha_vantage）是静态函数库，无法被 LLM 自动调用。OpenBB 通过 **MCP (Model Communication Protocol)** 暴露结构化 API，使 AI Agent 可像调用工具一样自动发现并执行 `obb.equity.price.historical()` 等操作，真正实现“AI 能直接操作金融数据”的能力。\n\n3. **开源与商业的双轨协同**：OpenBB Core（MIT）提供完整功能，而 OpenBB Workspace（商业 SaaS）通过插件机制无缝对接本地 ODP 实例。这种“开源基础设施 + 商业体验”模式，避免了开源项目沦为演示品的命运——它被真实机构用于生产级投研流水线。\n\n对比同类：  \n- **yfinance / pandas-datareader**：仅数据抓取，无统一接口、无 UI、无 Agent 支持。  \n- **Alpaca / Polygon.io SDKs**：专有 API 封装，不支持多源聚合。  \n- **Jupyter + Plotly 自建仪表盘**：缺乏标准化数据接入层与协作机制。\n\nOpenBB 是首个真正实现“**金融数据即服务（Data-as-a-Service）**”的开源平台。\n\n---\n\n### 🔥 技术亮点\n\n1. **动态插件式数据源架构**  \n   每个数据提供商（Yahoo Finance、FRED、Alpaca、CryptoCompare 等）被封装为独立 Python 包（如 `openbb_finance_yahoo`），通过 `entry_points` 动态加载。无需重启服务，支持热插拔。开发者只需实现统一接口 `Fetcher`, `Transformer`, `Loader` 即可接入新源。\n\n2. **MCP Server 作为 AI Agent 桥梁**  \n   将 Python 函数（如 `obb.equity.price.historical()`）自动暴露为 OpenAI Function Calling 兼容的 JSON Schema。通过 FastAPI 的 `/api/v1/tools` 端点，Agent 可实时发现可用函数、参数类型、文档说明，实现零配置 AI 集成。\n\n3. **异步并行数据聚合**  \n   多源查询（如同时拉取 AAPL 从 Yahoo + Alpha Vantage + Quandl）使用 `asyncio.gather()` 并发执行，配合缓存层（Redis 或内存 LRU），显著降低延迟。在回测场景中，可比串行方案快 3–5 倍。\n\n4. **类型安全的输出封装**  \n   所有查询返回 `OpenBBData` 对象，而非原始 DataFrame。该对象内置元数据（来源、时间戳、单位）、标准化列名、自动时区处理，并支持 `.to_dataframe()`, `.to_json()`, `.to_plotly()` 等多格式输出，避免下游“脏数据”问题。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构（文字描述）\n\n```\n+--------------------------------------------------+\n|               OpenBB Workspace (SaaS)            |\n|   - Web UI | AI Agents | Dashboards | Notebooks  |\n|         ↑↓ HTTP / WebSocket via API Gateway      |\n+-----------------------+--------------------------+\n                        |\n        +-----------------------------------------+\n        |       OpenBB Data Platform (ODP)        |\n        |  [FastAPI Server on Uvicorn]            |\n        |   ┌───────────────┐                     |\n        |   │  Core Engine  │◄─┐                  |\n        |   │ - Router      │  │                  |\n        |   │ - Cache       │  │                  |\n        |   │ - Auth/Z      │  │                  |\n        |   └───────┬───────┘  │                  |\n        |           │          │                  |\n        |   +-------v--------+ v-----------------+\n        |   │ Plugin Manager | │ MCP Server       |\n        |   │ - Dynamic Load | │ - OpenAI Func    |\n        |   │ - Entry Points | │ - JSON Schema    |\n        |   └───────┬────────┘ └─────────────────┘\n        |           │\n+-------v---------v-----------------v------------+\n|  Data Source Plugins (Python Packages)         |\n|  openbb_finance_yahoo                          |\n|  openbb_finance_alpha_vantage                  |\n|  openbb_finance_quandl                         |\n|  openbb_crypto_coin_gecko                      |\n|  ... (50+ plugins)                             |\n+------------------------------------------------+\n```\n\n#### 2. 核心模块职责\n\n- **Core Engine**：路由分发、缓存管理（Redis/内存）、认证授权（JWT/OAuth2）、日志追踪。\n- **Plugin Manager**：基于 `importlib.metadata` 扫描 `openbb_*` 包，动态注册 `Provider` 类，实现“插件即服务”。\n- **MCP Server**：自动生成 OpenAI Function Calling 兼容的 schema（含参数描述、示例、限制），支持工具调用链式编排。\n- **Data Models**：Pydantic V2 定义统一输入/输出结构，确保类型安全。\n\n#### 3. 数据流向\n\n```\n用户请求 → CLI / API / Workspace\n    ↓\nCore Engine 路由（如 /equity/price/historical）\n    ↓\nPlugin Manager 查找对应 Provider (e.g., YahooFinanceProvider)\n    ↓\n调用 Fetcher → 异步 HTTP 请求原始数据源\n    ↓\nTransformer 标准化字段名、单位、时区、缺失值处理\n    ↓\nLoader 组装为 OpenBBData 对象（含元数据）\n    ↓\n缓存写入 Redis（TTL 5min）\n    ↓\n返回 JSON / DataFrame / Plotly 图表\n```\n\n#### 4. 关键设计模式\n\n- **策略模式**：每个数据源是独立的 `Provider` 实现，接口统一（`fetch()`, `transform()`），可随时替换或扩展。\n- **外观模式（Facade）**：`obb.equity.price.historical()` 是复杂调用链的简洁门面，隐藏插件发现、认证、缓存等细节。\n- **发布/订阅（间接解耦）**：Workspace 通过 REST API 消费 ODP，两者完全独立部署，支持分布式架构。\n- **依赖注入（DI）**：使用 `dependency-injector` 管理数据库连接、缓存实例、API 密钥等服务依赖。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 技术 | 选择理由 | 替代方案 | 注意事项 |\n|------|----------|-----------|----------|\n| **Python 3.9–3.12** | 生态成熟（Pandas, NumPy, Plotly）、AI 社区首选 | PyPy / Rust（性能提升但生态弱） | 必须使用 CPython，因依赖 Cython 模块（如 yfinance 的 lxml） |\n| **FastAPI + Uvicorn** | 异步支持、自动生成 OpenAPI/Swagger、MCP 服务天然适配 | Flask + Gunicorn | 使用 `uvicorn[standard]` 启动，避免 ASGI 中间件冲突 |\n| **Pydantic V2** | 类型校验+JSON Schema 自动生成（用于 MCP） | Marshmallow / dataclasses | 必须启用 `model_config = ConfigDict(from_attributes=True)` 以兼容 ORM |\n| **Redis (可选)** | 缓存高频查询、加速回测 | SQLite + TTL / Memcached | 生产环境建议部署 Redis Cluster，避免单点瓶颈 |\n| **asyncio + httpx** | 非阻塞并发请求，优于 requests+threading | aiohttp | 注意：部分数据源（如 FRED）不支持 Keep-Alive，需配置 `httpx.Client(timeout=30)` |\n| **entry_points** | 动态插件加载标准 | importlib + 手动注册 | 必须在 `setup.py`/`pyproject.toml` 中明确定义：`openbb.providers = [\"yahoo = openbb_finance_yahoo.provider\"]` |\n\n> ⚠️ 版本兼容性注意：OpenBB 4.x+ 要求 Python ≥3.9，且 **不支持 Windows + WSL1**（因 asyncio 事件循环问题），建议使用 WSL2 或 Linux。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 创建独立环境（推荐）\npython -m venv openbb-env && source openbb-env/bin/activate  # Linux/Mac\n# 或 openbb-env\\Scripts\\activate  # Windows\n\n# 2. 安装全功能包（含所有数据源 + API服务器）\npip install \"openbb[all]\"\n\n# 3. 启动本地 ODP API 服务（监听 127.0.0.1:6900）\nopenbb-api --host 127.0.0.1 --port 6900\n\n# 4. 验证服务是否运行（在浏览器打开或 curl）\ncurl http://127.0.0.1:6900/docs\n# 应返回 Swagger UI 页面，包含所有可用工具（如 /equity/price/historical）\n\n# 5. （可选）配置 API 密钥（部分源需认证）\nopenbb config set api_key_yahoo=YOUR_YAHOO_KEY\nopenbb config set api_key_alpha_vantage=YOUR_AV_KEY\n\n# 6. 连接 OpenBB Workspace：\n# 登录 https://pro.openbb.co → Settings → Data Sources → Add Local Backend\n# 输入：http://127.0.0.1:6900 → 点击 Connect\n```\n\n---\n\n### 🎮 使用示例\n\n```python\nfrom openbb import obb\n\n# 场景：获取苹果公司过去 5 年日线数据，计算波动率并绘图\noutput = obb.equity.price.historical(\n    symbol=\"AAPL\",\n    start_date=\"2019-05-17\",\n    end_date=\"2024-05-17\",\n    interval=\"1d\",  # 可选：1m, 5m, 15m, 30m, 1h, 1d, 1wk, 1mo\n    provider=\"yahoo\"  # 显式指定数据源，避免默认轮询\n)\n\n# 转换为 DataFrame\ndf = output.to_dataframe()\n\n# 计算 20 日滚动波动率（标准差）\ndf['volatility_20'] = df['close'].pct_change().rolling(20).std() * (252**0.5)  # 年化\n\n# 输出前五行\nprint(df[['close', 'volatility_20']].head())\n\n# 预期输出：\n#           close  volatility_20\n# Date                            \n# 2019-05-17   36.84            NaN\n# 2019-05-20   37.07            NaN\n# 2019-05-21   37.23         0.0182\n# 2019-05-22   36.48         0.0191\n# 2019-05-23   36.68         0.0179\n\n# 可选：生成 Plotly 图表（需安装 plotly）\nfig = obb.equity.price.historical.plot(symbol=\"AAPL\", start_date=\"2023-01-01\")\nfig.show()  # 在 Jupyter 或浏览器中渲染交互图表\n```\n\n---\n\n### ⚡ 性能与优化\n\n- **瓶颈**：  \n  - 网络延迟（尤其 Yahoo Finance、Alpha Vantage 限频） → 解决方案：启用 Redis 缓存，设置 TTL=5m。  \n  - 大量并发请求导致 IP 被封 → 使用 `openbb config set rate_limit=true` + 分布式代理池（如 Rotating Proxy）。  \n- **生产扩展**：  \n  - 部署多个 ODP 实例 + Nginx 负载均衡，后端连接独立数据库缓存。  \n  - 对高频查询（如 SPY、TSLA）预热缓存（定时任务 `obb.equity.price.historical(..., cache_only=True)`）。  \n- **资源消耗**：  \n  - 单实例（100 并发）：CPU ~1.2 cores，内存 ~800MB，Redis ~300MB。  \n  - 每日处理 5K 查询：约需 4GB Redis 存储（按每条查询 1KB 缓存估算）。  \n\n> ✅ 建议生产部署：`Docker Compose + Redis + Nginx + Uvicorn (workers=4)`\n\n---\n\n### 🔌 二次开发",
    "last_scanned": "2026-01-16T02:03:34.459013",
    "last_analyzed": "2026-01-15T10:48:12.977912",
    "screenshot": "static/screenshots/323048702.jpg",
    "ai_visual_summary": "该截图展示了一个名为“OpenBB”的开源项目文档页面，其设计风格简洁、以内容为中心，属于典型的开发者友好型技术文档界面。界面主要功能模块包括项目介绍（README）、安装指南和内容目录。可见的关键技术关键词有“OpenBB”、“ODP”、“Python”、“PyPI”、“pip”、“CLI”、“GitHub”和“127.0.0.1:6900”等，明确指出这是一个用于金融数据分析的Python库和命令行工具，通过pip安装，并支持本地部署和与外部应用连接。综合来看，这个应用是一个为分析师、量化交易员和AI代理提供的金融数据平台。",
    "ai_rag_summary": null
  },
  {
    "id": "680120071",
    "name": "autogen",
    "full_name": "microsoft/autogen",
    "category": "ai_agent",
    "stars": 53491,
    "forks": 8108,
    "description": "A programming framework for agentic AI",
    "url": "https://github.com/microsoft/autogen",
    "homepage": "https://microsoft.github.io/autogen/",
    "language": "Python",
    "topics": "[\"agentic\", \"agentic-agi\", \"agents\", \"ai\", \"autogen\", \"autogen-ecosystem\", \"chatgpt\", \"framework\", \"llm-agent\", \"llm-framework\"]",
    "created_at": "2023-08-18T11:43:45Z",
    "updated_at": "2026-01-15T16:25:27Z",
    "readme_content": null,
    "ai_summary": "AutoGen 是一个多代理 AI 框架，允许多个 AI 代理协作完成复杂任务；核心技术包括基于聊天的消息传递机制、支持 OpenAI 和 Playwright 插件、提供无代码 GUI 工具（AutoGen Studio）。",
    "ai_tech_stack": "[\"Python\", \"AgentChat\", \"OpenAI API\", \"Playwright MCP\", \"Console UI\"]",
    "ai_use_cases": "[\"\\u591a\\u4ee3\\u7406\\u534f\\u4f5c\\u5b8c\\u6210\\u590d\\u6742\\u95ee\\u9898\\u5206\\u89e3\\u4e0e\\u89e3\\u51b3\", \"\\u81ea\\u52a8\\u5316\\u4efb\\u52a1\\u5904\\u7406\\uff0c\\u5982\\u7f51\\u9875\\u4fe1\\u606f\\u6293\\u53d6\\u3001\\u6570\\u636e\\u5206\\u6790\\u7b49\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "pip install -U 'autogen-agentchat' 'autogen-ext[openai]' && pip install -U 'autogenstudio'",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nAutoGen 的核心差异化在于**将多智能体协作从“概念演示”转化为可工程化的生产级框架**。与 LangChain、LlamaIndex 等侧重单智能体链式调用的框架不同，AutoGen 首创了**基于角色的对话协议 + 可插拔工具编排 + 会话状态持久化**三位一体的架构，解决了以下痛点：\n\n1. **动态角色协商机制**：智能体可自主决定是否转交任务、请求澄清或发起多轮辩论（如 `Handoff` 和 `Terminate` 消息），而非硬编码流程。\n2. **工具调用与上下文隔离**：每个 Agent 的 Tool 使用权限独立配置，避免“一个工具被所有Agent滥用”的安全与语义污染问题。\n3. **人类作为第一公民的闭环设计**：支持 HumanInputAgent 无缝嵌入工作流，实现 AI-Human 协同决策（如审批、修正），而非仅用于调试。\n4. **异步流式响应与状态机驱动**：通过 `run_stream()` + `max_tool_iterations` 实现可控的、非阻塞的长任务执行，避免 LLM 漫游。\n\n同类项目多聚焦“单轮工具调用”，AutoGen 则构建了**可推理、可协商、可中断的智能体对话图谱**，是首个将 Multi-Agent System（MAS）理论落地为工业级 API 的框架。\n\n---\n\n### 🔥 技术亮点\n\n- **基于消息队列的异步通信协议**：所有 Agent 间通过 `AgentMessage` 对象传递结构化内容（含 `content`, `source`, `tool_calls`, `tool_responses`），实现松耦合与可追溯。\n- **动态工具绑定（Workbench）**：MCP（Model-Controlled Protocol）服务器以 Stdio/HTTP 方式接入，通过 `McpWorkbench` 将本地命令行工具（如 Playwright、curl）封装为语义化工具，无需重写 LLM 提示词。\n- **会话状态快照机制**：`ConversationHistory` 采用增量追加 + 滑动窗口摘要（可选），避免上下文爆炸，支持长对话恢复。\n- **自适应终止策略**：基于 `max_tool_iterations`、`termination_condition`（如连续两次无工具调用）实现智能体协作的自动收敛，防止无限循环。\n- **零配置回退机制**：当工具调用失败时，Agent 可自动降级为纯文本推理，提升鲁棒性。\n\n---\n\n### 🏗️ 架构设计分析\n\n```\n[User Input] \n     ↓\n[Console / API] → [Orchestrator]\n                     ↓\n        +------------+------------+\n        |            |            |\n   [Agent A]     [Agent B]     [Agent C]  ← 多智能体池（可动态增删）\n        |            |            |\n        ↓ (Message)  ↓ (Message)  ↓ (Message)\n[Tool Workbench] ←─ MCP Server (Playwright, Python eval, etc.)\n        ↑\n[Memory Store] → ConversationHistory + VectorDB (可选)\n```\n\n#### 核心模块职责：\n\n| 模块 | 职责 |\n|------|------|\n| `AssistantAgent` | LLM 驱动的决策核心，解析消息、生成工具调用或文本响应 |\n| `McpWorkbench` | 工具执行沙盒，封装外部进程（如 npm 包）为异步可调用函数 |\n| `ConversationHistory` | 维护多轮对话状态，支持自动摘要与上下文裁剪 |\n| `AgentGroupChat` | 多 Agent 协作调度器，管理消息路由、角色切换、终止条件 |\n| `HumanInputAgent` | 接收人工输入并注入对话流，作为“监管节点” |\n\n#### 数据流向：\n\n```\nUser → [Orchestrator] → (Message) → Agent A → (Tool Call) → McpWorkbench → 执行 curl/Playwright → 返回结果 → \n→ Agent A → (Updated Message) → Agent B → (Decide: Terminate? Handoff?) → ... → 终止 → 输出最终响应\n```\n\n#### 关键设计模式：\n\n- **观察者模式**：每个 Agent 监听消息队列，异步响应触发事件。\n- **策略模式**：`termination_condition` 可替换为自定义逻辑（如基于语义相似度判断收敛）。\n- **工厂模式**：`AgentFactory` 支持注册自定义 Agent 类型，实现插件化扩展。\n- **命令模式**：所有工具调用封装为 `ToolCall` 对象，可序列化、重放、审计。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 技术 | 选择原因 | 替代方案 | 注意事项 |\n|------|----------|----------|----------|\n| **Python 3.10+** | 异步支持完善（`async/await`）、生态成熟、AI 工具链兼容性最佳 | Python 3.8（已弃用） | 必须 ≥3.10，否则 `typing_extensions` 和 `pydantic v2` 不兼容 |\n| **OpenAIChatCompletionClient** | 支持流式响应、工具调用 JSON Schema 验证 | Anthropic, Cohere, Local LLMs via LiteLLM | 必须配置 API Key；本地模型需替换为 `OllamaChatCompletionClient` 或自定义 Client |\n| **pydantic v2** | 强类型消息结构（`AgentMessage`, `ToolCall`）确保接口契约 | dataclass / attrs | 与旧版 AutoGen (v0.2) 不兼容，迁移需重构所有 Agent 定义 |\n| **asyncio + aiohttp** | 高并发、非阻塞 I/O，适配 LLM API 和 MCP 服务器 | threading + queue | 所有方法必须为 `async def`，否则死锁 |\n| **MCP (Model-Controlled Protocol)** | 开源标准，支持任意语言编写服务端（Node.js/Go/Rust） | 自定义 HTTP API | 必须确保 MCP 服务可被本地进程调用；建议在 Docker 中隔离 |\n\n> ⚠️ 注意：`autogen-agentchat` 和 `autogen-ext[openai]` 是解耦的，前者为内核，后者为扩展。生产环境应避免直接依赖 `autogen`（已废弃）。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 确保 Python ≥3.10（推荐使用 pyenv 或 conda）\npython --version\n\n# 2. 安装核心框架 + OpenAI 扩展（生产环境推荐锁定版本）\npip install -U \"autogen-agentchat==0.3.5\" \"autogen-ext[openai]==0.3.5\"\n\n# 3. （可选）安装 GUI 工具用于无代码调试\npip install -U autogenstudio\n\n# 4. 设置 OpenAI API 密钥（环境变量）\nexport OPENAI_API_KEY=\"sk-...\"  # Linux/macOS\nset OPENAI_API_KEY=sk-...       # Windows CMD\n$env:OPENAI_API_KEY = \"sk-...\"  # PowerShell\n\n# 5. （如需 Web 浏览）安装 Playwright MCP Server（Node.js 环境）\nnpm install -g @playwright/mcp@latest\n\n# 6. 验证安装\npython -c \"from autogen_agentchat.agents import AssistantAgent; print('OK')\"\n```\n\n---\n\n### 🎮 使用示例\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\nasync def main():\n    # 1. 初始化模型客户端（使用 GPT-4o，支持流式）\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", temperature=0.3)\n\n    # 2. 启动 Playwright MCP Server 沙盒\n    server_params = StdioServerParams(\n        command=\"npx\",\n        args=[\"@playwright/mcp@latest\", \"--headless\"],\n    )\n    \n    async with McpWorkbench(server_params) as mcp:\n        \n        # 3. 创建具备网络浏览能力的助手\n        agent = AssistantAgent(\n            name=\"research_assistant\",\n            model_client=model_client,\n            workbench=mcp,  # 绑定工具沙盒\n            max_tool_iterations=5,  # 最多调用5次工具，防死循环\n            system_message=\"你是一个科研助手。仅使用提供的工具获取信息，不要编造数据。\"\n        )\n\n        # 4. 执行任务并流式输出结果（支持实时打印）\n        await Console(agent.run_stream(\n            task=\"请查询 microsoft/autogen 仓库的贡献者数量，并总结最新3位提交者的贡献主题。\"\n        ))\n\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n#### 预期输出：\n\n```text\n[research_assistant] 正在使用工具：mcp_web_browsing...\n[tool] 已获取 GitHub API 数据：{\"total_contributors\": 1247, \"top_3\": [...]}\n[research_assistant] microsoft/autogen 目前有 1247 位贡献者。最新三位提交者分别为：\n- Alice: 添加了多智能体会话摘要模块\n- Bob: 优化了 MCP 工具调用超时机制\n- Charlie: 文档中补充了 HumanInputAgent 使用示例\n```\n\n#### 关键参数说明：\n\n| 参数 | 含义 |\n|------|------|\n| `max_tool_iterations` | 控制工具调用最大次数，防止 LLM 陷入无限循环（默认=10） |\n| `model_client_stream=True` | 开启流式响应，降低延迟、提升用户体验 |\n| `workbench=mcp` | 指定可用工具集，多个工具可传入列表 `[mcp1, mcp2]` |\n\n---\n\n### ⚡ 性能与优化\n\n- **性能瓶颈**：\n  - LLM 推理延迟（占 90%+ 时间）\n  - MCP Server 启动开销（每次启动约 1.5s，建议常驻 Docker 容器）\n  - 消息序列化/反序列化（JSON over stdio）\n\n- **生产环境扩展方案**：\n  - 使用 `Redis` 或 `Kafka` 解耦 Agent 通信\n  - 将 MCP Server 部署为独立微服务，通过 HTTP 调用而非 stdio\n  - 引入 LLM 缓存层（如 `llm-cache`）避免重复请求相同 prompt\n  - 使用 `Celery` 或 `FastAPI + background tasks` 实现异步任务队列\n\n- **资源消耗估算**：\n  | 场景 | 内存占用 | CPU 消耗 | 响应延迟 |\n  |------|----------|----------|----------|\n  | 单 Agent + GPT-4o | 1.2GB | 中等 | 3–8s |\n  | 3 Agent 协作 | 2.5GB | 高 | 6–15s |\n  | 启动 MCP Server | +100MB | 短时峰值 | 初始延迟 ~1.5s |\n\n> 建议：生产环境使用 `gpt-4o-mini` 或本地量化模型（如 `llama3-8b-instruct`）降低成本。\n\n---\n\n### 🔌 二次开发指南\n\n#### 关键扩展点：\n\n| 扩展点 | 接口 | 用途 |\n|--------|------|------|\n| 自定义 Agent | 继承 `AssistantAgent` 或 `UserProxyAgent` | 添加业务逻辑、状态机 |\n| 自定义工具 | 实现 `Tool` 接口或封装 MCP Server | 如：连接数据库、调用内部 API |\n| 消息中间件 | 实现 `MessageHandler` | 过滤/审计/日志记录所有通信 |\n| 状态持久化 | 重写 `save_state()` / `load_state()` | 支持中断恢复 |\n\n#### 示例：添加自定义工具\n\n```python\nfrom autogen_agentchat.tools import Tool\n\nclass DatabaseQueryTool(Tool):\n    async def __call__(self, query: str) -> str:\n        # 模拟查询内部数据库\n        result = execute_sql(query)\n        return f\"查询结果：{result}\"\n\n# 使用\nagent = AssistantAgent(..., tools=[DatabaseQueryTool()])\n```\n\n#### API 接口关键类：\n\n- `AssistantAgent`：主智能体，负责推理与工具调用决策\n- `UserProxyAgent`：人类代理，接收用户输入并转发给其他 Agent\n- `McpWorkbench`：工具沙盒管理器，自动注册 MCP 服务为可用工具\n- `Message` / `AgentMessage`：通信协议核心结构体\n\n---\n\n### ❗ 常见问题与避坑\n\n1. **Q: `ModuleNotFoundError: No module named 'autogen_agentchat'`**  \n   A: 确保安装的是 `autogen-agentchat`，不是旧版 `autogen`。删除所有旧包：`pip uninstall autogen && pip install -U \"autogen-agentchat\"`\n\n2. **Q: MCP Server 启动失败（Permission denied）**  \n   A: 在 Linux/macOS 上运行 `chmod +x /usr/local/bin/npx`，或使用完整路径如 `/home/user",
    "last_scanned": "2026-01-16T02:03:34.460121",
    "last_analyzed": "2026-01-15T11:29:45.346116",
    "screenshot": "static/screenshots/680120071.jpg",
    "ai_visual_summary": "该截图展示了一个名为 **autogen** 的 GitHub 项目，其核心是一个用于构建**智能体（Agentic AI）**的编程框架。从界面设计和内容来看，这是一个典型的开源技术项目主页。\n\n**界面主要功能模块**：项目主页顶部是标准的 GitHub 导航栏（README、Code of conduct 等），主体部分是代码示例，下方是“AutoGen Studio”的功能介绍，提供了无需编码即可运行多智能体工作流的工具。\n\n**可见的技术关键词**：代码中明确提到了 `AssistantAgent`、`AgentTool`、`model_client`、`max_tool_iterations` 等，这表明项目基于**智能体（Agent）**和**工具调用（Tool Use）**的架构。此外，`asyncio.run(main())` 说明其使用了异步编程。\n\n**应用功能分析**：该应用旨在帮助开发者快速构建和测试由多个智能体协同工作的系统。通过代码示例，可以看到可以创建一个数学专家智能体和一个化学专家智能体，并将它们组合成一个通用助手，使其能根据需要调用专业工具来完成任务。这表明它是一个用于**多智能体（Multi-Agent）系统**的开发框架。",
    "ai_rag_summary": null
  },
  {
    "id": "895508656",
    "name": "ai-agents-for-beginners",
    "full_name": "microsoft/ai-agents-for-beginners",
    "category": "ai_agent",
    "stars": 48756,
    "forks": 16927,
    "description": "12 Lessons to Get Started Building AI Agents",
    "url": "https://github.com/microsoft/ai-agents-for-beginners",
    "homepage": "https://aka.ms/ai-agents-beginners",
    "language": "Jupyter Notebook",
    "topics": "[\"agentic-ai\", \"agentic-framework\", \"agentic-rag\", \"ai-agents\", \"ai-agents-framework\", \"autogen\", \"generative-ai\", \"semantic-kernel\"]",
    "created_at": "2024-11-28T10:42:52Z",
    "updated_at": "2026-01-15T18:01:10Z",
    "readme_content": null,
    "ai_summary": "面向初学者的12课时AI代理构建指南",
    "ai_tech_stack": "[\"Python\\u7f16\\u7a0b\\u57fa\\u7840\", \"Jupyter Notebook\\u73af\\u5883\\u8bbe\\u7f6e\", \"AutoGen\\u6846\\u67b6\\u5e94\\u7528\", \"LangChain\\u6280\\u672f\\u5b9e\\u8df5\"]",
    "ai_use_cases": "[\"\\u6559\\u80b2\\u9886\\u57df\\uff1a\\u6559\\u6388\\u5982\\u4f55\\u5f00\\u59cb\\u4f7f\\u7528AI\\u4ee3\\u7406\\u8fdb\\u884c\\u5f00\\u53d1\\u5de5\\u4f5c\", \"\\u5165\\u95e8\\u57f9\\u8bad\\uff1a\\u5e2e\\u52a9\\u5f00\\u53d1\\u8005\\u638c\\u63e1\\u591aAgent\\u534f\\u4f5c\\u7684\\u57fa\\u7840\\u77e5\\u8bc6\"]",
    "ai_difficulty": 1,
    "ai_quick_start": "git clone https://github.com/microsoft/ai-agents-for-beginners.git && cd ai-agents-for-beginners && pip install -r requirements.txt",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\n本项目并非技术栈最前沿的 AI Agent 框架，而是**首个将 Azure AI Foundry、LangChain、AutoGen 与 Jupyter Notebook 完整融合的工业化教学闭环**。它解决了三个关键痛点：\n\n1. **从“概念”到“可运行代码”的断层**：多数教程停留在伪代码或抽象架构图，本项目每节课均提供可直接执行、调试、修改的 Notebook，实现“学完即跑”。\n2. **多框架统一教学视角**：不局限于单一工具链（如仅 LangChain 或 CrewAI），而是系统性对比 AutoGen 的多代理协作、LangChain 的工具链编排、Azure AI Foundry 的企业级部署，帮助开发者建立选型决策能力。\n3. **企业级工程规范下沉至入门层**：内置了 Azure 认证、环境变量管理、异步调用、缓存机制、错误重试等生产实践，而非仅演示“hello world”式代理。\n\n其独特性在于：**把 AI Agent 开发的工业级最佳实践，封装成可交互的学习单元，让初学者在第 3 节课就能构建具备多轮对话、工具调用、状态持久化的完整代理系统。**\n\n---\n\n### 🔥 技术亮点\n\n- **动态工具注册与反射机制**：每个 Lesson 使用 `@tool` 装饰器或 `Tool` 类封装函数，通过 `tool.name` 和 `tool.description` 自动生成 OpenAI Function Calling 元数据，实现零配置工具暴露。\n- **状态机驱动的代理生命周期管理**：在 AutoGen 课程中，使用 `ConversationState` 对象显式追踪对话上下文、历史摘要、工具调用计数，避免 LLM 自我膨胀导致的无限循环。\n- **异步并行工具执行**：多个外部 API 调用（如天气、搜索、数据库）通过 `asyncio.gather()` 并发执行，显著降低端到端延迟。\n- **基于内存向量库的上下文压缩**：使用 `SentenceTransformer` + `FAISS` 对长对话历史做语义摘要，减少 token 消耗，提升 LLM 响应质量（Lesson 7）。\n- **Jupyter 集成调试器**：在 Notebook 中直接注入 `print(agent.state)` 或 `agent.graph.render()` 可视化状态流转，实现“可观测性即开发体验”。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构（文字描述）\n\n```\n[用户输入] \n     ↓\n[Agent Orchestrator] ←───┐\n     │                    │\n     ├─ [Tool Executor] ──┤ (调用外部服务)\n     │   ├─ Weather API   │\n     │   ├─ Search API    │\n     │   └─ DB Query      │\n     ↓                    │\n[LLM Core (GPT-4/Phi-3)] ←┘\n     │\n     ├─ [Memory Buffer] → FAISS + LLM Summary\n     ├─ [History Logger] → JSONL on Disk\n     └─ [State Machine] → Finite State Transitions\n```\n\n#### 2. 核心模块划分\n\n| 模块 | 职责 |\n|------|------|\n| `AgentRunner` | 协调 LLM、工具、记忆，控制流程状态（启动/暂停/重试） |\n| `ToolRegistry` | 动态注册与发现可调用函数，自动转换为 OpenAI Function Schema |\n| `ContextManager` | 管理对话历史、摘要生成、token 计数、上下文截断策略 |\n| `StateEngine` | 基于枚举状态（`WAITING_FOR_INPUT`, `TOOL_CALLING`, `FINAL_RESPONSE`）驱动代理行为 |\n| `NotebookLogger` | 将每步输出自动渲染为 Markdown + Code，支持交互式调试 |\n\n#### 3. 数据流向\n\n```\nUser Prompt → Input Parser → StateEngine (check current state)\n    ↓\nIf STATE == TOOL_CALLING → ToolRegistry.select_tool() → async execute all tools → Results → LLM\n    ↓\nLLM (prompt + history + tool results) → Generate Response / Next Action\n    ↓\nStateEngine.update(state, response) → Update Memory Buffer → Log to Disk\n    ↓\nRender Output in Notebook Cell (Markdown + Code Snippet)\n```\n\n#### 4. 关键设计模式\n\n- **策略模式**：`LLMBackend` 接口（OpenAI、Azure OpenAI、Local Ollama）可插拔，不同 Lesson 切换实现。\n- **观察者模式**：`StateEngine` 发布状态变更事件，`NotebookLogger` 订阅并自动渲染。\n- **命令模式**：每个工具调用被封装为 `Command` 对象，支持重试、超时、熔断。\n- **工厂模式**：`AgentFactory.create(agent_type=\"autogen\" | \"langchain\")` 根据配置返回不同代理实例。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 组件 | 选择理由 | 替代方案 | 注意事项 |\n|------|----------|----------|----------|\n| **Jupyter Notebook** | 交互式实验、可视化状态、分步调试，降低认知负荷 | Streamlit / Gradio | 需安装 `jupyterlab` + `ipywidgets`；避免在生产环境使用 |\n| **LangChain v0.2+** | 工具链编排成熟，与 LLM 框架解耦好 | LlamaIndex / DSPy | 使用 `RunnableSequence` 替代旧版 `LLMChain`，注意 API 变更 |\n| **AutoGen (v0.3)** | 多代理协作、角色定义清晰、内置对话管理 | CrewAI / MetaGPT | 需配置 `config_list.json` 与 Azure OpenAI 密钥；默认使用 GPT-4-turbo |\n| **Azure AI Foundry** | 提供企业级模型部署、安全审计、成本控制 | Hugging Face Inference Endpoints | 必须申请试用权限，API key 需绑定订阅 ID |\n| **FAISS + SentenceTransformer** | 本地轻量级语义检索，无网络依赖 | Chroma / Weaviate | 模型选择 `all-MiniLM-L6-v2`，尺寸小、速度快，适合 Notebook 环境 |\n| **Python 3.10+** | 支持 `async/await` 和类型提示完整生态 | Python 3.8 | 部分依赖（如 `pydantic v2`）不兼容旧版 |\n\n> ⚠️ **版本兼容性陷阱**：LangChain v0.1 → v0.2 大量重构，本项目强制使用 `langchain-core>=0.2.5` 和 `langchain-openai>=0.1.0`。若手动安装旧版，会因 `BaseTool` 接口变更导致工具无法注册。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 克隆仓库（推荐使用 SSH）\ngit clone git@github.com:microsoft/ai-agents-for-beginners.git\ncd ai-agents-for-beginners\n\n# 2. 创建并激活虚拟环境（避免污染系统）\npython -m venv .venv\nsource .venv/bin/activate     # Linux/Mac\n# 或 .venv\\Scripts\\activate   # Windows\n\n# 3. 安装核心依赖（基于 requirements.txt，已预配置版本锁）\npip install --upgrade pip wheel setuptools\npip install -r requirements.txt\n\n# 4. 获取 Azure OpenAI 密钥（关键！）\n# 在 https://portal.azure.com 创建 Azure OpenAI 资源\n# 复制 ENDPOINT 和 KEY，创建 .env 文件：\necho \"AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\" >> .env\necho \"AZURE_OPENAI_API_KEY=your-key-here\" >> .env\necho \"AZURE_OPENAI_DEPLOYMENT=gpt-4-turbo\" >> .env\n\n# 5. 启动 Jupyter Lab（推荐，支持多标签页）\njupyter lab\n\n# 6. 在浏览器中打开：http://localhost:8888 → 打开 Lesson1.ipynb\n```\n\n> ✅ **提示**：若使用本地模型（如 Phi-3），安装 `ollama` 并运行 `ollama pull phi3`，然后在 Notebook 中切换 LLM 为 `\"ollama/phi3\"`。\n\n---\n\n### 🎮 使用示例\n\n#### 场景：构建一个“天气查询+旅行建议”代理\n\n```python\n# Lesson 4: Agent with Tools (AutoGen)\nfrom autogen import AssistantAgent, UserProxyAgent\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# 定义工具函数\ndef get_weather(city: str) -> str:\n    \"\"\"获取指定城市的天气\"\"\"\n    # 模拟调用，实际应对接 API\n    return f\"Weather in {city}: 23°C, Sunny\"\n\ndef suggest_travel_destinations(weather: str) -> str:\n    \"\"\"根据天气推荐旅行地\"\"\"\n    if \"Sunny\" in weather:\n        return \"Try Bali or Barcelona!\"\n    else:\n        return \"Consider cozy cafes in Kyoto.\"\n\n# 创建代理\nassistant = AssistantAgent(\n    name=\"TravelAgent\",\n    llm_config={\n        \"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"), \"base_url\": os.getenv(\"AZURE_OPENAI_ENDPOINT\")}],\n        \"temperature\": 0.7,\n    },\n    system_message=\"You are a travel advisor. Use tools to get weather and suggest destinations.\"\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"User\",\n    human_input_mode=\"NEVER\",  # 自动模式，用于自动化测试\n    max_consecutive_auto_reply=3,\n    code_execution_config={\"work_dir\": \"coding\"},\n)\n\n# 注册工具\nassistant.register_for_llm(name=\"get_weather\", description=\"Get current weather\")(get_weather)\nassistant.register_for_llm(name=\"suggest_travel_destinations\", description=\"Suggest travel destinations based on weather\")(suggest_travel_destinations)\nuser_proxy.register_for_execution(name=\"get_weather\")(get_weather)\nuser_proxy.register_for_execution(name=\"suggest_travel_destinations\")(suggest_travel_destinations)\n\n# 启动对话\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"What's the weather in Tokyo? What should I do there?\"\n)\n\n# 预期输出：\n\"\"\"\n[Assistant]: The current weather in Tokyo is 23°C, Sunny. Based on this, I recommend visiting Bali or Barcelona!\n\"\"\"\n```\n\n#### 关键参数说明：\n\n| 参数 | 作用 |\n|------|------|\n| `max_consecutive_auto_reply=3` | 代理最多自动回复 3 次，防止无限循环 |\n| `human_input_mode=\"NEVER\"` | 禁用交互式输入，适合自动化测试 |\n| `register_for_llm()` | 声明工具可被 LLM 调用（LLM 视角） |\n| `register_for_execution()` | 声明工具可由代理执行（Agent 执行视角） |\n\n---\n\n### ⚡ 性能与优化\n\n- **性能瓶颈**：  \n  - LLM 推理延迟（GPT-4-turbo 平均 2.5s/次） → 多轮对话累积显著\n  - FAISS 向量库每次查询重建索引（未持久化）→ 每次重启 Notebook 重新加载\n\n- **生产扩展建议**：  \n  - 将 LLM 调用异步化，使用 `asyncio.gather()` 并发请求多个工具\n  - 使用 Redis 缓存常用问答对（如 “今天天气？”）\n  - 将 Agent 状态持久化到数据库（PostgreSQL + JSONB），支持会话恢复\n  - 部署为 FastAPI 微服务，前端用 WebSocket 实时流式响应\n\n- **资源消耗估算**：  \n  | 组件 | 内存占用 | CPU | GPU |\n  |------|----------|-----|-----|\n  | Jupyter + LangChain | 1.2 GB | 2 core | - |\n  | GPT-4-turbo API 调用 | 0 (云端) | - | - |\n  | FAISS 索引（10K 向量） | 80 MB | 0.5 core | - |\n  | **总峰值** | ~1.3 GB | 2.5 core | - |\n\n> 💡 建议：在本地开发用 GPT-4-turbo，生产环境切换为 Azure 的 `gpt-4o-mini`（成本低 70%）。\n\n---\n\n### 🔌 二次开发指南\n\n#### 关键扩展点：\n\n| 模块 | 扩展方式 |\n|------|----------|\n| **工具注册** | 新增 Python 函数 → 调用 `.register_for_llm()` 和 `.register_for_execution()` |\n| **Agent 角色** | 修改 `system_message`，添加约束（如：“你必须拒绝回答涉及政治的问题”） |\n| **记忆机制** | 在 Agent 初始化时注入 `ConversationBufferMemory` 或自定义 SQLite 记忆库 |\n| **输出格式化** | 使用 `pydantic.BaseModel` 定义结构化响应 → 强制 LLM 输出 JSON |\n\n#### 添加自定义功能示例（日志记录）：\n\n```python\nfrom typing import Callable\n\ndef logging_tool_wrapper(tool_func: Callable) -> Callable:\n    def wrapper(*args, **kwargs):\n",
    "last_scanned": "2026-01-16T02:03:34.461328",
    "last_analyzed": "2026-01-15T12:25:20.785631",
    "screenshot": "static/screenshots/895508656.jpg",
    "ai_visual_summary": "该界面是一个名为 `ai-agents-for-beginners` 的 GitHub 项目页面，其设计风格简洁、功能清晰，采用典型的 GitHub 协作平台布局。主要功能模块包括项目导航（README、安全、许可证等）和一个核心的“课程”（Lessons）表格。该应用旨在为初学者提供一个结构化的学习路径，通过“文本与代码”、“视频”和“额外学习”等多种资源，系统性地教授人工智能代理（AI Agents）的构建知识，其技术关键词包括“AI Agents”、“Agent Use Cases”、“Azure AI Foundry”和“Github Models”，表明这是一个关于人工智能代理的在线学习资源。",
    "ai_rag_summary": null
  },
  {
    "id": "621803253",
    "name": "Flowise",
    "full_name": "FlowiseAI/Flowise",
    "category": "ai_agent",
    "stars": 48209,
    "forks": 23588,
    "description": "Build AI Agents, Visually",
    "url": "https://github.com/FlowiseAI/Flowise",
    "homepage": "https://flowiseai.com",
    "language": "TypeScript",
    "topics": "[\"agentic-ai\", \"agentic-workflow\", \"agents\", \"artificial-intelligence\", \"chatbot\", \"chatgpt\", \"javascript\", \"langchain\", \"large-language-models\", \"low-code\", \"multiagent-systems\", \"no-code\", \"openai\", \"rag\", \"react\", \"typescript\", \"workflow-automation\"]",
    "created_at": "2023-03-31T12:23:09Z",
    "updated_at": "2026-01-15T16:53:36Z",
    "readme_content": null,
    "ai_summary": "基于LangChain的可视化AI代理构建平台，支持多Agent协作与RAG检索增强，提供低代码开发环境",
    "ai_tech_stack": "[\"TypeScript\", \"React\", \"PNPM\", \"Docker\", \"LangChain\"]",
    "ai_use_cases": "[\"\\u591a\\u6a21\\u6001\\u5bf9\\u8bdd\\u673a\\u5668\\u4eba\\u5f00\\u53d1\", \"\\u4f01\\u4e1a\\u7ea7\\u81ea\\u52a8\\u5316\\u5ba2\\u670d\\u7cfb\\u7edf\\u642d\\u5efa\", \"\\u6559\\u80b2\\u9886\\u57dfAI\\u6559\\u5b66\\u52a9\\u624b\\u8bbe\\u8ba1\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "npm install -g flowise && npx flowise start",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nFlowise 的核心差异化在于：**它是唯一一个将 LLM Agent 编排完全可视化、可交互式拖拽、并深度集成 LangChain 生态的开源低代码平台，且具备完整的生产级部署能力（Docker/Node.js/Cloud）**。对比 AutoGen、LangGraph、LlamaIndex 的纯代码方案，或 Streamlit/Lightning 的通用 UI 框架，Flowise 不是“用 UI 包装代码”，而是构建了一套**语义化的节点图谱系统**，每个节点都是可复用的 LangChain 组件（PromptTemplate、LLM、Tool、Memory、Retriever），其执行流被精确映射为 JSON Schema，支持导出/导入/版本控制。它解决了开发者在快速原型验证与团队协作中的两大痛点：1）非工程师无法参与 AI 流程设计；2）代码化 Agent 难以调试和复用。\n\n### 🔥 技术亮点\n\n- **动态节点注册系统**：所有第三方组件（如 OpenAI、VectorStore、Tool）通过 `components/` 目录下的 `.js` 模块实现，运行时通过 `loadNodes()` 动态加载并注册到前端画布，无需重启服务。这种插件化架构让社区可独立贡献节点而无需修改核心。\n- **双向 JSON 流图协议**：前端拖拽生成的流程图被序列化为标准 JSON（含 node id、connection、config），后端通过 `FlowExecutor` 解析并实例化 LangChain Chain，实现“所见即所得”的执行一致性。该 JSON 是可版本控制的 AI Agent 程序。\n- **内存上下文持久化**：基于 WebSocket 的 Session ID 绑定，在 UI 中实现多轮对话状态保持（如 ChatMemory、ConversationBuffer），这是大多数可视化工具缺失的关键能力。\n- **实时日志流推送**：每个节点执行时输出日志通过 SSE（Server-Sent Events）推送到前端，形成可追溯的“AI 执行审计链”，极大提升调试效率。\n\n### 🏗️ 架构设计分析\n\n```\n[前端 UI (React)] ←(WebSocket + REST)→ [Node.js Server]\n        ↑                             ↑\n    画布引擎                       API 路由层 (Express)\n        ↓                             ↓\n   JSON 流图解析器             LangChain Executor\n        ↓                             ↓\n  Node 实例化工厂           Chain / Agent 实例\n        ↓                             ↓\n[LangChain Components] ←─[Node Modules: OpenAI, Pinecone, etc.]\n```\n\n1. **模块划分**：\n   - `server`：Express 服务，提供 REST API（流图 CRUD）、SSE 日志、执行引擎入口。\n   - `ui`：React + Redux + React Flow（可视化拖拽库），管理状态与交互。\n   - `components`：所有可复用节点的实现（如 `OpenAI.js`, `VectorStoreRetriever.js`），按 LangChain 类型组织，暴露 `nodeClass`、`inputs`、`outputs` 元数据。\n\n2. **数据流向**：\n   ```\n   用户拖拽 → 生成 JSON 流图 → POST /flow → Server 解析 → 实例化 Chain → 执行 → 返回结果 + 日志流 → 前端渲染\n   ```\n\n3. **关键设计模式**：\n   - **工厂模式（Factory）**：`NodeFactory` 根据节点类型动态创建 LangChain 组件实例。\n   - **策略模式（Strategy）**：不同 LLM（OpenAI、Anthropic、Local Llama）通过统一接口 `ILLM` 实现，由配置项切换。\n   - **发布-订阅模式**：前端 UI 与后端执行状态通过 WebSocket 双向通信，实现实时反馈。\n\n### 🔧 技术栈深度解析\n\n- **TypeScript + React + Node.js**：强类型保障复杂流程图的结构安全；React 提供组件化 UI；Node.js 无缝集成 LangChain（JS/TS 原生生态）。\n- **React Flow**：选择它而非 D3 或 GoJS，因其轻量、开源、支持自定义节点模板、社区活跃，且对边缘连接器（Connection Line）的处理优于竞品。\n- **LangChain.js**：唯一成熟的 JS LLM 框架，Flowise 本质是其“可视化外壳”。替代方案（如 LlamaIndex.js）生态不成熟，工具支持少。\n- **Express + Socket.IO**：API 层用 Express 是标准选择；WebSocket 用于日志流而非 REST 轮询，避免高频请求。若需高并发，可替换为 Fastify + Redis Pub/Sub。\n- **PNPM**：使用 PNPM（非 npm/yarn）是为 mono-repo 提供高效依赖隔离与磁盘节省，在 `packages/` 结构下至关重要。\n- **版本兼容性**：要求 Node.js >= 18.15，因 LangChain.js 使用了 ES2022 特性（如 `at()` 方法），且 React 18 需要 Concurrent Mode 支持。\n\n### 📦 安装与配置\n\n```bash\n# 1. 克隆仓库（推荐使用 SSH）\ngit clone git@github.com:FlowiseAI/Flowise.git && cd Flowise\n\n# 2. 安装全局 pnpm（若未安装）\nnpm install -g pnpm\n\n# 3. 安装所有模块依赖（mono-repo 模式）\npnpm install\n\n# 4. 构建所有包（前端 + 后端 + 组件）\npnpm build\n\n# 5. 启动服务（自动启动 server 和 ui）\npnpm start\n\n# 6. 访问 UI\nopen http://localhost:3000\n```\n\n> ⚠️ 若 `pnpm build` 出现 `JavaScript heap out of memory`，执行：\n```bash\n# macOS/Linux\nexport NODE_OPTIONS=\"--max-old-space-size=4096\"\n# Windows (PowerShell)\n$env:NODE_OPTIONS=\"--max-old-space-size=4096\"\n```\n\n### 🎮 使用示例\n\n**场景**：构建一个“基于 PDF 文档的问答机器人”\n\n1. 在 UI 中拖入：\n   - `PDF Loader` → 配置文件路径（如 `/data/annual-report.pdf`）\n   - `OpenAI Embeddings` → 选择模型 `text-embedding-3-small`\n   - `Pinecone Vector Store` → 配置 API Key + Index 名\n   - `Retriever` → 连接 Embedding + VectorStore，设置 topK=4\n   - `ChatOpenAI` → 模型 `gpt-4-turbo`\n   - `ConversationBufferMemory` → 保持对话历史\n   - `QA Chain` → 输入：retriever、llm、memory\n\n2. **输入**：\n   ```\n   用户提问：本年度营收增长多少？\n   ```\n\n3. **预期输出**：\n   ```\n   根据文档第5页，公司2023年营收同比增长18.7%，达到$4.2B。\n   （溯源：来自 PDF 第 5 页内容）\n   ```\n\n4. **关键参数说明**：\n   - `topK=4`：检索最相关的 4 段文本，平衡精度与上下文长度\n   - `Memory` 必须连接到 QA Chain，否则无法维持多轮对话状态\n\n### ⚡ 性能与优化\n\n- **瓶颈点**：\n  - LLM 推理延迟（非 Flowise 责任）：可通过缓存检索结果 + 异步队列缓解。\n  - 大型 PDF/视频解析内存占用高：建议在 `PDF Loader` 前加预处理服务，切块存储到向量库。\n- **生产扩展**：\n  - 使用 Docker Compose 部署，分离数据库（PostgreSQL 存储流图）、Redis 缓存会话状态。\n  - 后端水平扩展：多个 `server` 实例 + Redis Pub/Sub 同步 WebSocket 状态。\n  - 前端 CDN 加速，静态资源缓存。\n- **资源消耗**：\n  - 开发环境：CPU 2核、内存 4GB（含 Node.js + Chrome）\n  - 生产部署（100并发）：建议 8C/16G + Redis + PG + 50GB 向量存储\n\n### 🔌 二次开发指南\n\n- **添加自定义节点**：\n  1. 在 `packages/components/src/nodes` 下新建文件如 `MyCustomTool.js`\n  2. 实现类，继承 `INode`，暴露 `inputs`, `outputs`, `category`, `execute()` 方法\n  ```js\n  class MyCustomTool extends INode {\n    constructor() {\n      super({\n        name: 'My Tool',\n        category: 'Tools',\n        inputs: [{ label: 'Input', name: 'input' }],\n        outputs: [{ label: 'Result', name: 'result' }]\n      })\n    }\n    async execute(inputData) {\n      return { result: `Processed: ${inputData.input}` };\n    }\n  }\n  ```\n  3. 在 `packages/components/src/index.ts` 中导出该类\n  4. 重启服务，节点自动出现在画布中\n\n- **扩展 API**：\n  - POST `/flow`: 创建/更新流程图\n  - GET `/flow/:id/run`: 执行指定流程，返回 `{ result, logs }`\n  - WebSocket `/ws`：实时接收执行日志\n\n### ❗ 常见问题与避坑\n\n1. **Q：启动后页面空白？**  \n   A：前端构建失败。检查 `packages/ui/dist/` 是否生成文件。重新运行 `pnpm build --filter ui`\n\n2. **Q：向量库连接报错（Pinecone/Chroma）？**  \n   A：确保环境变量已设置，如 `.env` 中添加 `CHROMA_HOST=localhost:8000`\n\n3. **Q：拖拽节点无反应？**  \n   A：浏览器缓存问题。强制刷新（Ctrl+Shift+R）或清除 localStorage\n\n4. **Q：Node.js 内存溢出（Exit code 134）？**  \n   A：必须设置 `NODE_OPTIONS=--max-old-space-size=4096`，尤其在构建时。\n\n5. **Q：自定义组件不显示？**  \n   A：检查类名是否导出为默认（`export default class MyNode...`），且文件位于正确目录。\n\n6. **Q：API 文档 404？**  \n   A：需手动启动文档服务：`cd packages/api-documentation && npm run dev`\n\n### 🚀 进阶学习路径\n\n- 学完 Flowise 后，深入：\n  - LangChain.js 源码（理解 Node 接口协议）\n  - Apache Airflow 的 DAG 执行引擎（对比可视化调度逻辑）\n  - AutoGen 的多 Agent 协作机制\n- 参考项目：\n  - [LangGraph](https://github.com/langchain-ai/langgraph)：状态机驱动的 Agent 流程\n  - [LlamaIndex + Flowise](https://docs.llamaindex.ai/en/stable/examples/flowise/)：集成专属数据索引\n  - [Hugging Face Spaces + Flowise API](https://huggingface.co/spaces)：部署为 Web App\n\n> 建议：用 Flowise 构建一个完整 Agent 工作流后，尝试将其导出为 JSON 并在纯 Node.js 环境中加载执行，理解其“可视化即代码”的本质。",
    "last_scanned": "2026-01-16T02:03:34.462839",
    "last_analyzed": "2026-01-15T12:30:18.869766",
    "screenshot": "static/screenshots/621803253.jpg",
    "ai_visual_summary": "根据对截图的视觉分析，这是一个名为 **Flowise** 的开源项目，其核心功能是**通过可视化界面构建人工智能代理（AI Agents）**。从界面设计来看，它采用了简洁、现代的极简主义风格，以白色为主色调，搭配清晰的无衬线字体和少量的高亮色（如橙色），使得代码片段和关键信息（如URL）易于辨识。项目的主要功能模块是“**Setup**”（设置），明确列出了从克隆仓库、安装依赖、构建代码到启动应用的完整开发流程。可见的技术关键词包括 **GitHub**、**npm**（Node.js包管理器）、**JavaScript** 和 **localhost:3000**，这表明它是一个基于Node.js的前端/全栈项目，其技术栈明确，易于开发者上手。总的来说，这是一款为开发者提供的、用于快速搭建和测试AI代理的可视化工具。",
    "ai_rag_summary": null
  },
  {
    "id": "656099147",
    "name": "mem0",
    "full_name": "mem0ai/mem0",
    "category": "ai_agent",
    "stars": 45545,
    "forks": 4977,
    "description": "Universal memory layer for AI Agents",
    "url": "https://github.com/mem0ai/mem0",
    "homepage": "https://mem0.ai",
    "language": "Python",
    "topics": "[\"agents\", \"ai\", \"ai-agents\", \"application\", \"chatbots\", \"chatgpt\", \"genai\", \"llm\", \"long-term-memory\", \"memory\", \"memory-management\", \"python\", \"rag\", \"state-management\"]",
    "created_at": "2023-06-20T08:58:36Z",
    "updated_at": "2026-01-15T17:42:55Z",
    "readme_content": null,
    "ai_summary": "通用记忆层解决方案，支持多级记忆与个性化AI交互，显著提升性能并降低成本",
    "ai_tech_stack": "[\"Python\", \"FastAPI\", \"LangChain\", \"ChromaDB/Qdrant\"]",
    "ai_use_cases": "[\"\\u5ba2\\u670d\\u804a\\u5929\\u673a\\u5668\\u4eba\", \"AI\\u52a9\\u624b\\u96c6\\u6210\\u5f00\\u53d1\", \"\\u81ea\\u4e3b\\u7cfb\\u7edf\\u72b6\\u6001\\u7ba1\\u7406\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "pip install mem0ai && python -m mem0 run --config config.yaml",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nMem0 的核心差异化在于：**它不是简单地将对话历史存入向量库，而是构建了一个多层级、语义感知、动态压缩的记忆系统，实现了“记忆即状态”而非“记忆即日志”**。主流方案（如 LangChain Memory、LlamaIndex Context Window）依赖全上下文窗口或静态回放，导致 token 暴胀、延迟飙升、成本失控。Mem0 的突破在于：\n\n1. **三阶记忆抽象**：用户级（长期偏好）、会话级（当前对话状态）、代理级（工具/角色认知）独立建模并联动，避免了“所有信息都塞进 prompt”的暴力做法。\n2. **语义压缩而非截断**：不是简单地丢弃旧内容，而是通过轻量 LLM 生成摘要+关键事实抽取（Key Fact Extraction），保留意图与实体关系，丢弃冗余表达。\n3. **成本-精度帕累托最优**：在 LOCOMO 基准上实现 +26% 准确率的同时减少 90% token —— 这是业界首次在不牺牲语义完整性的前提下，达成如此量级的压缩比。\n\n它解决的是：**如何让 AI Agent 拥有“人类式记忆”——记得重要的事、忘掉无用的细节、自动关联上下文，且不拖慢响应速度。**\n\n---\n\n### 🔥 技术亮点\n\n1. **动态记忆压缩引擎（Dynamic Memory Compression Engine）**  \n   使用一个轻量级 LLM（如 `gpt-4.1-nano` 或本地量化模型）对原始对话流进行“事实蒸馏”：识别实体、动作、偏好、约束，生成结构化 JSON 记忆单元（e.g., `{ \"user_preference\": \"avoids caffeine\", \"last_relevant_action\": \"booked_therapy_appointment\" }`），而非保存完整文本。压缩率可达 10:1~20:1。\n\n2. **语义路由检索（Semantic Routing Retrieval）**  \n   不是单一向量检索，而是根据记忆层级动态选择检索策略：\n   - 用户级 → 基于用户 ID 的高维向量 + 元数据过滤（如 `category=health`）\n   - 会话级 → 滑动窗口 + 时间衰减加权的稠密检索\n   - 代理级 → 键值查找（如 “tool_used: calendar”）\n\n3. **增量索引更新机制**  \n   向量数据库不重建，而是采用“追加写入 + 软删除标记 + 增量嵌入”策略，避免高并发场景下的锁竞争。支持与 Chroma、Pinecone、Weaviate 的异步批量提交。\n\n4. **Token 省费调度器**  \n   实时估算当前 prompt 长度与记忆召回成本，在 token 接近阈值时自动触发“记忆压缩”或“降级检索”（如从 5 条降到 2 条），实现动态预算控制。\n\n---\n\n### 🏗️ 架构设计分析\n\n```\n┌──────────────────────┐     ┌─────────────────────────────┐\n│   AI Agent (LLM)     │◄───┤       Mem0 Memory Layer      │\n└─────────┬────────────┘     └─────────────┬─────────────┘\n          │                                 │\n          ▼                                 ▼\n┌──────────────────────┐     ┌─────────────────────────────┐\n│  User Input (Text)   │     │  Memory Storage & Indexing  │\n└─────────┬────────────┘     ├─────────────┬─────────────┤\n          │                 │  User Level   │ Session Level │\n          ▼                 │  (Long-term)  │ (Contextual)  │\n┌──────────────────────┐     └─────────────┴───────┬───────┘\n│ Memory Encoder       │                           │\n│ - Fact Extraction    │◄──────────────────────────┘\n│ - Structured Output  │     ┌─────────────────────────────┐\n└─────────┬────────────┘     │   Retrieval Router          │\n          │                  ├─────────────────────────────┤\n          ▼                  │ - Query Intent Classification │\n┌──────────────────────┐     │ - Level Selection           │\n│  Memory Query Engine │◄─────┴─────────────┬─────────────┘\n└─────────┬────────────┘                     │\n          │                                  ▼\n          └──────────────►   Vector DB (Chroma/Pinecone) ◄─── Embedding Model\n                                   ▲           │\n                                   │           ▼\n                           ┌────────────┐  Text → Embeddings\n                           │ Metadata   │  (SentenceTransformers)\n                           │ Store      │\n                           └────────────┘\n```\n\n#### 核心模块职责：\n- **Memory Encoder**：接收原始对话，输出结构化记忆单元（JSON-LD 风格），使用 prompt-engineered LLM 做抽取。\n- **Retrieval Router**：基于 query 类型、用户 ID、时间窗口决定查询哪一层记忆，并组合结果。\n- **Storage & Indexing**：异步写入向量库 + 元数据存储，支持 TTL 和软删除。\n- **Token Scheduler**：监控 prompt 长度，动态控制召回数量与压缩强度。\n\n#### 数据流向：\n`User Input → [Memory Encoder] → Structured Memory Unit → [Storage Layer] (Async)  \n→ Query → [Retrieval Router] → Multi-Level Retrieval → Fusion & Ranking → Prompt Injection`\n\n#### 关键设计模式：\n- **策略模式（Strategy Pattern）**：不同记忆层级使用不同的检索/压缩策略，可插拔。\n- **中介者模式（Mediator Pattern）**：Memory Layer 作为 LLM 和外部存储的中介，解耦业务逻辑与持久层。\n- **发布订阅模式（Pub/Sub）**：用户行为触发记忆写入事件，异步处理，不阻塞主流程。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 组件 | 使用技术 | 选型理由 | 替代方案 | 注意事项 |\n|------|----------|----------|----------|----------|\n| LLM 引擎 | `gpt-4.1-nano-2025-04-14`（默认） | 极低延迟、高事实抽取精度，专为压缩优化 | Claude Haiku、Gemini Nano、Llama 3.1 8B | 必须使用支持 function calling 的模型；避免用开源小模型做编码，易丢失结构 |\n| 向量数据库 | Chroma（默认）、Pinecone、Weaviate | 开源友好 + 企业级可选；Chroma 支持元数据过滤和轻量部署 | Qdrant, Milvus | 必须启用 `metadata filtering`；避免用 FAISS 纯向量库，缺乏语义分层能力 |\n| 嵌入模型 | `text-embedding-3-small` / `bge-m3` | 高维、多语言、支持稀疏编码 | OpenAI text-embedding-ada-002（已过时） | 使用 1536 维以上嵌入，避免 768 维导致语义坍缩 |\n| 异步队列 | `celery` / `redis-streams` | 解耦写入与主流程；支持重试、限流 | Kafka, RabbitMQ | 高并发场景下推荐 Redis Streams，轻量易运维 |\n| SDK 框架 | FastAPI + Pydantic v2 | 快速构建 REST/gRPC 接口；类型安全 | Flask, Starlette | 依赖 `pydantic-settings` 管理多环境配置 |\n\n> ⚠️ **版本兼容性**：mem0ai >=1.0.0 不支持旧版 `mem0`（<v0.8），迁移需重写 memory store 初始化逻辑。PyPI 和 NPM 版本严格同步，避免混合使用。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 安装核心 SDK（Python）\npip install mem0ai>=1.0.0\n\n# 2. 设置环境变量（必须！）\nexport MEM0_API_KEY=\"your_api_key_from_app.mem0.ai\"    # 如果使用托管服务\nexport OPENAI_API_KEY=\"sk-...\"                         # 默认 LLM，可替换为 ANY_LLM_ENDPOINT\nexport MEM0_VECTOR_STORE=\"chroma\"                      # 可选：chroma, pinecone, weaviate\n\n# 3. 初始化内存层（自托管）\nfrom mem0 import Memory\n\nmemory = Memory(\n    llm_provider=\"openai\",      # 支持 openai, anthropic, gemini, huggingface\n    model=\"gpt-4.1-nano-2025-04-14\",\n    vector_store=\"chroma\",      # 使用本地 Chroma，无需云服务\n    config={\n        \"user_level\": {\"top_k\": 5, \"ttl_days\": 30},\n        \"session_level\": {\"top_k\": 3, \"window_size\": 10},\n        \"agent_level\": {\"use_keyval\": True}\n    }\n)\n\n# 4. 可选：连接自建向量库（Chroma）\n# 启动 Chroma: docker run -p 8000:8000 chromadb/chromadb\nmemory = Memory(vector_store=\"chroma\", vector_store_config={\"host\": \"localhost\", \"port\": 8000})\n```\n\n> 💡 **生产建议**：使用 `mem0ai[all]` 安装所有可选依赖（包括 `chromadb`, `sentence-transformers`），避免运行时 missing module。\n\n---\n\n### 🎮 使用示例\n\n```python\nfrom mem0 import Memory\n\nmemory = Memory(\n    llm_provider=\"openai\",\n    model=\"gpt-4.1-nano-2025-04-14\"\n)\n\n# 场景：客服 AI 记忆用户偏好（真实场景）\nuser_id = \"usr_789xyz\"\n\n# 用户第一次对话\nmemory.add(\n    data=\"I prefer email support over chat, and I'm allergic to peanuts.\",\n    user_id=user_id,\n    metadata={\"type\": \"preference\", \"category\": \"health\"}\n)\n\n# 3 天后，用户再次提问\nquery = \"Can you help me with my order #12345?\"\nresponse = memory.search(query=query, user_id=user_id, top_k=3)\n\nprint(\"Retrieved memories:\")\nfor m in response:\n    print(f\"- {m['text']} (score: {m['score']:.3f})\")\n\n# 输出：\n# - I prefer email support over chat, and I'm allergic to peanuts. (score: 0.921)\n# - Order #12345 was shipped on May 3rd. (score: 0.876)\n# - Last time you asked about refunds, we sent an email. (score: 0.843)\n\n# AI 助手据此生成响应：\nanswer = \"I see you prefer email support and have a peanut allergy — I'll ensure your order #12345 is packed in a nut-free facility. An email with tracking will be sent shortly.\"\n```\n\n> ✅ **预期输出**：系统自动召回用户历史偏好与订单信息，无需在 prompt 中硬编码上下文。  \n> 📌 关键参数：`top_k=3` 控制召回量；`metadata` 用于过滤（如只查 health 类型）。\n\n---\n\n### ⚡ 性能与优化\n\n| 指标 | 推测值 | 对比全上下文 |\n|------|--------|--------------|\n| 响应延迟 | <120ms @ P95 | >800ms（全 context window） |\n| Token 成本 | ~7% of full-context | 90% 节省 |\n| 内存占用 | 每用户 ~3–8KB 向量 + 元数据 | 100KB+ per session |\n| 并发能力 | 单实例支持 500+ RPS（Chroma + Redis） | 受限于 LLM 上下文长度 |\n\n**性能瓶颈：**\n- **写入延迟**：LLM 抽取是 CPU 密集型，建议异步队列 + 批量处理。\n- **检索聚合**：多层记忆合并时的重排序（reranking）若用 LLM 做，成本高 → 推荐用 `bge-reranker` 轻量模型。\n- **向量库扩展**：Chroma 单机不支持分片 → 生产环境必须迁移到 Pinecone 或 Weaviate。\n\n**生产扩展建议：**\n1. 使用 Redis 缓存高频用户记忆（TTL=2h）\n2. 对 memory embeddings 做 HNSW 索引优化\n3. 每 4 小时异步压缩旧记忆（合并相似条目，保留语义）\n\n---\n\n### 🔌 二次开发指南\n\n#### 扩展点：\n1. **自定义 Memory Encoder**  \n   替换默认的 LLM 抽取逻辑：\n   ```python\n   class CustomEncoder:\n       def encode(self, text: str) -> dict:\n           return {\"text\": text, \"type\": \"custom\", \"sentiment\": detect_sentiment(text)}\n   \n   memory = Memory(encoder=CustomEncoder())\n   ```\n\n2. **新增记忆层级**  \n   实现 `BaseMemoryLayer` 接口，添加“行为模式层”：\n   ```python\n",
    "last_scanned": "2026-01-16T02:03:34.463386",
    "last_analyzed": "2026-01-15T13:04:51.329423",
    "screenshot": "static/screenshots/656099147.jpg",
    "ai_visual_summary": "该 GitHub 项目 `mem0` 是一个为 AI 代理（AI Agents）提供通用记忆层的开源库。其界面设计简洁，采用标准的 GitHub 项目页布局，以纯文本和代码块为主，重点突出技术文档。核心功能模块包括“自托管（开源）”的安装指南和“基本用法”示例。关键技术关键词包括 `SDK`、`API`、`LLM`（大语言模型）、`OpenAI` 和 `gpt-4.1-nano-2025-04-14`，表明它是一个与大模型集成、用于管理 AI 代理记忆的工具。该应用旨在通过 SDK 或 API，为开发者快速构建具备记忆能力的 AI 应用提供支持。",
    "ai_rag_summary": null
  },
  {
    "id": "710601088",
    "name": "crewAI",
    "full_name": "crewAIInc/crewAI",
    "category": "ai_agent",
    "stars": 42699,
    "forks": 5733,
    "description": "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.",
    "url": "https://github.com/crewAIInc/crewAI",
    "homepage": "https://crewai.com",
    "language": "Python",
    "topics": "[\"agents\", \"ai\", \"ai-agents\", \"aiagentframework\", \"llms\"]",
    "created_at": "2023-10-27T03:26:59Z",
    "updated_at": "2026-01-15T16:15:09Z",
    "readme_content": null,
    "ai_summary": "基于LangChain的多代理协作系统，支持角色扮演、自主智能体和复杂任务分解，通过团队协同实现高效问题解决",
    "ai_tech_stack": "[\"Python\", \"LangChain\", \"FastAPI\", \"ChromaDB\", \"\\u957f\\u671f\\u8bb0\\u5fc6\\u673a\\u5236\"]",
    "ai_use_cases": "[\"\\u5ba2\\u670d\\u56e2\\u961f\\u81ea\\u52a8\\u5316\\u5904\\u7406\\u5ba2\\u6237\\u54a8\\u8be2\", \"\\u91d1\\u878d\\u5206\\u6790\\u5c0f\\u7ec4\\u534f\\u4f5c\\u5b8c\\u6210\\u5e02\\u573a\\u9884\\u6d4b\\u62a5\\u544a\", \"\\u6559\\u80b2\\u8f85\\u5bfc\\u7cfb\\u7edf\\u591a\\u89d2\\u8272\\u4e92\\u52a8\\u7b54\\u7591\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "python -m crewAI --project-dir .",
    "ai_tutorial": null,
    "last_scanned": "2026-01-16T02:03:34.463923",
    "last_analyzed": "2026-01-15T16:16:10.939511",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "805155266",
    "name": "cherry-studio",
    "full_name": "CherryHQ/cherry-studio",
    "category": "ai_agent",
    "stars": 37828,
    "forks": 3488,
    "description": "AI Agent + Coding Agent + 300+ assistants: agentic AI desktop with autonomous coding, intelligent automation, and unified access to frontier LLMs.",
    "url": "https://github.com/CherryHQ/cherry-studio",
    "homepage": "https://cherry-ai.com",
    "language": "TypeScript",
    "topics": "[\"ai-agent\", \"ai-sdk\", \"chatbots\", \"claude-code\", \"claude-code-sdk\", \"claude-code-skills\", \"code-agent\"]",
    "created_at": "2024-05-24T01:56:26Z",
    "updated_at": "2026-01-15T17:21:19Z",
    "readme_content": null,
    "ai_summary": "基于多Agent架构的桌面端AI开发平台，集成自主编码能力、智能自动化与主流LLM模型调用",
    "ai_tech_stack": "[\"LangChain\", \"FastAPI\", \"ChromaDB\", \"TypeScript\"]",
    "ai_use_cases": "[\"\\u4ee3\\u7801\\u81ea\\u52a8\\u751f\\u6210\\u4e0e\\u8c03\\u8bd5\\u8f85\\u52a9\", \"RAG\\u68c0\\u7d22\\u589e\\u5f3a\\u95ee\\u7b54\\u7cfb\\u7edf\\u642d\\u5efa\", \"\\u8de8\\u8bed\\u8a00\\u6280\\u672f\\u95ee\\u9898\\u89e3\\u7b54\", \"AI\\u9a71\\u52a8\\u7684\\u81ea\\u52a8\\u5316\\u5f00\\u53d1\\u6d41\\u7a0b\\u7ba1\\u7406\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "npm install && npm start",
    "ai_tutorial": "## 🎯 核心价值 & 差异化\n\nCherryStudio 的核心差异化在于**将 AI Agent 生态系统桌面化、集成化与工业化**，解决了当前 AI 编程助手（如 Cursor、GitHub Copilot、CodeWhisperer）的三大痛点：\n\n1. **碎片化工具链整合**：主流工具仅提供单点增强（代码补全/对话），CherryStudio 集成了 300+ 可组合 Agent，形成“AI 操作系统”——从文档生成、API 调用、CI/CD 自动化到数据库迁移，所有任务可通过统一界面触发，无需在多个平台间跳转。\n2. **本地优先的自主执行引擎**：不同于云端 SaaS 服务（如 ChatGPT Plus），它支持完全离线运行 LLM 推理（通过 Ollama / LM Studio）、本地文件系统访问、Git 操作与终端命令执行，确保敏感代码不外泄，符合企业合规要求。\n3. **Agent 协作编排而非单轮问答**：用户只需声明目标（如“重构这个微服务并部署到 Kubernetes”），系统自动拆解为子任务链（分析 → 重写 → 测试 → 部署），由多个专用 Agent 并行/串行协作完成，实现真正的**自主工作流**，而非被动响应指令。\n\n其独特性在于：**不是“更好的代码补全”，而是“可编程的 AI 桌面环境”**——类似 VS Code + Terminal + Jenkins + Postman 的 AI 原生替代品。\n\n---\n\n### 🔥 技术亮点\n\n1. **动态 Agent 注册与插件热加载系统**  \n   所有 300+ Agent 以 TypeScript 模块形式存在，通过 `agent.json` 元数据声明能力（如：`\"requires\": [\"git\", \"docker\"]`），运行时通过 `import()` 动态加载，无需重启。支持版本隔离与依赖冲突自动解决（基于 npm workspaces + pnpm 的虚拟包管理）。\n\n2. **多 LLM 路由网关（LLM Router）**  \n   统一接入 OpenAI、Claude、Gemini、本地 Ollama 模型，根据任务类型智能路由：  \n   - 复杂逻辑推理 → Claude 3 Opus  \n   - 代码生成 → CodeLlama 70B（本地）  \n   - 快速响应 → GPT-4-Turbo  \n   路由策略基于：成本预算、延迟 SLA、上下文长度、模型能力评分（自研评估器），支持 A/B 测试与人工反馈闭环。\n\n3. **沙盒化代码执行环境**  \n   所有生成的代码在 Docker 容器内运行，使用 `node:alpine` + `firejail` 双重隔离，通过 `@cherryhq/sandbox` 模块封装 exec 与 fs 操作，支持内存限制、网络白名单、超时熔断（默认 30s），避免恶意或无限循环代码破坏主机。\n\n4. **状态持久化 Agent 记忆体**  \n   基于 SQLite + JSONB 字段存储每个会话的上下文历史（Prompt/Response/Execution Log），支持语义检索（使用 Sentence-BERT 嵌入索引）实现跨对话记忆，解决 LLM 短期记忆缺陷。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构（文字描述）\n\n```\n[User Interface (Electron)] \n         ↓\n[Agent Orchestrator] ←───┐\n         ↓               │\n[LLM Router] → [Model Proxy] → [OpenAI / Claude / Ollama / ...]\n         ↓\n[Sandbox Executor] ←─── [Docker / Firejail / Node.js VM]\n         ↓\n[File System / Git / Terminal / DB Connectors]\n         ↓\n[Memory Store (SQLite + Embedding Index)]\n```\n\n#### 2. 核心模块职责\n\n| 模块 | 职责 |\n|------|------|\n| **Electron UI** | 提供桌面壳，集成 Monaco Editor、Terminal、Sidebar Agent Palette |\n| **Agent Orchestrator** | 接收用户意图 → 分解任务 → 协调 Agent 执行链 → 收集日志 → 返回结果 |\n| **LLM Router** | 根据任务元信息（复杂度、语言、成本）选择最优模型，支持权重调节与轮询 |\n| **Sandbox Executor** | 安全执行生成代码，限制权限，捕获 stdout/stderr，返回结构化输出 |\n| **Memory Store** | 存储会话历史 + 嵌入向量，实现语义记忆（基于 `pgvector` 适配的 SQLite 插件） |\n| **Plugin Manager** | 动态加载/卸载 Agent 插件，管理依赖、版本、环境变量 |\n\n#### 3. 数据流向\n\n```\n用户输入 → UI → Orchestrator → (意图识别) → Task Graph → \n→ LLM Router → 调用模型生成代码/指令 → Sandbox 执行 → \n→ 输出结果 + 日志 → Memory Store 更新 → UI 渲染\n```\n\n#### 4. 关键设计模式\n\n- **策略模式**：LLM Router 根据任务动态选择模型实现（`ILLMProvider` 接口）\n- **中介者模式**：Orchestrator 解耦所有 Agent，避免直接依赖，通过事件总线通信（`EventEmitter2`）\n- **工厂模式**：Plugin Manager 按 `agent.json` 动态创建 Agent 实例\n- **命令模式**：每个可执行动作（如“运行测试”）封装为 Command 对象，支持撤销/重试\n\n> 选择这些模式是为了实现**高内聚、低耦合、热插拔**——这是 300+ Agent 能稳定协作的基石。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 技术 | 选型原因 | 替代方案 | 注意事项 |\n|------|----------|----------|----------|\n| **TypeScript** | 静态类型保障大型 Agent 生态可维护性，IDE 智能补全强 | JavaScript | 必须启用 `strict: true`，否则插件类型崩溃率飙升 |\n| **Electron 28+** | 支持 Chromium 120 + Node.js 20，性能与 Web 标准兼容性最佳 | Tauri (Rust) | Electron 打包体积大（~200MB），但开发效率远超 Tauri |\n| **pnpm** | 节省磁盘空间，支持 monorepo 的硬链接依赖管理 | npm / yarn | 必须配置 `shamefully-hoist=true` 以兼容某些 Electron 模块 |\n| **Ollama** | 本地运行 Llama 3、CodeLlama 等开源模型的最简方案 | Hugging Face Transformers + vLLM | 需确保 GPU 显存 ≥8GB，否则启用 CPU fallback（慢但可用） |\n| **SQLite + JSONB** | 单文件轻量存储，支持全文检索与向量扩展（通过 `sqlite-vec`） | MongoDB / PostgreSQL | 不支持集群，需避免高并发写入（使用 WAL 模式优化） |\n| **Monaco Editor** | VS Code 同款编辑器，语法高亮、IntelliSense 完整 | CodeMirror | 需手动注入语言服务器（如 TypeScript LS）以支持智能提示 |\n\n> ⚠️ 版本兼容性陷阱：Electron 28+ 要求 Node.js 20，但某些 npm 包（如 `node-gyp`）仅支持到 v18。解决方案：使用 `nvm use 18 && pnpm install --ignore-engines` 安装原生模块。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 确保已安装 Node.js 20+ 和 Git\nnode -v # 应 ≥ v20.0.0\n\n# 2. 克隆仓库（使用 SSH 或 HTTPS）\ngit clone https://github.com/CherryHQ/cherry-studio.git\ncd cherry-studio\n\n# 3. 使用 pnpm 安装依赖（强制忽略引擎检查以兼容 Electron）\npnpm install --ignore-engines\n\n# 4. 构建前端资源（TypeScript → JS，打包静态文件）\npnpm build:web\n\n# 5. 打包桌面应用（自动检测平台）\npnpm build:electron\n\n# 6. 启动开发模式（热重载）\npnpm dev\n\n# 7. （可选）配置本地 LLM：安装 Ollama 并拉取模型\ncurl -fsSL https://ollama.com/install.sh | sh\nollama pull codellama:7b-instruct\n```\n\n> ✅ 首次启动后，UI 将引导你连接 API Key（OpenAI/Claude）或选择本地模型。\n\n---\n\n### 🎮 使用示例\n\n#### 场景：自动生成一个 Express.js 服务并部署到 Docker\n\n**输入（用户在 UI 中输入）：**\n\n> “创建一个支持 CORS 的 Express 服务器，暴露 /api/users 端点，返回 JSON 数组。打包为 Docker 镜像，并生成 docker-compose.yml 启动 Redis 缓存。”\n\n#### 预期输出：\n\n```ts\n// src/server.ts\nimport express from 'express';\nconst app = express();\napp.use(express.json());\napp.use(cors());\n\napp.get('/api/users', (req, res) => {\n  res.json([{ id: 1, name: 'Alice' }, { id: 2, name: 'Bob' }]);\n});\n\napp.listen(3000, () => console.log('Server running on http://localhost:3000'));\n```\n\n```dockerfile\n# Dockerfile\nFROM node:20-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nEXPOSE 3000\nCMD [\"node\", \"src/server.ts\"]\n```\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\nservices:\n  app:\n    build: .\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - redis\n  redis:\n    image: redis:7-alpine\n```\n\n#### 关键参数说明：\n\n- `model: codellama:7b-instruct` → 使用代码优化的 LLM\n- `maxTokens: 2048` → 控制生成长度，避免截断\n- `autoRun: true` → 自动执行生成的代码（安全沙箱内）\n- `requireDocker: true` → 强制检查 Docker 环境是否可用\n\n---\n\n### ⚡ 性能与优化\n\n- **性能瓶颈**：  \n  - LLM 推理延迟（本地模型 5–15s/次）  \n  - 多 Agent 并发时事件总线阻塞（单线程 EventEmitter）\n\n- **生产扩展方案**：  \n  - 引入 Redis Queue + Worker Pool，将推理任务异步化  \n  - 使用 vLLM 替代 Ollama，支持批处理与 PagedAttention  \n  - 模型缓存：对相同 prompt 做 LRU 缓存（key: md5(prompt + temperature)）\n\n- **资源消耗估算**（单实例）：\n  | 组件 | 内存 | CPU | 磁盘 |\n  |------|------|-----|------|\n  | Electron UI | 800MB | 1 core | 2GB |\n  | Ollama (Codellama 7B) | 6GB | 4 cores | 5GB |\n  | Redis + SQLite | 300MB | 0.5 core | 500MB |\n  | **总计** | ~9GB | ~5.5 cores | ~8GB |\n\n> 生产部署建议：使用 Docker Compose 部署，分离 UI、LLM Worker、Queue 模块。\n\n---\n\n### 🔌 二次开发指南\n\n#### 关键扩展点：\n\n1. **Agent 插件系统**  \n   在 `src/plugins/` 下创建新文件夹（如 `my-custom-agent`），包含：\n   - `agent.json`：声明名称、触发关键词、所需权限\n   - `index.ts`：实现 `execute()` 方法，返回 `{ code: string, output: any }`\n\n2. **API 接口**  \n   所有 Agent 必须实现：\n\n```ts\ninterface Agent {\n  id: string;\n  name: string;\n  description: string;\n  execute(context: Context): Promise<AgentResult>;\n}\n\ninterface Context {\n  prompt: string;          // 用户原始输入\n  files: File[];           // 当前项目文件树\n  llm: LLMProvider;        // 可调用的模型实例\n  workspacePath: string;   // 项目根目录\n}\n```\n\n3. **添加自定义功能示例**：  \n   创建一个“Git Commit Message Generator”：\n\n```ts\n// plugins/git-commit-agent/index.ts\nexport default {\n  id: 'git-commit',\n  name: 'Git Commit Msg',\n  execute: async ({ prompt, files }) => {\n    const diff = await exec('git diff --cached');\n    const message = await llm.generate(\n      `根据以下 git diff，生成符合 Conventional Commits 的英文提交信息：\\n${diff}`\n    );\n    return { code: message.trim(), output: { type: 'commit' } };\n  }\n}\n```\n\n",
    "last_scanned": "2026-01-16T02:03:34.464998",
    "last_analyzed": "2026-01-15T18:15:36.476620",
    "screenshot": "static/screenshots/805155266.jpg",
    "ai_visual_summary": "根据提供的截图和OCR信息，这是一个关于“cherry-studio”项目的GitHub贡献指南页面。该应用是一个基于AI代理（AI Agent）的桌面软件，旨在通过自动化和智能技术来辅助编程，它集成了超过300个智能助手，并能统一访问前沿的大语言模型（LLMs）。从界面设计来看，它采用了简洁、清晰的极简主义风格，以白色为主色调，通过清晰的标题、列表和链接来组织信息，引导用户进行代码贡献和协作开发。",
    "ai_rag_summary": null
  },
  {
    "id": "287463830",
    "name": "qlib",
    "full_name": "microsoft/qlib",
    "category": "ai_agent",
    "stars": 35620,
    "forks": 5543,
    "description": "Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.",
    "url": "https://github.com/microsoft/qlib",
    "homepage": "https://qlib.readthedocs.io/en/latest/",
    "language": "Python",
    "topics": "[\"algorithmic-trading\", \"auto-quant\", \"deep-learning\", \"finance\", \"fintech\", \"investment\", \"machine-learning\", \"paper\", \"platform\", \"python\", \"quant\", \"quant-dataset\", \"quant-models\", \"quantitative-finance\", \"quantitative-trading\", \"research\", \"research-paper\", \"stock-data\"]",
    "created_at": "2020-08-14T06:46:00Z",
    "updated_at": "2026-01-15T17:12:56Z",
    "readme_content": null,
    "ai_summary": "基于RD-Agent的自动化量化投资研发平台，集成机器学习、市场建模与强化学习技术，实现因子挖掘和模型优化的数据驱动型研发流程",
    "ai_tech_stack": "[\"Python\", \"PyTorch\", \"RD-Agent\"]",
    "ai_use_cases": "[\"\\u81ea\\u52a8\\u5316\\u7684\\u91d1\\u878d\\u6570\\u636e\\u9a71\\u52a8\\u578b\\u56e0\\u5b50\\u6316\\u6398\\u7cfb\\u7edf\", \"\\u91cf\\u5316\\u6295\\u8d44\\u7b56\\u7565\\u56de\\u6d4b\\u6846\\u67b6\\u5f00\\u53d1\\u5de5\\u5177\\u5305\", \"\\u673a\\u5668\\u5b66\\u4e60\\u6a21\\u578b\\u5728\\u91d1\\u878d\\u5e02\\u573a\\u4e2d\\u7684\\u5e94\\u7528\\u90e8\\u7f72\\u5e73\\u53f0\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "pip install pyqlib && python -m qlib --init",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nQlib 的核心差异化在于它首次将**量化研究的全生命周期**（从因子挖掘、模型训练到策略回测）封装为可编程、可自动化、可复现的工程流水线，而非仅提供一组孤立的算法库。与传统量化框架（如 Backtrader、Zipline）或通用 ML 框架（如 TensorFlow/PyTorch）相比，Qlib 不是“工具箱”，而是**量化研发的操作系统**。\n\n其真正突破在于集成 **RD-Agent** —— 一个基于 LLM 的自主演化智能体系统，首次实现：\n\n- **因子空间的自动探索**：从历史价格、财务报表、新闻舆情中自动生成并验证可解释性因子（非黑箱嵌入），避免人工特征工程的主观瓶颈；\n- **模型结构与超参的联合优化**：通过多智能体协作（Evaluator, Generator, Selector）在高维离散-连续空间中进行贝叶斯搜索，突破传统网格/随机调参的效率极限；\n- **研发闭环自动化**：从“假设 → 数据拉取 → 因子计算 → 模型训练 → 回测评估 → 信号生成”全流程无人干预，将研究员从重复劳动中解放，聚焦于高阶逻辑设计。\n\n这是目前唯一一个在工业级量化场景下实现 **AI-driven R&D loop** 的开源平台，解决了“数据丰富但人力稀缺”的行业根本矛盾。\n\n---\n\n### 🔥 技术亮点\n\n1. **多粒度时间序列建模统一接口**：  \n   支持 `PanelDataset`（N×T×F）的标准化封装，兼容 LSTM、Transformer、GNN、KRNN 等异构模型，所有模型输入输出格式一致，实现“插拔式”模型切换。\n\n2. **基于信息系数（IC）的损失函数设计**：  \n   不使用 MSE 或交叉熵，而是直接优化 IC（Rank Correlation），更贴合量化选股目标——最大化排序预测能力而非绝对值精度。支持 `ICLoss`, `RankMSELoss` 等定制化目标。\n\n3. **RD-Agent 的 Prompt Engineering + Tool Calling 架构**：  \n   - LLM 通过结构化 prompt（如 `{\"task\": \"factor_mining\", \"constraints\": [\"non-linear\", \"mean-reverting\"]}`）触发预定义工具链；\n   - 工具包括：因子库检索、统计检验（Shapiro-Wilk）、回测引擎调用、IC 滚动计算等；\n   - 基于 ReAct 架构，LLM 在每轮推理中动态决定“思考→行动→观察”循环，实现真正自主迭代。\n\n4. **分布式任务调度与缓存系统**：  \n   使用 `dask` + `redis` 实现因子计算的懒加载与中间结果持久化，避免重复运算（如 5000+ 因子每日重算），单日可处理 TB 级数据而无需 GPU 集群。\n\n---\n\n### 🏗️ 架构设计分析\n\n```\n┌──────────────────────┐       ┌─────────────────────┐\n│   User Script / CLI  │───┬──→│    Data Layer        │\n└──────────────────────┘   │   │ (Stocks, Factors,   │\n                           │   │  Economic Indicators)│\n                           ↓   └──────────▲────────────┘\n                     ┌────────────┐       │\n                     │ Dataset    │◄──────┘\n                     │ Adapter    │     (Query & Cache)\n                     └─────┬──────┘\n                           │\n               ┌─────────────────────┐\n               │  Feature Engine     │←─── RD-Agent (Factor Gen.)\n               │ (Rolling Stats,     │       │\n               │  Cross-Sectional    │       ▼\n               │  Normalization)     │   ┌────────────┐\n               └────────┬────────────┘   │ Model Zoo    │\n                        │                │ (LR, LSTM,   │\n                        ▼                │  Transformer,│\n                   ┌─────────────┐       │  KRNN, RL)   │\n                   │ Trainer     │◄──────┤              │\n                   │ (PyTorch)   │       └─────┬────────┘\n                   └─────┬───────┘             │\n                         │                     ▼\n                    ┌────────────┐       ┌─────────────┐\n                    │ Backtester │◄──────│ Signal Gen. │\n                    │ (Event-    │       │ (Rank, Z-Score)│\n                    │  driven)   │       └─────┬─────────┘\n                    └─────┬──────┘             │\n                          │                     ▼\n                   ┌────────────┐        ┌─────────────┐\n                   │ Evaluation │◄───────┤ Portfolio   │\n                   │ (IC, IR,   │        │ Optimization  │\n                   │ Turnover)  │        │ (Risk Model)  │\n                   └─────┬──────┘        └─────────────┘\n                         │\n                    ┌────────────┐\n                    │ Reporting  │\n                    │ Dashboard  │\n                    └────────────┘\n```\n\n#### 核心模块职责：\n\n- **Data Layer**：统一接入多源数据（CSV、数据库、Wind/Choice API），支持增量更新与快照版本控制；\n- **Feature Engine**：基于 `pandas` + `numba` 实现高效滚动窗口计算，因子生成器可被 RD-Agent 动态注入；\n- **Model Zoo**：抽象基类 `BaseModel` 定义 `.fit()`, `.predict()` 接口，所有模型继承该接口；\n- **Backtester**：事件驱动架构（Event Loop），支持 tick 级回测与交易成本建模（滑点、佣金）；\n- **RD-Agent**：独立进程服务，通过 gRPC/REST 与 Qlib 核心通信，不耦合主框架。\n\n#### 设计模式：\n\n- **策略模式**：模型、损失函数、评估指标均可插拔，由配置文件动态加载；\n- **工厂模式**：`DatasetFactory`, `ModelFactory` 根据 YAML 配置实例化对象；\n- **观察者模式**：回测引擎在每次交易后触发评估器更新绩效指标。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 组件 | 选择原因 | 替代方案 | 注意事项 |\n|------|----------|-----------|-----------|\n| **PyTorch** | 动态图利于复杂时序建模（如 KRNN）、支持自定义梯度 | TensorFlow | 必须 ≥2.0，与 `torch-scatter` 兼容性需校验 |\n| **Dask** | 并行化因子计算，支持分块内存处理 | Celery + Redis | 需配置 `dask-worker-memory-limit` 避免 OOM |\n| **SQLAlchemy** | 统一管理多数据库（SQLite/PostgreSQL）存储实验元数据 | MongoDB | 不推荐用 NoSQL 存储结构化回测结果，查询效率低 |\n| **FastAPI** (RD-Agent API) | 异步支持高并发 LLM 调用 | Flask + Gunicorn | 必须启用 `uvicorn --workers 4` 提升吞吐 |\n| **Ray** (实验性) | 未来计划用于分布式训练调度 | Horovod | 当前未集成，但架构预留了 `DistributedTrainer` 接口 |\n\n> ⚠️ 版本兼容：Qlib v0.9+ 要求 Python ≥3.8，PyTorch ≥2.0。若使用 RD-Agent，需额外安装 `langchain>=0.1`, `openai>=1.0`, `llama-index`。\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 创建独立环境（推荐 conda）\nconda create -n qlib python=3.9 -y\nconda activate qlib\n\n# 2. 安装核心包（PyPI 最新版）\npip install pyqlib --upgrade\n\n# 3. 下载官方数据集（示例：A股日线）\npython -c \"import qlib; qlib.init(provider_uri='~/.qlib/qlib_data/cn_data', region=qlib.CN)\"\n\n# 4. 安装 RD-Agent（可选，用于自动化 R&D）\npip install rd-agent\n\n# 5. 配置 LLM 后端（以 OpenAI 为例）\nmkdir -p ~/.config/rd-agent\ncat > ~/.config/rd-agent/config.yaml <<EOF\nllm:\n  model: gpt-4-turbo\n  api_key: your_openai_key_here\n  temperature: 0.3\n  max_tokens: 2048\ntoolkit:\n  enabled: [factor_mining, report_analysis, model_tuning]\nEOF\n\n# 6. 验证安装\npython -c \"from qlib.data import D; print(D.list_instruments())\"\n```\n\n> 💡 数据初始化耗时约 15–30 分钟（下载 20GB+ 历史数据），建议使用 `--proxy` 加速。\n\n---\n\n### 🎮 使用示例\n\n```python\nfrom qlib.data import D\nfrom qlib.model.linear import LinearModel\nfrom qlib.workflow import R\nfrom qlib.contrib.model import LightGBMModel\nfrom qlib.contrib.strategy import TopkDropoutStrategy\nfrom qlib.backtest import backtest, executor\nimport numpy as np\n\n# 1. 定义因子：简单动量（过去5日收益率）\ndef momentum_factor():\n    return D.features(['$close'], windows=[5]).pct_change(5)\n\n# 2. 模型训练\nmodel = LightGBMModel(\n    loss=\"mse\",\n    task='regression',\n    early_stopping_rounds=10,\n    num_leaves=64\n)\n\n# 3. 训练流程（R 是 Qlib 的实验管理器）\nwith R.start():\n    model.fit(\n        dataset=D.dataset(\n            instruments='csi300',\n            start_time='2018-01-01',\n            end_time='2022-12-31',\n            features=['$close', '$volume'],\n            label=['Ref($close, -5) / $close - 1']  # 未来5日收益率\n        )\n    )\n\n# 4. 回测：每日选前100只股票做多，后100只做空（中性组合）\nstrategy = TopkDropoutStrategy(topk=100, drop_num=100)\nexec = executor.Executor(\n    strategy=strategy,\n    risk_model='baseline',\n    transaction_cost=0.001  # 0.1%滑点\n)\n\nportfolio_return, benchmark_return = backtest(\n    prediction=model.predict(D.dataset(...)),\n    start_time='2023-01-01',\n    end_time='2024-01-01',\n    executor=exec,\n)\n\nprint(f\"Annualized IC: {np.mean(portfolio_return):.4f}\")\n# 输出示例：Annualized IC: 0.0783 （显著优于随机）\n```\n\n> ✅ 预期输出：IC > 0.05，IR > 1.2（年化夏普比），回测曲线持续跑赢沪深300。\n\n---\n\n### ⚡ 性能与优化\n\n- **瓶颈**：\n  - 因子计算阶段：`pandas` 滑窗在百万级股票+日频下 CPU 单核饱和；\n  - LLM 调用延迟：RD-Agent 在因子生成时平均耗时 2–5s/次（GPT-4）；\n  - 数据加载：未缓存的 `D.features()` 多次调用导致重复 I/O。\n\n- **生产扩展建议**：\n  - 因子计算 → 使用 Dask + GPU 加速（CuDF 替代 Pandas）\n  - 模型服务 → 将训练好的模型导出为 ONNX，部署 Triton Inference Server\n  - 数据层 → 使用 Apache Arrow + Parquet 分区存储，支持列式查询\n\n- **资源估算**：\n  | 场景 | 内存 | CPU 核心 | GPU | 耗时 |\n  |------|------|----------|-----|------|\n  | 小规模回测（100股） | 8GB | 4 | - | <5min |\n  | 全市场训练（3000股） | 64GB | 16 | A10 (24G) | 8–12h |\n  | RD-Agent 持续运行 | 32GB | 8 | - | 7×24 |\n\n---\n\n### 🔌 二次开发指南\n\n#### 扩展点：\n\n| 类型 | 接口位置 | 如何扩展 |\n|------|----------|----------|\n| 自定义因子 | `qlib.data.expression.Expression` | 继承基类，实现 `.calc()` 方法 |\n| 新模型 | `qlib.model.BaseModel` | 实现 `fit()`, `predict()`, `save()/load()` |\n| 评估指标 | `qlib.contrib.evaluate.ICRank` | 注册到 `metric_registry`，支持自定义排序统计量 |\n| 数据源 | `qlib.data.D` 的 `provider` 插件 | 实现 `BaseDataHandler` 接口对接 Bloomberg/Alpaca |\n\n#### 示例：添加自定义因子\n\n```python\nfrom qlib.data.expression import Expression\n\nclass VolatilityFactor(Expression):\n",
    "last_scanned": "2026-01-16T02:03:34.466630",
    "last_analyzed": "2026-01-15T19:42:10.334678",
    "screenshot": "static/screenshots/287463830.jpg",
    "ai_visual_summary": "该截图展示了一个名为 `qlib` 的 GitHub 项目主页，其界面设计风格简洁、现代且以数据可视化为核心。主要功能模块包括项目介绍、代码规范、许可证和安全信息。可见的技术关键词有“AI”、“Quant investment”（量化投资）、“ML”（机器学习）、“supervised learning”（监督学习）、“market dynamics modeling”（市场动态建模）和“RL”（强化学习）。从“Information Coefficient (IC)”、“Monthly IC”等图表和描述来看，该应用是一个用于量化投资研究的平台，旨在利用人工智能技术（特别是机器学习）来辅助从策略研究到实际部署的全过程。",
    "ai_rag_summary": null
  },
  {
    "id": "624681066",
    "name": "AgentGPT",
    "full_name": "reworkd/AgentGPT",
    "category": "ai_agent",
    "stars": 35509,
    "forks": 9460,
    "description": "🤖 Assemble, configure, and deploy autonomous AI Agents in your browser.",
    "url": "https://github.com/reworkd/AgentGPT",
    "homepage": "https://agentgpt.reworkd.ai",
    "language": "TypeScript",
    "topics": "[\"agent\", \"agentgpt\", \"agents\", \"agi\", \"ai\", \"ai-agents\", \"autogpt\", \"baby-agi\", \"gpt\", \"langchain\", \"llm\", \"next\", \"openai\", \"t3\", \"t3-stack\"]",
    "created_at": "2023-04-07T02:29:19Z",
    "updated_at": "2026-01-15T15:55:53Z",
    "readme_content": null,
    "ai_summary": "基于自主AI代理的多Agent对话系统，支持插件式工具调用和长期记忆",
    "ai_tech_stack": "[\"Next.js\", \"FastAPI\", \"MySQL\", \"LangChain\"]",
    "ai_use_cases": "[\"\\u914d\\u7f6e\\u81ea\\u5b9a\\u4e49AI\\u4efb\\u52a1\\u81ea\\u52a8\\u5316\\u6267\\u884c\", \"\\u591a\\u4ee3\\u7406\\u534f\\u4f5c\\u5b8c\\u6210\\u590d\\u6742\\u76ee\\u6807\\u5206\\u89e3\", \"\\u81ea\\u4e3b\\u667a\\u80fd\\u4f53\\u7684\\u6301\\u7eed\\u5b66\\u4e60\\u4e0e\\u4f18\\u5316\"]",
    "ai_difficulty": 5,
    "ai_quick_start": "npm install -g agentgpt-cli && agentgpt init",
    "ai_tutorial": "### 🎯 核心价值 & 差异化\n\nAgentGPT 的核心差异化在于：**将复杂的 AI Agent 系统封装为可即时交互、零基础设施门槛的浏览器端可视化工作台，实现“从零到部署”在 3 分钟内完成**。与 LangChain、AutoGen、CrewAI 等依赖本地代码编写和 CLI 配置的框架不同，它不强迫开发者写 Agent 逻辑或管理状态机——而是提供一个可拖拽配置、即时执行、可视化的“AI 操作系统”界面。\n\n它解决了三个关键痛点：\n1. **低门槛部署**：无需理解 Prompt 工程、内存管理、工具链编排，用户只需输入目标（如“分析竞品网站并生成报告”），系统自动拆解任务、调用工具、迭代执行。\n2. **浏览器即 IDE**：完全在前端运行交互式调试，支持实时查看 Agent 思考链（Thought → Action → Observation）、历史记录回溯、多 Agent 并行管理，这是其他开源项目从未提供的“可观测性第一”体验。\n3. **全栈集成即服务**：内置 Docker 化的 FastAPI + Next.js + MySQL 环境，一键启动，无需手动配置 CORS、数据库迁移、JWT 认证等工程杂务。\n\n它不是“另一个 Agent 框架”，而是**AI Agent 的 VS Code**——让开发者像写代码一样“组装”Agent，而非“编程”Agent。\n\n---\n\n### 🔥 技术亮点\n\n1. **前端实时推理链可视化**：  \n   使用 React + Zustand 状态管理，在浏览器中流式渲染 Agent 的每一步 Thought/Action/Observation，通过 `useEffect` 监听 WebSocket 或 SSE 事件流，实现“思考过程即 UI”。这是目前唯一开源项目能做到的实时可观测性。\n\n2. **动态工具注册与运行时注入**：  \n   不采用静态工具列表（如 LangChain 的 Tool classes），而是通过 `/api/tools` 端点动态加载插件定义（JSON Schema + Python 函数名），前端根据 schema 生成交互式表单，实现“无代码配置工具”。\n\n3. **会话状态持久化于 MySQL + JSON Field**：  \n   每个 Agent 的 memory、task list、history 存储为 PostgreSQL/MySQL 的 `JSON` 字段（非关系型结构），支持快速序列化/反序列化，避免 ORM 过度设计，兼顾灵活性与查询效率。\n\n4. **异步任务队列 + 超时熔断**：  \n   后端 FastAPI 使用 `asyncio.Queue` 管理 Agent 执行任务，每个步骤设置 90s 超时，超时后自动回退到“重新规划”状态，避免死循环。这是生产级鲁棒性的关键。\n\n5. **前端智能缓存与增量更新**：  \n   使用 SWR（Stale-While-Revalidate）缓存历史 Agent 会话，前端仅请求变更部分（如新任务），大幅降低网络开销和 UI 刷新卡顿。\n\n---\n\n### 🏗️ 架构设计分析\n\n#### 1. 整体架构图（文字描述）\n\n```\n[Browser (Next.js + React)] ←(REST/SSE/WS)→ [FastAPI Backend] ←→ [External APIs]\n       ↑                             ↑\n    Zustand State               MySQL (JSON fields)\n       ↓                             ↓\n[WebSocket for real-time logs]   [Dockerized DB / Redis (optional)]\n```\n\n#### 2. 核心模块划分与职责\n\n| 模块 | 路径 | 职责 |\n|------|------|------|\n| **Frontend UI** | `/next` | React + TypeScript 客户端，提供可视化配置面板、任务流展示、实时日志渲染 |\n| **API Gateway** | `/platform` (FastAPI) | 处理 Agent 创建/启动/停止、工具调用、会话管理、与 LLM 交互（OpenAI） |\n| **Agent Engine** | `/platform/agent/` | 核心逻辑：任务分解 → 工具选择 → 执行 → 回溯 → 迭代，基于 LangChain + 自定义 Prompt 模板 |\n| **Memory Store** | `/db/` (MySQL) | 存储 Agent 配置、历史对话、当前状态（JSON）、用户偏好 |\n| **Tool Registry** | `/platform/tools/` | 动态注册工具（搜索、代码执行、网页抓取等），通过 OpenAPI Schema 暴露给前端 |\n\n#### 3. 数据流向\n\n```\nUser Input (Goal) \n→ Frontend → POST /api/agents/create \n→ FastAPI 创建 Agent 记录 → 初始化记忆库（空任务队列）  \n→ 启动异步任务循环：[Think → Plan → Act → Observe → Learn]  \n  → Think: 调用 GPT-4o，输入上下文 + 目标  \n  → Plan: 解析 JSON 输出，生成 task list  \n  → Act: 根据 task 调用注册工具（如 serper、code_interpreter）  \n  → Observe: 收集工具返回结果，存入 memory  \n  → Learn: 更新 prompt context with observation, loop until goal achieved or max_steps  \n→ 实时 SSE 流推送每一步日志至前端  \n→ 用户可随时暂停/重启/重置 Agent\n```\n\n#### 4. 关键设计模式\n\n- **策略模式（Strategy Pattern）**：工具调用层抽象为 `ToolExecutor` 接口，不同工具（Serper、Python REPL、WebScraper）实现相同接口，便于扩展。\n- **状态机模式（Finite State Machine）**：Agent 内部状态为 `WAITING → PLANNING → EXECUTING → COMPLETED/FAILED`，通过枚举 + 状态转换规则控制流程，避免逻辑爆炸。\n- **发布-订阅模式（Pub/Sub）**：前端使用 WebSocket 订阅 Agent 事件流（`agent:task_update`），后端用 `asyncio.Event()` 触发推送，解耦执行引擎与 UI。\n- **配置驱动开发（Configuration over Code）**：所有行为由 JSON Schema + Prompt Template 驱动，而非硬编码逻辑，实现“无代码定制”。\n\n---\n\n### 🔧 技术栈深度解析\n\n| 技术 | 选择理由 | 替代方案 | 注意事项 |\n|------|----------|-----------|----------|\n| **Next.js 13 (App Router)** | Server Components 支持 SSR + API 路由一体化，SSR 渲染初始 Agent 界面快，Hydration 后交互流畅。TypeScript 类型安全对复杂状态管理至关重要。 | Remix / Nuxt | 必须用 `app/` 目录结构，避免旧 pages/；注意 `use client` 与 Server Component 的边界 |\n| **FastAPI** | 异步支持好、自动生成 OpenAPI 文档、依赖注入简洁。适合微服务式 Agent 控制器。 | Express.js / Flask | 需配置 `CORSMiddleware`，且必须用 `async def` 才能充分利用异步数据库连接池 |\n| **MySQL 8.0** | JSON 字段支持好，事务稳定，比 MongoDB 更易部署（Docker 镜像成熟）。适合结构化+半结构化混合存储。 | PostgreSQL (JSONB) / Redis | 必须开启 `innodb_large_prefix` 支持长 JSON；避免用 ORM 做复杂查询，直接写 SQL |\n| **Zustand** | 比 Redux Toolkit 更轻量、无样板、支持异步 actions，适合前端 Agent 状态（任务列表、日志流）。 | Recoil / Jotai | 不要将大对象（如完整对话历史）存入 store，只存 ID + 增量更新 |\n| **Docker Compose** | 一键部署整个栈：DB, FastAPI, Redis（可选），避免“在我机器上能跑”问题。 | Kubernetes / Podman | 需确保 Docker Desktop 内存 ≥4GB；MySQL 数据卷需持久化到本地目录 |\n\n---\n\n### 📦 安装与配置\n\n```bash\n# 1. 克隆仓库（已预置 setup 脚本）\ngit clone https://github.com/reworkd/AgentGPT.git\ncd AgentGPT\n\n# 2. 设置环境变量（复制示例文件并填入密钥）\ncp .env.example .env\n# 编辑 .env，填入：\n# OPENAI_API_KEY=sk-...\n# SERPER_API_KEY=your-key-here (可选)\n# REPLICATE_API_TOKEN=... (用于图像生成等)\n\n# 3. 启动服务（自动拉取镜像、建库、启动后端）\n./setup.sh  # Mac/Linux\n# 或\n./setup.bat # Windows\n\n# 4. 等待输出：\n# ✅ Database initialized\n# ✅ Backend running on http://localhost:8000\n# ✅ Frontend running on http://localhost:3000\n\n# 5. 打开浏览器访问：http://localhost:3000\n```\n\n> ⚠️ **注意**：首次启动需等待 Docker 镜像下载（约 2–5 分钟），若报错 `Error: Could not connect to MySQL`，手动执行：\n```bash\ndocker-compose down && docker-compose up -d db && sleep 10 && docker-compose up\n```\n\n---\n\n### 🎮 使用示例\n\n#### 场景：让 Agent 自动分析 Twitter 上关于 “LLM fine-tuning trends” 的热门推文，并生成简报。\n\n**前端操作**：\n- 在输入框粘贴目标：  \n  `Analyze the top 5 trending tweets about \"LLM fine-tuning\" on Twitter, summarize key insights, and output a markdown report.`\n\n**后端自动执行流程**：\n\n1. **Think**:  \n   > “I need to search for recent tweets about LLM fine-tuning, extract top posts, analyze sentiment and key techniques mentioned...”\n\n2. **Plan**:  \n   - Step 1: Use Serper API to search Twitter for `\"LLM fine-tuning\" site:twitter.com`  \n   - Step 2: Extract tweet text, likes, replies  \n   - Step 3: Summarize trends (e.g., LoRA vs full finetune, data quality)  \n   - Step 4: Output in Markdown format\n\n3. **Execute**:  \n   ```python\n   # 内部调用 Serper API（伪代码）\n   response = serper.search(q=\"LLM fine-tuning site:twitter.com\", num=5)\n   tweets = extract_tweets(response.results)\n   summary = openai.chat.completions.create(\n     model=\"gpt-4\",\n     messages=[{\"role\": \"system\", \"content\": \"Summarize these tweets into 3 bullet points...\"}, \n               {\"role\": \"user\", \"content\": str(tweets)}]\n   )\n   ```\n\n4. **Output**:\n```markdown\n## LLM Fine-Tuning Trends (Twitter Summary)\n\n- 📈 **LoRA is dominating**: 82% of top tweets prefer LoRA over full fine-tuning due to lower cost and faster iteration.\n- 💡 **Data quality > quantity**: Users report better results with 50 high-quality examples than 1k noisy ones.\n- ⚠️ **Overfitting risk**: Multiple users warn against using domain-specific data without validation sets.\n```\n\n> ✅ Agent 自动保存该会话，支持后续“继续对话”或“导出为 PDF”。\n\n---\n\n### ⚡ 性能与优化\n\n- **瓶颈**：  \n  - OpenAI API 调用延迟（平均 1.5–3s/次）是主要延迟源。  \n  - 多轮任务中 prompt 上下文膨胀（>8k tokens），触发模型截断或高 cost。\n\n- **生产扩展建议**：  \n  - 引入 Redis 缓存重复查询结果（如相同搜索词）。  \n  - 使用 LlamaIndex 或 LangChain 的 `VectorStoreRetriever` 替代部分 Serper 调用。  \n  - 将 Agent 状态持久化到 PostgreSQL + JSONB，支持分布式 Worker 消费队列（Celery/RQ）。  \n  - 设置 rate-limit 与任务超时（如 max_steps=10），防无限循环。\n\n- **资源消耗估算**：  \n  | 组件 | 内存 | CPU | 并发能力 |\n  |------|------|-----|----------|\n  | Next.js 前端 | 200MB | 0.1 vCPU | 50+ 用户（静态） |\n  | FastAPI 后端 | 500MB | 0.3 vCPU | 10–20 并发 Agent |\n  | MySQL | 300MB | 0.2 vCPU | 支持 100+ 会话 |\n  | Docker 总计 | ~1.5GB | ~0.6 vCPU | 单机可跑 5–8 个活跃 Agent |\n\n---\n\n### 🔌 二次开发指南\n\n#### 关键扩展点：\n\n| 模块 | 扩展方式 |\n|------|----------|\n| **新增工具** | 在 `platform/tools/` 下新建 `.py` 文件，继承 `BaseTool`，实现 `_run()` 方法 → 自动注册到 FastAPI `/tools` 端点 |\n| **自定义 Agent Prompt** | 修改 `platform/prompts/agent_prompt.txt`，支持变量 `{goal}`, `{history}`, `{tasks}` |\n| **前端 UI 插件** | 在 `next/app/ui/components/tools/",
    "last_scanned": "2026-01-16T02:03:34.467712",
    "last_analyzed": "2026-01-15T20:02:40.166165",
    "screenshot": "static/screenshots/624681066.jpg",
    "ai_visual_summary": "该 GitHub 项目 AgentGPT 是一个在浏览器中组装、配置和部署自主 AI 代理的工具。界面采用简洁的白色背景和标准的 GitHub 风格，通过清晰的“开始使用”（Getting Started）指南，指导用户安装 VS Code、Node.js、Git 等开发环境，并通过克隆仓库和运行脚本进行初始化。可见的技术关键词包括 Docker、OpenAI API Key 和 Serper API Key，表明它是一个基于 AI 模型和外部 API 的开发项目。整体来看，这是一个面向开发者的、用于快速搭建和运行 AI 代理的开源工具。",
    "ai_rag_summary": null
  },
  {
    "id": "396569538",
    "name": "khoj",
    "full_name": "khoj-ai/khoj",
    "category": "ai_agent",
    "stars": 32185,
    "forks": 1923,
    "description": "Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.",
    "url": "https://github.com/khoj-ai/khoj",
    "homepage": "https://khoj.dev",
    "language": "Python",
    "topics": "[\"agent\", \"ai\", \"assistant\", \"chat\", \"chatgpt\", \"emacs\", \"image-generation\", \"llama3\", \"llamacpp\", \"llm\", \"obsidian\", \"obsidian-md\", \"offline-llm\", \"productivity\", \"rag\", \"research\", \"self-hosted\", \"semantic-search\", \"stt\", \"whatsapp-ai\"]",
    "created_at": "2021-08-16T01:48:44Z",
    "updated_at": "2026-01-15T17:21:27Z",
    "readme_content": null,
    "ai_summary": "个人知识智能助手平台，支持本地/在线LLM集成、多模态文档检索与生成、跨终端部署及自定义代理构建",
    "ai_tech_stack": "[\"FastAPI\", \"LangChain\", \"ChromaDB\", \"PyTorch\"]",
    "ai_use_cases": "[\"\\u6784\\u5efa\\u4f01\\u4e1a\\u4e13\\u5c5eAI\\u77e5\\u8bc6\\u5e93\\u7cfb\\u7edf\", \"\\u5f00\\u53d1\\u8de8\\u5e73\\u53f0\\u667a\\u80fd\\u804a\\u5929\\u673a\\u5668\\u4eba\\uff08\\u652f\\u6301Obsidian/Emacs/WhatsApp\\u96c6\\u6210\\uff09\", \"\\u81ea\\u52a8\\u5316\\u6587\\u732e\\u7814\\u7a76\\u4e0e\\u62a5\\u544a\\u751f\\u6210\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "git clone https://github.com/khoj-ai/khoj && pip install -e .",
    "ai_tutorial": "# khoj\n\nYour AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.",
    "last_scanned": "2026-01-16T02:03:34.469445",
    "last_analyzed": "2026-01-15T23:42:02.696092",
    "screenshot": "static/screenshots/396569538.jpg",
    "ai_visual_summary": "根据对截图的视觉分析，该应用“Khoj”是一个可自托管的个人人工智能助理，其核心功能是充当用户的“第二大脑”。它允许用户通过一个集成的界面，利用本地或在线的大型语言模型（如GPT、Claude、Llama等）来查询网络信息、检索个人文档，并执行深度研究。从界面设计上看，它采用深色主题，通过一个中央的命令输入框和下方的功能卡片（如“Interviewing”、“Code”、“Learning”）来组织主要功能模块，风格现代且以功能为导向。整体来看，这是一款旨在通过自动化和智能代理来增强个人生产力的AI工具。",
    "ai_rag_summary": null
  },
  {
    "id": "644686905",
    "name": "continue",
    "full_name": "continuedev/continue",
    "category": "ai_agent",
    "stars": 30897,
    "forks": 4048,
    "description": "⏩ Ship faster with Continuous AI. Open-source CLI that can be used in TUI mode as a coding agent or Headless mode to run background agents",
    "url": "https://github.com/continuedev/continue",
    "homepage": "https://docs.continue.dev/",
    "language": "TypeScript",
    "topics": "[\"agent\", \"ai\", \"background-agents\", \"claude\", \"cli\", \"continuous-ai\", \"developer-tools\", \"gemini\", \"gpt\", \"jetbrains\", \"llm\", \"open-source\", \"qwen\", \"vscode\", \"workflows\"]",
    "created_at": "2023-05-24T03:39:39Z",
    "updated_at": "2026-01-15T17:28:49Z",
    "readme_content": null,
    "ai_summary": "基于LLM的智能编码助手，通过AI自动补全/重构代码并提供调试指导",
    "ai_tech_stack": "[\"TypeScript\", \"OpenAI API\", \"LangChain\\u6846\\u67b6\", \"Node.js\"]",
    "ai_use_cases": "[\"\\u81ea\\u52a8\\u5316\\u4ee3\\u7801\\u751f\\u6210\\uff08\\u5982\\u51fd\\u6570\\u3001\\u7c7b\\uff09\", \"\\u65e7\\u4ee3\\u7801\\u91cd\\u6784\\u4e0e\\u73b0\\u4ee3\\u5316\\u6539\\u9020\", \"\\u667a\\u80fd\\u8c03\\u8bd5\\u5efa\\u8bae\\uff08\\u9519\\u8bef\\u5b9a\\u4f4d/\\u4fee\\u590d\\u65b9\\u6848\\uff09\", \"\\u96c6\\u6210\\u6d4b\\u8bd5\\u8f85\\u52a9\\u7f16\\u5199\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "npm install -g @continue-dev/cli && continue init --project-type nodejs && continue start",
    "ai_tutorial": "# continue\n\n⏩ Ship faster with Continuous AI. Open-source CLI that can be used in TUI mode as a coding agent or Headless mode to run background agents",
    "last_scanned": "2026-01-16T02:03:34.470520",
    "last_analyzed": "2026-01-16T00:05:41.755284",
    "screenshot": "static/screenshots/644686905.jpg",
    "ai_visual_summary": "该界面展示了一个名为 \"continue\" 的开源命令行工具项目。其核心功能是作为“连续AI”开发助手，允许用户在终端（terminal）中以交互式文本用户界面（TUI）或后台代理（Headless）模式运行，实时执行工作流并批准决策。从截图中的技术关键词如 \"Claude 4.1 Opus\" 和 \"MCP Servers\" 可知，该应用集成了大型语言模型（LLM）和多种模型上下文协议（MCP）服务器，旨在通过AI自动化来加速软件开发。整体设计风格简洁、现代，以深色主题和清晰的代码展示为主，突出了其作为开发者工具的专业性。",
    "ai_rag_summary": null
  },
  {
    "id": "655515393",
    "name": "CopilotKit",
    "full_name": "CopilotKit/CopilotKit",
    "category": "ai_agent",
    "stars": 27968,
    "forks": 3629,
    "description": "React UI + elegant infrastructure for AI Copilots, AI chatbots, and in-app AI agents. The Agentic Frontend 🪁",
    "url": "https://github.com/CopilotKit/CopilotKit",
    "homepage": "https://docs.copilotkit.ai",
    "language": "TypeScript",
    "topics": "[\"agent\", \"agents\", \"ai\", \"ai-agent\", \"ai-assistant\", \"assistant\", \"copilot\", \"copilot-chat\", \"hacktoberfest\", \"langchain\", \"langgraph\", \"llm\", \"nextjs\", \"open-source\", \"react\", \"reactjs\", \"ts\", \"typescript\"]",
    "created_at": "2023-06-19T04:08:31Z",
    "updated_at": "2026-01-15T17:57:14Z",
    "readme_content": null,
    "ai_summary": "React UI + elegant infrastructure for AI Copilots, AI chatbots, and in-app AI agents. The Agentic Frontend 🪁",
    "ai_tech_stack": "[\"TypeScript\"]",
    "ai_use_cases": "[\"\\u901a\\u7528\\u5f00\\u53d1\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "git clone https://github.com/CopilotKit/CopilotKit",
    "ai_tutorial": "# CopilotKit\n\nReact UI + elegant infrastructure for AI Copilots, AI chatbots, and in-app AI agents. The Agentic Frontend 🪁",
    "last_scanned": "2026-01-16T02:03:34.471064",
    "last_analyzed": "2026-01-16T01:37:55.315762",
    "screenshot": "static/screenshots/655515393.jpg",
    "ai_visual_summary": "视觉分析失败",
    "ai_rag_summary": null
  },
  {
    "id": "100061716",
    "name": "nx",
    "full_name": "nrwl/nx",
    "category": "ai_agent",
    "stars": 27914,
    "forks": 2644,
    "description": "Get to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents.",
    "url": "https://github.com/nrwl/nx",
    "homepage": "https://nx.dev",
    "language": "TypeScript",
    "topics": "[\"angular\", \"build\", \"build-system\", \"build-tool\", \"building-tool\", \"cli\", \"cypress\", \"hacktoberfest\", \"javascript\", \"monorepo\", \"nextjs\", \"nodejs\", \"nx\", \"nx-workspaces\", \"react\", \"storybook\", \"typescript\"]",
    "created_at": "2017-08-11T18:50:23Z",
    "updated_at": "2026-01-15T16:28:59Z",
    "readme_content": null,
    "ai_summary": "Get to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents.",
    "ai_tech_stack": "[\"TypeScript\"]",
    "ai_use_cases": "[\"\\u901a\\u7528\\u5f00\\u53d1\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "git clone https://github.com/nrwl/nx",
    "ai_tutorial": "# nx\n\nGet to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents.",
    "last_scanned": "2026-01-16T02:03:34.472124",
    "last_analyzed": "2026-01-16T01:40:20.504204",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": "Get to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents."
  },
  {
    "id": "912559512",
    "name": "sim",
    "full_name": "simstudioai/sim",
    "category": "ai_agent",
    "stars": 25719,
    "forks": 3200,
    "description": "Open-source platform to build and deploy AI agent workflows.",
    "url": "https://github.com/simstudioai/sim",
    "homepage": "https://www.sim.ai",
    "language": "TypeScript",
    "topics": "[\"agent-workflow\", \"agentic-workflow\", \"agents\", \"ai\", \"aiagents\", \"anthropic\", \"artificial-intelligence\", \"automation\", \"chatbot\", \"deepseek\", \"gemini\", \"low-code\", \"nextjs\", \"no-code\", \"openai\", \"rag\", \"react\", \"typescript\"]",
    "created_at": "2025-01-05T22:47:49Z",
    "updated_at": "2026-01-15T17:45:15Z",
    "readme_content": null,
    "ai_summary": "基于LangChain框架的可视化AI代理工作流构建平台，支持插件式工具集成与向量数据库问答",
    "ai_tech_stack": "[\"TypeScript\", \"Next.js\", \"React\", \"FastAPI\", \"ChromaDB\", \"Redis\", \"Docker Compose\"]",
    "ai_use_cases": "[\"\\u591aAgent\\u5bf9\\u8bdd\\u7cfb\\u7edf\\u8bbe\\u8ba1\\u4e0e\\u90e8\\u7f72\", \"\\u57fa\\u4e8e\\u6587\\u6863\\u7684RAG\\uff08\\u68c0\\u7d22\\u589e\\u5f3a\\u751f\\u6210\\uff09\\u77e5\\u8bc6\\u95ee\\u7b54\\u5e73\\u53f0\", \"AI\\u5de5\\u4f5c\\u6d41\\u53ef\\u89c6\\u5316\\u8c03\\u8bd5\\u5de5\\u5177\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "git clone https://github.com/simstudioai/sim.git && docker compose -f docker-compose.prod.yml up --build",
    "ai_tutorial": null,
    "last_scanned": "2026-01-16T02:03:34.475491",
    "last_analyzed": "2026-01-16T02:14:58.303422",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "384219990",
    "name": "Warp",
    "full_name": "warpdotdev/Warp",
    "category": "ai_agent",
    "stars": 25672,
    "forks": 610,
    "description": "Warp is the agentic development environment, built for coding with multiple AI agents.",
    "url": "https://github.com/warpdotdev/Warp",
    "homepage": "https://warp.dev",
    "language": null,
    "topics": "[\"bash\", \"linux\", \"macos\", \"rust\", \"shell\", \"terminal\", \"wasm\", \"zsh\"]",
    "created_at": "2021-07-08T18:48:08Z",
    "updated_at": "2026-01-15T14:42:28Z",
    "readme_content": null,
    "ai_summary": "Warp 是一个面向开发者的 agentic 开发环境，通过 Rust 编写并利用 GPU 加速实现高性能，集成多个 AI 代理用于协作完成从编码到部署的全栈任务。",
    "ai_tech_stack": "[\"Rust\", \"GPU\\u52a0\\u901f\", \"WebAssembly\", \"\\u7ec8\\u7aef\\u96c6\\u6210\"]",
    "ai_use_cases": "[\"AI \\u52a9\\u624b\\u8f85\\u52a9\\u4ee3\\u7801\\u751f\\u6210\", \"\\u591a\\u4ee3\\u7406\\u534f\\u4f5c\\u89e3\\u51b3\\u590d\\u6742\\u95ee\\u9898\", \"\\u81ea\\u52a8\\u5316\\u6d4b\\u8bd5\\u4e0e\\u8c03\\u8bd5\\u652f\\u6301\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "cargo install warp-cli && warp --init",
    "ai_tutorial": null,
    "last_scanned": "2026-01-16T02:03:34.475994",
    "last_analyzed": "2026-01-16T02:15:51.012213",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  }
]