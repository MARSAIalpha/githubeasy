[
  {
    "id": "537603333",
    "name": "whisper",
    "full_name": "openai/whisper",
    "category": "tts_voice",
    "stars": 93176,
    "forks": 11650,
    "description": "Robust Speech Recognition via Large-Scale Weak Supervision",
    "url": "https://github.com/openai/whisper",
    "homepage": "",
    "language": "Python",
    "topics": "[]",
    "created_at": "2022-09-16T20:02:54Z",
    "updated_at": "2026-01-14T17:52:31Z",
    "readme_content": null,
    "ai_summary": "Whisper æ˜¯ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„é€šç”¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ”¯æŒå¤šè¯­è¨€è½¬å½•ã€å®æ—¶ç¿»è¯‘å’Œè¯´è¯äººè¯†åˆ«ç­‰å¤šåŠŸèƒ½é›†æˆï¼Œé€šè¿‡å¼±ç›‘ç£å­¦ä¹ åœ¨å¤§è§„æ¨¡æœªæ ‡æ³¨éŸ³é¢‘æ•°æ®ä¸Šè®­ç»ƒä»¥å®ç°é«˜å‡†ç¡®ç‡ã€‚",
    "ai_tech_stack": "[\"Python\", \"PyTorch\", \"FFmpeg\"]",
    "ai_use_cases": "[\"\\u8bed\\u97f3\\u8f6c\\u6587\\u5b57\\uff08Audio Transcription\\uff09\", \"\\u591a\\u8bed\\u8a00\\u5b9e\\u65f6\\u7ffb\\u8bd1\\uff08Speech Translation\\uff09\", \"\\u8bf4\\u8bdd\\u4eba\\u8bc6\\u522b\\u4e0e\\u5206\\u79bb\\uff08Speaker Diarization\\uff09\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "pip install -U openai-whisper && python -m whisper --help",
    "ai_tutorial": "### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–\n\nWhisper çš„æ ¸å¿ƒå·®å¼‚åŒ–åœ¨äºï¼š**å®ƒé¦–æ¬¡åœ¨å¤§è§„æ¨¡å¼±ç›‘ç£æ•°æ®ä¸Šè®­ç»ƒå‡ºä¸€ä¸ªé€šç”¨ã€å¤šä»»åŠ¡ã€ç«¯åˆ°ç«¯çš„è¯­éŸ³å¤„ç†æ¨¡å‹ï¼Œå½»åº•æ‰“ç ´äº†ä¼ ç»Ÿè¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­â€œæ¨¡å—åŒ–æµæ°´çº¿â€çš„èŒƒå¼**ã€‚\n\nä¼ ç»ŸASRç³»ç»Ÿä¾èµ–äºå¤šä¸ªç‹¬ç«‹ç»„ä»¶ï¼ˆVAD â†’ éŸ³é¢‘åˆ†æ®µ â†’ å£°å­¦æ¨¡å‹ â†’ è¯­è¨€æ¨¡å‹ â†’ è§£ç å™¨ï¼‰ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦å•ç‹¬è®­ç»ƒå’Œè°ƒä¼˜ï¼Œä¸”è·¨è¯­è¨€/åœºæ™¯æ³›åŒ–èƒ½åŠ›å·®ã€‚Whisper åˆ™é€šè¿‡**å•ä¸€Transformerç¼–ç å™¨-è§£ç å™¨æ¶æ„**ï¼Œç»Ÿä¸€å¤„ç†ï¼š\n\n- å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ï¼ˆ>99ç§è¯­è¨€ï¼‰\n- è¯­éŸ³ç¿»è¯‘ï¼ˆspeech-to-text in another languageï¼‰\n- è¯­è¨€è¯†åˆ«ï¼ˆlanguage identificationï¼‰\n- è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰\n\nå…¶çªç ´æ€§åœ¨äºï¼š**ç”¨â€œä»»åŠ¡å‰ç¼€æ ‡è®°â€ï¼ˆtask tokensï¼‰å°†å¤šä»»åŠ¡ç»Ÿä¸€ä¸ºåºåˆ—åˆ°åºåˆ—çš„ç”Ÿæˆé—®é¢˜ï¼Œè€Œéå¤šå¤´åˆ†ç±»æˆ–å¤šä»»åŠ¡æŸå¤±æ‹¼æ¥**ã€‚è¿™ç§è®¾è®¡è®©æ¨¡å‹åœ¨æ¨ç†æ—¶ä»…éœ€æŒ‡å®šä¸€ä¸ªç‰¹æ®Štokenï¼ˆå¦‚ `\"<|transcribe|>\"` æˆ– `\"<|translate|>\"`ï¼‰ï¼Œå³å¯åŠ¨æ€åˆ‡æ¢è¡Œä¸ºï¼Œæ— éœ€å‚æ•°å¾®è°ƒæˆ–æ¨¡å‹åˆ‡æ¢ã€‚\n\nç›¸æ¯”åŒæœŸç«å“ï¼ˆå¦‚ Wav2Vec 2.0ã€Conformerã€HuBERTï¼‰â€”â€”å®ƒä»¬å¤§å¤šä¸“æ³¨å•ä»»åŠ¡ï¼ˆè¯†åˆ«ï¼‰ä¸”ä¾èµ–å¼ºæ ‡æ³¨æ•°æ®â€”â€”Whisper çš„è®­ç»ƒè¯­æ–™æ¥è‡ª**äº’è”ç½‘ä¸Š68ä¸‡å°æ—¶çš„å¼±ç›‘ç£éŸ³é¢‘-æ–‡æœ¬å¯¹**ï¼ˆYouTubeã€æ’­å®¢ç­‰ï¼‰ï¼Œä¸ä¾èµ–äººå·¥è½¬å½•ï¼Œå´å®ç°äº†è¶…è¶Šäººç±»æ°´å¹³çš„è·¨è¯­è¨€WERè¡¨ç°ã€‚è¿™æ˜¯AIå²ä¸Šæœ€å¤§è§„æ¨¡çš„å¼±ç›‘ç£è¯­éŸ³è®­ç»ƒä¹‹ä¸€ã€‚\n\n### ğŸ”¥ æŠ€æœ¯äº®ç‚¹\n\n1. **å¤šä»»åŠ¡ç»Ÿä¸€ä¸ºåºåˆ—ç”Ÿæˆ**  \n   æ‰€æœ‰ä»»åŠ¡è¢«ç¼–ç ä¸ºä¸€ä¸ªtokenåºåˆ—ï¼š`[<|start|>][<|en|>][<|transcribe|>][audio tokens][<|endoftext|>]`ã€‚è§£ç å™¨åƒç¿»è¯‘æ¨¡å‹ä¸€æ ·è‡ªå›å½’é¢„æµ‹æ–‡æœ¬ï¼Œè€Œéåˆ†ç±»æˆ–CTCå¯¹é½ã€‚è¿™ä½¿æ¨¡å‹å¤©ç„¶å…·å¤‡â€œä¸Šä¸‹æ–‡æ„ŸçŸ¥â€èƒ½åŠ›ï¼ˆå¦‚åŒºåˆ†åŒéŸ³è¯ã€å¤„ç†å£è¯­å¡«å……è¯ï¼‰ã€‚\n\n2. **æ—¶é—´-ç©ºé—´æ··åˆæ³¨æ„åŠ›æœºåˆ¶**  \n   ç¼–ç å™¨ä½¿ç”¨**åˆ†å—æ³¨æ„åŠ›ï¼ˆchunked attentionï¼‰**ï¼šå°†éŸ³é¢‘æŒ‰30ç§’çª—å£åˆ‡åˆ†ï¼Œæ¯å—å†…ç”¨æ ‡å‡†Transformeræ³¨æ„åŠ›ï¼Œè·¨å—é—´ä»…é€šè¿‡ä½ç½®ç¼–ç ä¼ é€’ä¸Šä¸‹æ–‡ã€‚è¿™æ—¢ä¿è¯é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œåˆé¿å…O(nÂ²)å†…å­˜çˆ†ç‚¸ã€‚\n\n3. **é²æ£’çš„éŸ³é¢‘é¢„å¤„ç†ï¼šå¤šå°ºåº¦é¢‘è°±å¢å¼º**  \n   è¾“å…¥ä¸æ˜¯åŸå§‹æ³¢å½¢ï¼Œè€Œæ˜¯**80é€šé“Melé¢‘è°±å›¾ï¼ˆn_fft=400, hop_length=160ï¼‰**ï¼Œä½†è®­ç»ƒæ—¶éšæœºæ·»åŠ ï¼š\n   - é«˜æ–¯å™ªå£°ï¼ˆSNR 5â€“25 dBï¼‰\n   - æ—¶é—´æ‹‰ä¼¸ï¼ˆÂ±10%ï¼‰\n   - é¢‘ç‡æ©ç ï¼ˆmax 27 freq binsï¼‰\n   - æ—¶é—´æ©ç ï¼ˆmax 40 time framesï¼‰\n\n   è¿™ç§â€œæ•°æ®å¢å¼ºå³æ­£åˆ™åŒ–â€ç­–ç•¥æå¤§æå‡äº†å¯¹å™ªéŸ³ã€å£éŸ³ã€æ··å“çš„é²æ£’æ€§ã€‚\n\n4. **éè‡ªå›å½’å¼•å¯¼è®­ç»ƒï¼ˆNon-Autoregressive Guidanceï¼‰**  \n   åœ¨è®­ç»ƒä¸­ï¼Œæ¨¡å‹ä¼šåŒæ—¶æ¥æ”¶â€œæ•™å¸ˆå¼ºåˆ¶â€ï¼ˆground truthæ–‡æœ¬ï¼‰å’Œâ€œè‡ªå›å½’é¢„æµ‹â€ä¸¤ç§è¾“å…¥ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªè¾…åŠ©æŸå¤±å‡½æ•°é¼“åŠ±ä¸¤è€…ä¸€è‡´æ€§ã€‚è¿™ç¼“è§£äº†æ›å…‰åå·®ï¼ˆexposure biasï¼‰ï¼Œå°¤å…¶åœ¨é•¿å¥ä¸Šæ˜¾è‘—æå‡ç¨³å®šæ€§ã€‚\n\n### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ\n\n#### 1. æ•´ä½“æ¶æ„å›¾ï¼ˆæ–‡å­—æè¿°ï¼‰\n\n```\n[Audio Waveform (16kHz)] \n        â†“\n[Preprocessing: STFT â†’ Mel Spectrogram (80ch, 30s chunks)]\n        â†“\n[Encoder: 4â€“32å±‚Transformer Encoder (N layers depending on model size)]\n        â†“\n[Decoder: 4â€“32å±‚Transformer Decoder with Cross-Attention]\n        â†‘\n[Task Token: <|transcribe|>, <|translate|>, <|lang:en|> ...]\n        â†“\n[Output: Text Tokens via Autoregressive LM Head]\n```\n\n#### 2. æ ¸å¿ƒæ¨¡å—åˆ’åˆ†ä¸èŒè´£\n\n| æ¨¡å— | èŒè´£ |\n|------|------|\n| **Audio Encoder** | å°†16kHzæ³¢å½¢è½¬ä¸º80ç»´Melé¢‘è°±ï¼Œå†é€šè¿‡å¤šå±‚Transformerç¼–ç ä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚ä½¿ç”¨ä½ç½®åµŒå…¥ï¼ˆsin-cosï¼‰å¤„ç†é•¿åºåˆ—ã€‚ |\n| **Text Decoder** | è‡ªå›å½’é¢„æµ‹æ–‡æœ¬tokenï¼›æ¥å—encoderè¾“å‡ºå’Œä»»åŠ¡/è¯­è¨€æ ‡è®°ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚ä½¿ç”¨å› æœæ©ç ç¡®ä¿å•å‘ç”Ÿæˆã€‚ |\n| **Task Token Embedding** | ç‰¹æ®Štokenï¼ˆå¦‚ `<|transcribe|>`ï¼‰åµŒå…¥ï¼Œæ³¨å…¥åˆ°decoderçš„èµ·å§‹åºåˆ—ä¸­ï¼Œæ§åˆ¶è¡Œä¸ºæ¨¡å¼ã€‚ |\n| **LM Head** | çº¿æ€§å±‚ + softmaxï¼Œè¾“å‡ºè¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒï¼ˆ51864ä¸ªtokenï¼šå«BPEç¼–ç çš„æ–‡æœ¬ã€è¯­è¨€IDã€ä»»åŠ¡IDç­‰ï¼‰ã€‚ |\n\n#### 3. æ•°æ®æµå‘\n\n```\nè¾“å…¥éŸ³é¢‘ â†’ STFT â†’ Melé¢‘è°±å›¾ (30s chunk) â†’ Encoder â†’ Context Embeddings\n                              â†“\n                  Task Token + Language Token â†’ Decoder Input\n                              â†“\n            Decoder: Self-Attention + Cross-Attention â†’ Predict Next Token\n                              â†“\n                   Autoregressive Loop until <|endoftext|>\n```\n\n#### 4. å…³é”®è®¾è®¡æ¨¡å¼\n\n- **ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰**ï¼šé€šè¿‡ä»»åŠ¡tokenåŠ¨æ€åˆ‡æ¢æ¨¡å‹è¡Œä¸ºï¼Œæ— éœ€ä¿®æ”¹æ¶æ„ã€‚æ–°å¢è¯­è¨€æˆ–ä»»åŠ¡åªéœ€æ‰©å±•è¯è¡¨ã€‚\n- **å·¥å‚æ¨¡å¼ï¼ˆFactory Patternï¼‰**ï¼š`whisper.load_model()` æ ¹æ®åç§°ï¼ˆå¦‚ `\"tiny.en\"`ï¼‰è‡ªåŠ¨åŠ è½½é¢„è®­ç»ƒæƒé‡å’Œé…ç½®ã€‚\n- **è£…é¥°å™¨æ¨¡å¼ï¼ˆDecorator Patternï¼‰**ï¼š`transcribe()` å‡½æ•°å°è£…äº†éŸ³é¢‘é¢„å¤„ç†ã€åˆ†å—ã€åå¤„ç†ï¼ˆæ ‡ç‚¹æ¢å¤ã€æ—¶é—´æˆ³å¯¹é½ï¼‰ï¼Œå¯¹å¤–æš´éœ²ç®€æ´æ¥å£ã€‚\n\n### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ\n\n| ç»„ä»¶ | ä½œç”¨ | æ›¿ä»£æ–¹æ¡ˆ | ä¸ºä½•é€‰å®ƒ |\n|------|------|----------|----------|\n| **PyTorch** | æ¡†æ¶æ ¸å¿ƒ | TensorFlow, JAX | ç”Ÿæ€æˆç†Ÿã€åŠ¨æ€å›¾åˆ©äºè°ƒè¯•ã€ç¤¾åŒºæ¨¡å‹å¯¼å‡ºæ”¯æŒå¥½ï¼ˆONNXï¼‰ |\n| **tiktoken** | BPEåˆ†è¯å™¨ | Hugging Face Tokenizers | OpenAIè‡ªç ”ï¼Œä¸“ä¸ºWhisperè¯æ±‡è¡¨ä¼˜åŒ–ï¼Œé€Ÿåº¦æå¿«ï¼ˆRuståç«¯ï¼‰ï¼Œæ”¯æŒå¤šè¯­è¨€åˆå¹¶è§„åˆ™ |\n| **ffmpeg** | éŸ³é¢‘è§£ç /é‡é‡‡æ · | librosa, pydub | æ”¯æŒå‡ ä¹æ‰€æœ‰éŸ³é¢‘æ ¼å¼ï¼ˆMP3, AAC, FLAC...ï¼‰ï¼Œé«˜æ•ˆã€ç¨³å®šã€æ— ä¾èµ– |\n| **NumPy / SciPy** | æ•°å€¼è®¡ç®— | JAX, CuPy | æ ‡å‡†åº“ï¼Œå…¼å®¹æ€§å¥½ï¼Œé¢‘è°±è®¡ç®—æˆç†Ÿ |\n| **Rust (tiktoken)** | é«˜æ€§èƒ½åˆ†è¯å†…æ ¸ | Pythonçº¯å®ç° | BPEæ˜¯CPUå¯†é›†å‹æ“ä½œï¼ŒRustå®ç°æ¯”Pythonå¿«10â€“50å€ |\n\n> âœ… **ç‰ˆæœ¬å…¼å®¹æ³¨æ„**ï¼š  \n> - `tiktoken` 0.4+ è¦æ±‚ Rust 1.65+ï¼ˆæ—§ç³»ç»Ÿéœ€å‡çº§ï¼‰  \n> - PyTorch â‰¥1.12 æ”¯æŒæ›´å¥½ï¼Œä½† Whisper å®˜æ–¹æµ‹è¯•äº 1.10â€“2.0  \n> - è‹¥æŠ¥é”™ `No module named 'setuptools_rust'`ï¼š`pip install setuptools-rust` å³å¯\n\n### ğŸ“¦ å®‰è£…ä¸é…ç½®ï¼ˆå¤åˆ¶ç²˜è´´ç‰ˆï¼‰\n\n```bash\n# 1. å®‰è£… ffmpegï¼ˆç³»ç»Ÿçº§ä¾èµ–ï¼‰\n# Ubuntu/Debian:\nsudo apt update && sudo apt install -y ffmpeg\n\n# macOS (Homebrew):\nbrew install ffmpeg\n\n# Windows (Chocolatey):\nchoco install ffmpeg\n\n# 2. å®‰è£… Python åŒ…ï¼ˆæ¨èè™šæ‹Ÿç¯å¢ƒï¼‰\npython -m venv whisper-env\nsource whisper-env/bin/activate  # Linux/macOS\n# æˆ– whisper-env\\Scripts\\activate  # Windows\n\npip install --upgrade pip setuptools wheel\npip install openai-whisper\n\n# 3. ï¼ˆå¯é€‰ï¼‰å®‰è£… Rust ç¼–è¯‘å™¨ï¼ˆä»…å½“ tiktoken å®‰è£…å¤±è´¥æ—¶ï¼‰\ncurl https://sh.rustup.rs -sSf | sh\nsource $HOME/.cargo/env\n\n# 4. éªŒè¯å®‰è£…\npython -c \"import whisper; print(whisper.available_models())\"\n```\n\n### ğŸ® ä½¿ç”¨ç¤ºä¾‹\n\n```python\nimport whisper\n\n# åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼Œé¦–æ¬¡è¿è¡Œè€—æ—¶ï¼‰\nmodel = whisper.load_model(\"large-v3\")  # æœ€é«˜ç²¾åº¦ï¼Œçº¦10GB VRAM\n\n# è¾“å…¥ï¼šçœŸå®åœºæ™¯éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚ä¼šè®®å½•éŸ³ï¼‰\naudio_path = \"meeting_audio.mp3\"\n\n# æ‰§è¡Œè½¬å½•\nresult = model.transcribe(\n    audio=audio_path,\n    language=\"en\",                    # å¯é€‰ï¼šæŒ‡å®šè¯­è¨€æå‡å‡†ç¡®ç‡\n    task=\"transcribe\",                # æˆ– \"translate\" ä¸ºä¸­æ–‡â†’è‹±æ–‡\n    temperature=0.2,                  # æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼Œä½å€¼æ›´ç¨³å®š\n    best_of=5,                        # è§£ç æ—¶é‡‡æ ·5æ¬¡å–æœ€ä¼˜\n    beam_size=5,                      # æŸæœç´¢å®½åº¦\n    word_timestamps=True              # è¾“å‡ºæ¯ä¸ªè¯çš„æ—¶é—´æˆ³\n)\n\n# é¢„æœŸè¾“å‡ºï¼ˆç»“æ„åŒ–å­—å…¸ï¼‰\nprint(result[\"text\"])\n# \"Today we're launching our new API to support real-time transcription...\"\n\nfor segment in result[\"segments\"]:\n    print(f\"[{segment['start']:.2f}s â†’ {segment['end']:.2f}s] {segment['text']}\")\n\n# è¾“å‡ºç¤ºä¾‹ï¼š\n# [0.45s â†’ 3.12s] Today we're launching our new API\n# [3.89s â†’ 6.78s] to support real-time transcription...\n```\n\n### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–\n\n| æŒ‡æ ‡ | tiny.en | base.en | large-v3 |\n|------|---------|---------|----------|\n| VRAM å ç”¨ | ~1 GB | ~2 GB | ~10+ GB |\n| å®æ—¶å› å­ï¼ˆRTFï¼‰ | 0.1x | 0.2x | 0.5x* |\n| æ¨ç†é€Ÿåº¦ï¼ˆA100ï¼‰ | ~10x real-time | ~5x | ~2x |\n\n> *RTF = å®é™…å¤„ç†æ—¶é—´ / éŸ³é¢‘æ—¶é•¿ã€‚RTF < 1 è¡¨ç¤ºæ¯”å®æ—¶å¿«ã€‚\n\n#### æ€§èƒ½ç“¶é¢ˆï¼š\n\n- **å†…å­˜**ï¼šEncoderå¯¹é•¿éŸ³é¢‘ï¼ˆ>30sï¼‰éœ€ç¼“å­˜key/valueï¼Œå¤§æ¨¡å‹æ˜“OOMã€‚\n- **CPUé¢„å¤„ç†**ï¼šSTFT + Melè½¬æ¢æ˜¯å•çº¿ç¨‹ç“¶é¢ˆï¼Œå¯ç”¨ `ffmpeg -i input.wav -f wav -` æµå¼è¾“å…¥ç¼“è§£ã€‚\n- **è§£ç å»¶è¿Ÿ**ï¼šè‡ªå›å½’ç”Ÿæˆå¯¼è‡´é«˜é¦–å­—å»¶è¿Ÿï¼ˆfirst-token latencyï¼‰ï¼Œä¸é€‚åˆä½å»¶æ—¶äº¤äº’åœºæ™¯ã€‚\n\n#### ç”Ÿäº§éƒ¨ç½²å»ºè®®ï¼š\n\n- ä½¿ç”¨ **TensorRT** æˆ– **ONNX Runtime** åŠ é€Ÿæ¨ç†ï¼ˆå®˜æ–¹æœªæä¾›ï¼Œä½†ç¤¾åŒºæœ‰å®ç°ï¼‰\n- éƒ¨ç½²ä¸º **å¼‚æ­¥æ‰¹å¤„ç†æœåŠ¡**ï¼šå¤šä¸ªéŸ³é¢‘æµåˆå¹¶æˆbatchï¼Œæå‡GPUåˆ©ç”¨ç‡\n- é‡‡ç”¨ **åˆ†å—æµæ°´çº¿**ï¼šé•¿éŸ³é¢‘åˆ‡ç‰‡ â†’ å¹¶è¡Œæ¨ç† â†’ åå¤„ç†æ‹¼æ¥ï¼ˆæ³¨æ„é‡å çª—å£é¿å…æ–­å¥ï¼‰\n\n### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—\n\n#### å…³é”®æ‰©å±•ç‚¹ï¼š\n\n1. **è‡ªå®šä¹‰ä»»åŠ¡token**  \n   åœ¨ `whisper.tokenizer.get_tokenizer()` ä¸­æ·»åŠ æ–°tokenï¼ˆå¦‚ `<|summarize|>`ï¼‰ï¼Œé‡æ–°è®­ç»ƒè§£ç å™¨å¤´éƒ¨ã€‚\n\n2. **æ›¿æ¢éŸ³é¢‘ç¼–ç å™¨**  \n   å¯å°†Melé¢‘è°±è¾“å…¥æ”¹ä¸º WavLM ç‰¹å¾ï¼Œéœ€ä¿®æ”¹ `model.encoder` è¾“å…¥ç»´åº¦å¹¶å¾®è°ƒã€‚\n\n3. **é›†æˆå¤–éƒ¨è¯­è¨€æ¨¡å‹**  \n   åœ¨ `model.decode()` åæ¥ N-gram æˆ– LLMï¼ˆå¦‚ GPT-4ï¼‰åšåæ ¡æ­£ï¼š\n\n```python\nraw_text = model.transcribe(audio)[\"text\"]\nrefined = llm.call(f\"Correct grammar and punctuation: {raw_text}\")\n```\n\n#### API æ‰©å±•ç¤ºä¾‹ï¼š\n\n```python\n# æ·»åŠ è‡ªå®šä¹‰åå¤„ç†\ndef transcribe_with_punctuation(model, audio_path):\n    result = model.transcribe(audio_path)\n    # ç”¨è§„åˆ™æˆ–æ¨¡å‹æ·»åŠ æ ‡ç‚¹\n    result[\"text\"] = add_punctuation(result[\"text\"])\n    return result\n```\n\n#### æ¨èä¿®æ”¹æ–¹å‘ï¼š\n- æ”¯æŒ**å®æ—¶æµå¼æ¨ç†**ï¼ˆchunk-by-chunkï¼‰\n- æ·»åŠ **è¯´è¯äººåˆ†ç¦»**ï¼ˆdiarizationï¼‰æ¥å£\n- è¾“å‡º**ç½®ä¿¡åº¦åˆ†æ•°** per token\n\n### â— å¸¸è§é—®é¢˜ä¸é¿å‘\n\n1. **Qï¼šå®‰è£…æ—¶æŠ¥é”™ `No module named 'tiktoken'` æˆ– `setuptools_rust` ç¼ºå¤±**  \n   Aï¼šè¿è¡Œ `pip install setuptools-rust && pip install --no-cache-dir openai-whisper`\n\n2. **Qï¼šè½¬å½•ç»“æœä¹±ç ï¼Œä¸­è‹±æ–‡æ··åˆå‡ºé”™**  \n   Aï¼šæ˜¾å¼æŒ‡å®š `language=\"zh\"` è€Œéç•™ç©ºã€‚Whisperå¯¹æœªæŒ‡å®šè¯­è¨€çš„å¤šè¯­ç§éŸ³é¢‘è¯†åˆ«ä¸å‡†ã€‚\n\n3. **Qï¼šæ¨¡å‹åŠ è½½æ…¢ï¼ˆé¦–æ¬¡ä¸‹è½½ï¼‰**  \n   Aï¼šæ‰‹åŠ¨ä¸‹è½½æƒé‡æ–‡ä»¶åˆ° `~/.cache/whisper/`ï¼Œé¿å…é‡å¤ä¸‹è½½ã€‚é“¾æ¥è§ model-card.md\n\n4. **Qï¼šé•¿éŸ³é¢‘ï¼ˆ>30minï¼‰OOM**  \n   Aï¼šä½¿ç”¨ `chunk_length=30` å‚æ•°åˆ†å—å¤„ç†ï¼Œå¹¶ç”¨ `stride=(5, 5)` åšé‡å å¯¹é½ã€‚\n\n5. **Qï¼šè¾“å‡ºæ—¶é—´æˆ³ä¸å‡†ï¼Œè¯ä¸éŸ³ç”»ä¸åŒæ­¥**  \n   Aï¼šå¯ç”¨ `word_timestamps=True` + åå¤„ç†å¯¹é½ï¼ˆå¦‚ä½¿ç”¨ `whisper-timestamped` åº“ï¼‰\n\n6. **Qï¼šåœ¨Mac M1ä¸Šè¿è¡Œæ…¢",
    "last_scanned": "2026-01-15T02:01:30.456278",
    "last_analyzed": "2026-01-15T05:16:55.305213",
    "screenshot": "static/screenshots/537603333.jpg",
    "ai_visual_summary": "è¯¥æˆªå›¾å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œwhisperâ€çš„GitHubé¡¹ç›®é¡µé¢ï¼Œå…¶æ ¸å¿ƒåŠŸèƒ½æ˜¯è¿›è¡Œ**é²æ£’çš„è¯­éŸ³è¯†åˆ«**ã€‚ä»ç•Œé¢è®¾è®¡æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„**æŠ€æœ¯æ–‡æ¡£/ä»£ç åº“ä¸»é¡µ**ï¼Œé‡‡ç”¨ç®€æ´çš„å•æ å¸ƒå±€å’Œæ¸…æ™°çš„æ–‡æœ¬æ’ç‰ˆï¼Œé‡ç‚¹çªå‡º**æŠ€æœ¯ä¿¡æ¯**ã€‚ä¸»è¦åŠŸèƒ½æ¨¡å—æ˜¯**é¡¹ç›®æè¿°ä¸æ€§èƒ½åˆ†æ**ï¼Œé€šè¿‡æ–‡å­—å’Œä¸¤ä¸ªå¹¶åˆ—çš„æŸ±çŠ¶å›¾ï¼ˆåˆ†åˆ«åœ¨Common Voice 15å’ŒFleursæ•°æ®é›†ä¸Šï¼‰è¯¦ç»†å¯¹æ¯”äº†`large-v3`å’Œ`large-v2`ç­‰æ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚å¯è§çš„å…³é”®æŠ€æœ¯å…³é”®è¯åŒ…æ‹¬`Whisper`ã€`speech recognition`ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰ã€`large-v3`ã€`large-v2`ã€`WER`ï¼ˆè¯é”™è¯¯ç‡ï¼‰å’Œ`CER`ï¼ˆå­—ç¬¦é”™è¯¯ç‡ï¼‰ã€‚æ•´ä¸ªåº”ç”¨æ—¨åœ¨å±•ç¤ºå’Œåˆ†æä¸€ä¸ªå¤§å‹è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨ä¸åŒè¯­è¨€å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚",
    "ai_rag_summary": null
  },
  {
    "id": "188660663",
    "name": "Real-Time-Voice-Cloning",
    "full_name": "CorentinJ/Real-Time-Voice-Cloning",
    "category": "tts_voice",
    "stars": 59198,
    "forks": 9412,
    "description": "Clone a voice in 5 seconds to generate arbitrary speech in real-time",
    "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning",
    "homepage": "",
    "language": "Python",
    "topics": "[\"deep-learning\", \"python\", \"pytorch\", \"tensorflow\", \"tts\", \"voice-cloning\"]",
    "created_at": "2019-05-26T08:56:15Z",
    "updated_at": "2026-01-14T15:17:17Z",
    "readme_content": null,
    "ai_summary": "åŸºäºSV2TTSæ¡†æ¶å’ŒWaveRNN vocoderçš„å®æ—¶è¯­éŸ³å…‹éš†å·¥å…·ï¼Œèƒ½åœ¨5ç§’å†…åˆ›å»ºæ•°å­—äººå£°æ¨¡å‹å¹¶ç”Ÿæˆä»»æ„æ–‡æœ¬å¯¹åº”çš„é«˜è´¨é‡è¯­éŸ³",
    "ai_tech_stack": "[\"Python\", \"TensorFlow 2.x\", \"SV2TTS\", \"WaveNet/WaveRNN\"]",
    "ai_use_cases": "[\"\\u5b9e\\u65f6\\u8bed\\u97f3\\u5408\\u6210\\u7cfb\\u7edf\\u96c6\\u6210\", \"\\u58f0\\u97f3\\u514b\\u9686\\u5de5\\u5177\\u5f00\\u53d1\", \"\\u6559\\u80b2\\u7814\\u7a76\\u573a\\u666f\\u6f14\\u793a\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "python train.py --help",
    "ai_tutorial": "### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–\n\nè¯¥é¡¹ç›®åœ¨2018â€“2020å¹´æœŸé—´å®ç°äº†**é¦–ä¸ªå¼€æºã€ç«¯åˆ°ç«¯ã€5ç§’å†…å®æ—¶è¯­éŸ³å…‹éš†çš„å®Œæ•´æµæ°´çº¿**ï¼Œå…¶æ ¸å¿ƒçªç ´åœ¨äºï¼š**å°†è¯´è¯äººéªŒè¯ï¼ˆSpeaker Verificationï¼‰ä¸­çš„åµŒå…¥è¡¨ç¤ºï¼ˆembeddingï¼‰æ— ç¼è¿ç§»åˆ°TTSåˆæˆç³»ç»Ÿä¸­ï¼Œä¸”åœ¨æ¶ˆè´¹çº§GPUä¸Šå®ç°ä½å»¶è¿Ÿæ¨ç†**ã€‚ä¸åŒæœŸæ–¹æ¡ˆç›¸æ¯”ï¼š\n\n- **Tacotron + WaveRNN çš„ç»„åˆé¦–æ¬¡è¢«å·¥ç¨‹åŒ–ä¸ºå¯å®æ—¶è¿è¡Œçš„è¯­éŸ³å…‹éš†ç³»ç»Ÿ**ï¼šå¤šæ•°ç ”ç©¶ä»…éªŒè¯ç¦»çº¿ç”Ÿæˆï¼Œè€Œæœ¬é¡¹ç›®é€šè¿‡æ¨¡å‹è½»é‡åŒ–ã€æ‰¹å¤„ç†ä¼˜åŒ–å’Œå¹¶è¡Œæ¨ç†å®ç°äº†<200msç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆåœ¨1080Tiä¸Šï¼‰ã€‚\n- **GE2EåµŒå…¥çš„å·¥ä¸šçº§å®ç”¨åŒ–**ï¼šé¦–æ¬¡å°†GE2EæŸå¤±å‡½æ•°ä»è¯´è¯äººè¯†åˆ«ä»»åŠ¡è¿ç§»åˆ°TTSä¸Šä¸‹æ–‡ï¼Œå®ç°â€œ5ç§’æ ·æœ¬â†’ç¨³å®šèº«ä»½åµŒå…¥â€è¿™ä¸€ä¸šç•Œéš¾é¢˜ï¼Œå¹¶ä¿æŒè·¨è¯­ç§ã€è·¨éŸ³è‰²æ³›åŒ–èƒ½åŠ›ã€‚\n- **å¼€æºé—­ç¯ç”Ÿæ€**ï¼šæ•´åˆäº†å½“æ—¶ä¸‰å¤§SOTAç»„ä»¶ï¼ˆSV2TTS, Tacotron, WaveRNNï¼‰ï¼Œæä¾›å¯è¿è¡Œçš„GUI/CLIå·¥å…·é“¾ï¼Œè€Œéä»…è®ºæ–‡å¤ç°ä»£ç ã€‚\n\nç›¸æ¯”å•†ä¸šæ–¹æ¡ˆï¼ˆResemble AI, ElevenLabsï¼‰æˆ–åç»­æ¨¡å‹ï¼ˆVITS, FastSpeech2ï¼‰ï¼Œå®ƒç‰ºç‰²äº†éŸ³è´¨ä¸Šé™æ¢å–**ä½èµ„æºé—¨æ§›+å®æ—¶æ€§+å¼€æºè‡ªç”±åº¦**ï¼Œè‡³ä»Šä»æ˜¯æ•™è‚²ã€åŸå‹éªŒè¯å’Œè¾¹ç¼˜éƒ¨ç½²çš„é»„é‡‘åŸºå‡†ã€‚\n\n---\n\n### ğŸ”¥ æŠ€æœ¯äº®ç‚¹\n\n- **ä¸‰é˜¶æ®µåµŒå…¥å…±äº«æ¶æ„**ï¼šç¼–ç å™¨ï¼ˆGE2Eï¼‰ã€åˆæˆå™¨ï¼ˆTacotronï¼‰ã€å£°ç å™¨ï¼ˆWaveRNNï¼‰å…±ç”¨åŒä¸€è¯­éŸ³åµŒå…¥ç©ºé—´ã€‚è¯¥è®¾è®¡ä½¿â€œ5ç§’æ ·æœ¬â€èƒ½ç›´æ¥ä½œä¸ºæ¡ä»¶å‘é‡æ³¨å…¥TTSï¼Œæ— éœ€å¾®è°ƒã€‚\n- **WaveRNN çš„åºåˆ—å¹¶è¡ŒåŒ–ä¼˜åŒ–**ï¼šåŸå§‹WaveRNNä¸ºè‡ªå›å½’é€é‡‡æ ·ç”Ÿæˆï¼Œå»¶è¿Ÿæé«˜ã€‚æœ¬é¡¹ç›®é‡‡ç”¨**å¤šå°ºåº¦å¹¶è¡Œé¢„æµ‹ï¼ˆmulti-scale predictionï¼‰+ æ»‘åŠ¨çª—å£ç¼“å­˜æœºåˆ¶**ï¼Œå°†å•æ ·æœ¬æ¨ç†ä»~100msé™è‡³<5msã€‚\n- **åŠ¨æ€åµŒå…¥å½’ä¸€åŒ–ï¼ˆDynamic Embedding Normalizationï¼‰**ï¼šå¯¹è¾“å…¥çš„è¯´è¯äººåµŒå…¥è¿›è¡ŒL2å½’ä¸€åŒ– + å‡å€¼åç§»æ ¡å‡†ï¼Œæå‡è·¨æ ·æœ¬ä¸€è‡´æ€§ï¼Œè§£å†³â€œå°æ ·æœ¬å™ªå£°æ•æ„Ÿâ€é—®é¢˜ã€‚\n- **æ··åˆç²¾åº¦æ¨ç†æ”¯æŒ**ï¼šåœ¨CUDAåˆ†æ”¯ä¸­ä¸»åŠ¨å¯ç”¨FP16æ¨æ–­ï¼ˆTensorRTæœªä½¿ç”¨ï¼‰ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨è€Œä¸æŸå¤±éŸ³è´¨ã€‚\n\n---\n\n### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ\n\n#### 1. æ•´ä½“æ¶æ„ï¼ˆæ–‡å­—æè¿°ï¼‰\n\n```\n[è¾“å…¥éŸ³é¢‘] â†’ [Speaker Encoder (GE2E)] â†’ [Voice Embedding (512-d)]\n                                     â†“\n[æ–‡æœ¬è¾“å…¥] â†’ [Tacotron Synthesizer] â†’ [Mel-Spectrogram (80-band)]\n                                     â†“\n[Mel + Voice Embedding] â†’ [WaveRNN Vocoder] â†’ [Raw Audio (16kHz)]\n```\n\n#### 2. æ ¸å¿ƒæ¨¡å—èŒè´£\n\n| æ¨¡å— | èŒè´£ |\n|------|------|\n| **Speaker Encoder** | åŸºäºLSTM+CNNçš„GE2Eæ¨¡å‹ï¼Œå°†â‰¤5séŸ³é¢‘ç¼–ç ä¸º512ç»´åµŒå…¥å‘é‡ã€‚ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ– + ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±è®­ç»ƒã€‚ |\n| **Tacotron Synthesizer** | æ”¹è¿›ç‰ˆTacotron-1ï¼ˆé2ï¼‰ï¼Œå¸¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¾“å‡º80-band Melè°±å›¾ã€‚è¾“å…¥å«æ–‡æœ¬åµŒå…¥+è¯´è¯äººåµŒå…¥ï¼ˆæ‹¼æ¥åå–‚å…¥Decoderï¼‰ã€‚ |\n| **WaveRNN Vocoder** | åŒå±‚RNN + æ¡ä»¶ç”Ÿæˆï¼šä»¥Melè°±å›¾ä¸ºæ¡ä»¶ï¼Œé¢„æµ‹16bit PCMé‡‡æ ·ç‚¹ã€‚é‡‡ç”¨â€œmu-lawé‡åŒ– + åˆ†æ²»é¢„æµ‹â€å®ç°é«˜é€Ÿæ¨ç†ã€‚ |\n\n#### 3. æ•°æ®æµå‘\n\n```\nè¾“å…¥éŸ³é¢‘ï¼ˆWAV, 16kHzï¼‰  \n    â†“ (é¢„å¤„ç†: å»å™ªã€åˆ†å¸§ã€æ¢…å°”å˜æ¢)  \nSpeaker Encoder â†’ Voice Embedding âˆˆ â„âµÂ¹Â²  \n    â†“  \næ–‡æœ¬ï¼ˆASCIIï¼‰â†’ Text Encoder â†’ å­—ç¬¦åµŒå…¥ + Attention  \n    â†“  \nTacotron Decoder (with speaker embedding concat) â†’ Mel Spectrogram âˆˆ â„^(TÃ—80)  \n    â†“  \nWaveRNN (conditioned on Mel + optional speaker emb) â†’ 16-bit PCM Audio  \n```\n\n#### 4. å…³é”®è®¾è®¡æ¨¡å¼\n\n- **ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰**ï¼š`Synthesizer` å’Œ `Vocoder` æ¥å£æŠ½è±¡ï¼Œå…è®¸åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼ˆå¦‚æœªæ¥æ›¿æ¢Tacotronä¸ºFastSpeech2ï¼‰ã€‚  \n- **å·¥å‚æ¨¡å¼ï¼ˆFactory Patternï¼‰**ï¼š`load_model()` å‡½æ•°æ ¹æ®é…ç½®åŠ¨æ€åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆHugging Faceè·¯å¾„ï¼‰ï¼Œè§£è€¦æ¨¡å‹ä¸‹è½½ä¸æ¨ç†é€»è¾‘ã€‚  \n- **ç®¡é“æ¨¡å¼ï¼ˆPipeline Patternï¼‰**ï¼šæ¯ä¸ªæ¨¡å—ç‹¬ç«‹å°è£…ï¼Œé€šè¿‡æ ‡å‡†åŒ–æ¥å£ï¼ˆå¦‚ `embed_utterance()`, `synthesize_spectrogram()`ï¼‰ä¸²è”ï¼Œæ”¯æŒå¼‚æ­¥/å¹¶è¡Œè°ƒåº¦ã€‚\n\n---\n\n### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ\n\n| æŠ€æœ¯ | é€‰æ‹©åŸå›  | æ›¿ä»£æ–¹æ¡ˆ | ç‰ˆæœ¬æ³¨æ„ |\n|------|----------|-----------|----------|\n| **PyTorch** | åŠ¨æ€å›¾ä¾¿äºè°ƒè¯•ä¸‰é˜¶æ®µæ¨¡å‹ï¼Œç¤¾åŒºç”Ÿæ€æ”¯æŒGE2Eå®ç° | TensorFlow (é™æ€å›¾ä¸åˆ©äºåµŒå…¥åŠ¨æ€æ³¨å…¥) | æ¨è1.7â€“1.9ï¼›é«˜ç‰ˆæœ¬ï¼ˆ>2.0ï¼‰éœ€ä¿®æ”¹`torch.nn.utils.rnn.pack_padded_sequence`è°ƒç”¨ |\n| **NumPy / Librosa** | éŸ³é¢‘é¢„å¤„ç†ã€æ¢…å°”æ»¤æ³¢å™¨ç»„æ„å»º | torchaudio (å½“æ—¶ä¸æ”¯æŒWindowså®Œæ•´ç¼–è¯‘) | Librosa 0.8+ å…¼å®¹æ€§å·®ï¼Œå»ºè®®é”å®š0.7.2 |\n| **WaveRNN (fatchord)** | å”¯ä¸€å¼€æºä¸”å¯å®æ—¶è¿è¡Œçš„è‡ªå›å½’å£°ç å™¨ | Griffin-Limï¼ˆéŸ³è´¨å·®ï¼‰ã€Parallel WaveGANï¼ˆå»¶è¿Ÿé«˜ï¼‰ | å¿…é¡»ä½¿ç”¨`fatchord/WaveRNN`åˆ†æ”¯ï¼Œéå®˜æ–¹ç‰ˆä¸æ”¯æŒæ¡ä»¶è¾“å…¥ |\n| **uv** | æ›¿ä»£pip/virtualenvï¼Œè§£å†³ä¾èµ–å†²çªå’Œè·¨å¹³å°é—®é¢˜ | pip + condaï¼ˆç¯å¢ƒçˆ†ç‚¸ï¼‰ | `uv`æ˜¯å…³é”®ï¼Œå¦åˆ™`ffmpeg`è·¯å¾„ã€CUDAç‰ˆæœ¬æ˜“é”™ |\n| **PyQt5 / Tkinter** | GUIè½»é‡çº§ã€æ— Webä¾èµ– | Web UI (Gradio) â€”â€” å½“æ—¶æœªæˆç†Ÿï¼Œä¸”éœ€æœåŠ¡å™¨éƒ¨ç½² |\n\n> âš ï¸ CUDAå…¼å®¹æ€§é™·é˜±ï¼šé¡¹ç›®é»˜è®¤ä½¿ç”¨CUDA 10.2 + cuDNN 7.6ã€‚è‹¥ç”¨RTX30+æ˜¾å¡ï¼Œè¯·æ‰‹åŠ¨é™çº§PyTorchè‡³1.7æˆ–ä½¿ç”¨`--extra cpu`è§„é¿ã€‚\n\n---\n\n### ğŸ“¦ å®‰è£…ä¸é…ç½®\n\n```bash\n# 1. ç¡®è®¤ffmpegå·²å®‰è£…ï¼ˆWindowsç”¨æˆ·å¯ç›´æ¥ä¸‹è½½exeå¹¶æ·»åŠ PATHï¼‰\nffmpeg -version\n\n# 2. å®‰è£…uvï¼ˆè·¨å¹³å°ç»Ÿä¸€åŒ…ç®¡ç†å™¨ï¼‰\n# Windows:\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Linux/macOS:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# 3. å®‰è£…é¡¹ç›®ä¾èµ–ï¼ˆè‡ªåŠ¨åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼‰\ngit clone https://github.com/CorentinJ/Real-Time-Voice-Cloning.git\ncd Real-Time-Voice-Cloning\n\n# ä½¿ç”¨GPUç‰ˆæœ¬ï¼ˆæ¨èNVIDIAï¼‰\nuv run --extra cuda demo_toolbox.py\n\n# æˆ–CPUç‰ˆï¼ˆæ— GPUæ—¶ï¼‰\nuv run --extra cpu demo_toolbox.py\n\n# å¯åŠ¨CLIæ¨¡å¼ï¼ˆæ— GUIï¼Œé€‚åˆæœåŠ¡å™¨ï¼‰\nuv run --extra cuda demo_cli.py\n```\n\n> âœ… é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä»Hugging Faceä¸‹è½½~1.2GBæ¨¡å‹æƒé‡ï¼ˆ`encoder`, `synthesizer`, `vocoder`ï¼‰ï¼Œéœ€ç¨³å®šç½‘ç»œã€‚\n\n---\n\n### ğŸ® ä½¿ç”¨ç¤ºä¾‹\n\n```python\n# demo_cli.py ç¤ºä¾‹ï¼šä½¿ç”¨5ç§’éŸ³é¢‘å…‹éš†è¯­éŸ³å¹¶åˆæˆâ€œHello worldâ€\n\nfrom synthesizer.inference import Synthesizer\nfrom encoder import inference as encoder\nfrom vocoder import inference as vocoder\nimport numpy as np\n\n# 1. åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä»Hugging Faceä¸‹è½½ï¼‰\nencoder.load_model(\"saved_models/default/encoder.pt\")\nsynthesizer = Synthesizer(\"saved_models/default/synthesizer.pt\")\nvocoder.load_model(\"saved_models/default/vocoder.pt\")\n\n# 2. è¾“å…¥ï¼š5ç§’å‚è€ƒè¯­éŸ³ï¼ˆå¿…é¡»æ˜¯16kHzå•å£°é“WAVï¼‰\nref_wav_path = \"audio_samples/my_voice.wav\"\nref_wav = encoder.preprocess_wav(ref_wav_path)\n\n# 3. ç”Ÿæˆè¯´è¯äººåµŒå…¥\nembed = encoder.embed_utterance(ref_wav)  # è¾“å‡º: (512,) å‘é‡\n\n# 4. è¾“å…¥æ–‡æœ¬\ntext = \"Hello world, this is a cloned voice.\"\n\n# 5. åˆæˆæ¢…å°”è°±å›¾ï¼ˆå¸¦è¯´è¯äººæ¡ä»¶ï¼‰\nmel = synthesizer.synthesize_spectrogram(texts=[text], embeddings=[embed])  # è¾“å‡º: (80, T)\n\n# 6. å£°ç å™¨ç”ŸæˆéŸ³é¢‘\nwav = vocoder.infer_waveform(mel)  # è¾“å‡º: (N,) 16-bit PCM\n\n# 7. ä¿å­˜ç»“æœ\nfrom scipy.io import wavfile\nwavfile.write(\"output_cloned.wav\", 16000, wav.astype(np.int16))\n```\n\n**é¢„æœŸè¾“å‡º**ï¼šç”Ÿæˆä¸€ä¸ªä¸`my_voice.wav`éŸ³è‰²ä¸€è‡´ã€å†…å®¹ä¸ºâ€œHello worldâ€çš„è¯­éŸ³æ–‡ä»¶ï¼Œå»¶è¿Ÿçº¦3â€“5ç§’ï¼ˆå«æ¨ç†+ä¿å­˜ï¼‰ã€‚\n\n---\n\n### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–\n\n- **æ€§èƒ½ç“¶é¢ˆ**ï¼š\n  - **Tacotronåˆæˆå™¨**ï¼šåºåˆ—é•¿åº¦Tå½±å“æ˜¾è‘—ã€‚é•¿å¥ï¼ˆ>200å­—ç¬¦ï¼‰ç”Ÿæˆæ—¶é—´å¯è¾¾10s+ã€‚\n  - **WaveRNN**ï¼šè‡ªå›å½’æœ¬è´¨å¯¼è‡´é‡‡æ ·ç‡å—é™ï¼ˆ~300Hzï¼‰ï¼Œæ— æ³•å¹¶è¡ŒåŒ–è‡³GPUçº§ååã€‚\n\n- **ç”Ÿäº§ç¯å¢ƒæ‰©å±•å»ºè®®**ï¼š\n  - æ›¿æ¢WaveRNNä¸º**HiFi-GAN**æˆ–**Parallel WaveGAN**ï¼Œæå‡é€Ÿåº¦10x+ã€‚\n  - ä½¿ç”¨ONNXå¯¼å‡ºTacotron + Encoderï¼Œéƒ¨ç½²åˆ°TensorRTï¼ˆæ”¯æŒINT8é‡åŒ–ï¼‰ã€‚\n  - å¼•å…¥ç¼“å­˜æœºåˆ¶ï¼šå¯¹é«˜é¢‘è¯´è¯äººåµŒå…¥ç¼“å­˜äºå†…å­˜ï¼Œé¿å…é‡å¤ç¼–ç ã€‚\n\n- **èµ„æºæ¶ˆè€—ä¼°ç®—ï¼ˆGPUç‰ˆï¼‰**ï¼š\n  | æ¨¡å— | æ˜¾å­˜å ç”¨ | æ¨ç†å»¶è¿Ÿ |\n  |------|----------|-----------|\n  | Encoder | ~150 MB | <20ms |\n  | Synthesizer | ~800 MB | 3â€“6sï¼ˆä¸­ç­‰é•¿åº¦ï¼‰ |\n  | WaveRNN | ~400 MB | 1.5â€“2sï¼ˆå®æ—¶æµå¼è¾“å‡ºï¼‰ |\n  | **æ€»è®¡** | ~1.4 GB | **~5ç§’ç«¯åˆ°ç«¯** |\n\n> âœ… åœ¨Jetson Xavierä¸Šå¯è¿è¡Œï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡è¯­éŸ³åŠ©æ‰‹ã€‚\n\n---\n\n### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—\n\n#### å…³é”®æ‰©å±•ç‚¹ï¼š\n- `encoder/inference.py`ï¼šä¿®æ”¹GE2EæŸå¤±å‡½æ•°ã€æ›´æ¢Backboneï¼ˆå¦‚ResNet â†’ ECAPA-TDNNï¼‰ã€‚\n- `synthesizer/model.py`ï¼šæ›¿æ¢Tacotronä¸ºFastSpeech2ï¼Œæ”¯æŒæ›´ç¨³å®šéŸ³é«˜æ§åˆ¶ã€‚\n- `vocoder/models/wavernn.py`ï¼šæ·»åŠ æ¡ä»¶è¾“å…¥ï¼ˆspeaker embeddingï¼‰åˆ°RNNå±‚ã€‚\n\n#### APIæ¥å£è¯´æ˜ï¼š\n```python\nencoder.embed_utterance(wav) â†’ np.ndarray[512]  # æ ¸å¿ƒæ¥å£\nsynthesizer.synthesize_spectrogram(texts, embeddings) â†’ np.ndarray[80, T]\nvocoder.infer_waveform(mel) â†’ np.ndarray[N]     # è¾“å‡º16kHz PCM\n```\n\n#### æ·»åŠ è‡ªå®šä¹‰åŠŸèƒ½ç¤ºä¾‹ï¼š\n```python\n# å¢åŠ éŸ³é‡æ§åˆ¶ï¼ˆåœ¨åˆæˆåï¼‰\nwav = vocoder.infer_waveform(mel)\nwav = wav * 0.8  # å‡å°‘20%éŸ³é‡\n```\n\n> ğŸ’¡ æ¨èï¼šç”¨`pydub`åšåå¤„ç†ï¼Œæˆ–é›†æˆ`librosa.effects.pitch_shift()`å®ç°éŸ³é«˜å˜æ¢ã€‚\n\n---\n\n### â— å¸¸è§é—®é¢˜ä¸é¿å‘\n\n1. **Q: `ImportError: No module named 'torch'`**  \n   A: æœªä½¿ç”¨`uv run`å¯åŠ¨ã€‚å§‹ç»ˆç”¨ `uv run --extra cuda demo_toolbox.py`ï¼Œä¸è¦ç›´æ¥`python demo_toolbox.py`\n\n2. **Q: GPUè¿è¡ŒæŠ¥é”™ `CUDA out of memory`**  \n   A: å‡å°‘æ‰¹é‡å¤§å°ï¼ˆåœ¨`synthesizer/inference.py`ä¸­è®¾`batch_size=1`ï¼‰ï¼Œæˆ–æ”¹ç”¨CPUæ¨¡å¼ã€‚\n\n3. **Q: ç”Ÿæˆè¯­éŸ³æœ‰æ‚éŸ³/é‡‘å±æ„Ÿ**  \n   A: WaveRNNæœªæ­£ç¡®åŠ è½½ã€‚ç¡®è®¤`vocoder.pt`æ–‡ä»¶å®Œæ•´ï¼Œæ‰‹åŠ¨ä»Hugging Faceé‡ä¸‹ã€‚\n\n4. **Q: éŸ³é¢‘é‡‡æ ·ç‡ä¸åŒ¹é…ï¼ˆ48kHz vs 16kHzï¼‰**  \n   A: ä½¿ç”¨`ffmpeg -i input.wav -ar 16000 output.wav`é¢„å¤„ç†è¾“å…¥éŸ³é¢‘ã€‚\n\n5. **Q: GUIç•Œé¢ç©ºç™½æˆ–å´©æºƒ**  \n   A: å®‰è£…PyQt5ï¼š`pip install PyQt5==5.15.9`ï¼Œé¿å…é«˜ç‰ˆæœ¬å†²çªã€‚\n\n6. **Q: æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼ˆHugging Face 403ï¼‰**  \n   A: è®¾ç½®ä»£ç†ï¼š`export HF_HUB_ENABLE_HF_TRANSFER=1 && uv run ...`\n\n7. **Q: å¤šäººå…‹éš†æ—¶äº¤å‰å¹²æ‰°ï¼Ÿ**  \n   A: SV2TTSæœ¬è´¨æ˜¯speaker-conditionedï¼Œéœ€ä¸ºæ¯ä¸ªè¯´è¯äººå•ç‹¬ä¿å­˜embeddingã€‚ä¸è¦æ··ç”¨ã€‚\n\n---\n\n### ğŸš€ è¿›é˜¶å­¦ä¹ ",
    "last_scanned": "2026-01-15T02:01:30.459459",
    "last_analyzed": "2026-01-15T10:43:14.452322",
    "screenshot": "static/screenshots/188660663.jpg",
    "ai_visual_summary": "è¯¥ GitHub é¡¹ç›® `Real-Time-Voice-Cloning` æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„å¼€æºè¯­éŸ³å…‹éš†å·¥å…·ï¼Œå…¶æ ¸å¿ƒåŠŸèƒ½æ˜¯é€šè¿‡å°‘é‡ï¼ˆ5ç§’ï¼‰çš„è¯­éŸ³æ ·æœ¬ï¼Œå®æ—¶åˆæˆä»»æ„å†…å®¹çš„è¯­éŸ³ã€‚ç•Œé¢è®¾è®¡é£æ ¼ç®€æ´ã€åŠŸèƒ½å¯¼å‘ï¼Œä¸»è¦åŠŸèƒ½æ¨¡å—åŒ…æ‹¬é¡¹ç›®æ¦‚è¿°ã€å…³é”®æŠ€æœ¯ï¼ˆå¦‚ SV2TTSã€WaveRNNã€Tacotronã€GE2Eï¼‰çš„ä»‹ç»ï¼Œä»¥åŠè¯¦ç»†çš„å·¥å…·ç®±è¿è¡ŒæŒ‡å—ã€‚å¯è§çš„å…³é”®æŠ€æœ¯è¯åŒ…æ‹¬ `Transfer Learning`ï¼ˆè¿ç§»å­¦ä¹ ï¼‰ã€`Text-To-Speech Synthesis`ï¼ˆæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼‰ã€`Neural Audio Synthesis`ï¼ˆç¥ç»éŸ³é¢‘åˆæˆï¼‰å’Œ `Speaker Verification`ï¼ˆè¯´è¯äººéªŒè¯ï¼‰ã€‚è¯¥åº”ç”¨æ—¨åœ¨æä¾›ä¸€ä¸ªé«˜ä¿çœŸåº¦çš„ã€å¼€æºçš„è¯­éŸ³å…‹éš†è§£å†³æ–¹æ¡ˆï¼Œä¸å•†ä¸š SaaS æœåŠ¡å½¢æˆå¯¹æ¯”ã€‚",
    "ai_rag_summary": null
  },
  {
    "id": "743230067",
    "name": "GPT-SoVITS",
    "full_name": "RVC-Boss/GPT-SoVITS",
    "category": "tts_voice",
    "stars": 54075,
    "forks": 5922,
    "description": "1 min voice data can also be used to train a good TTS model! (few shot voice cloning)",
    "url": "https://github.com/RVC-Boss/GPT-SoVITS",
    "homepage": "",
    "language": "Python",
    "topics": "[\"text-to-speech\", \"tts\", \"vits\", \"voice-clone\", \"voice-cloneai\", \"voice-cloning\"]",
    "created_at": "2024-01-14T18:05:21Z",
    "updated_at": "2026-01-14T17:28:36Z",
    "readme_content": null,
    "ai_summary": "åŸºäºGPTä¸SoVITSçš„å°‘æ ·æœ¬è¯­éŸ³å…‹éš†åŠè·¨è¯­è¨€TTSç³»ç»Ÿï¼Œæ ¸å¿ƒç‰¹ç‚¹æ˜¯5ç§’å³å¯å®ç°é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆï¼Œ1åˆ†é’Ÿè®­ç»ƒæ•°æ®å¯æ˜¾è‘—æå‡éŸ³è´¨å¹¶æ”¯æŒå¤šè¯­ç§æ— ç¼è½¬æ¢ã€‚",
    "ai_tech_stack": "[\"FastAPI\", \"Hugging Face Transformers/Diffusers\", \"PyTorch\", \"Librosa\", \"ASR\\u5f15\\u64ce\\uff08Whisper/\\u7aef\\u4fa7\\u65b9\\u6848\\uff09\"]",
    "ai_use_cases": "[\"\\u97f3\\u4e50\\u5236\\u4f5c\\u4e2d\\u7684\\u58f0\\u4e50\\u66ff\\u6362\\u4e0e\\u6df7\\u97f3\", \"\\u591a\\u8bed\\u8a00\\u8bed\\u97f3\\u52a9\\u624b\\u5f00\\u53d1\", \"\\u5f71\\u89c6\\u914d\\u97f3\\u81ea\\u52a8\\u5316\\u751f\\u6210\", \"\\u4e2a\\u6027\\u5316AI\\u5ba2\\u670d\\u58f0\\u97f3\\u5b9a\\u5236\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "docker run xxxxrt666/gpt-sovits:latest && python webui.py --port 7890",
    "ai_tutorial": "### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–\n\nGPT-SoVITS çš„æ ¸å¿ƒå·®å¼‚åŒ–åœ¨äº**åœ¨ 1 åˆ†é’Ÿå†…å®ç°é«˜ä¿çœŸã€è·¨è¯­è¨€çš„å°‘æ ·æœ¬è¯­éŸ³å…‹éš†ï¼Œä¸”æ¨ç†é€Ÿåº¦è¾¾ RTF < 0.03ï¼ˆæ¶ˆè´¹çº§ GPUï¼‰**ï¼Œè¿œè¶…åŒç±»æ–¹æ¡ˆï¼ˆå¦‚ YourTTSã€Coqui TTSã€VoiceCloning 2.0ï¼‰ã€‚ä¼ ç»Ÿå°‘æ ·æœ¬ TTS éœ€è¦ 5â€“10 åˆ†é’Ÿæ•°æ®æ‰èƒ½å‹‰å¼ºå¯ç”¨ï¼Œè€Œ GPT-SoVITS åœ¨ <60s æ•°æ®ä¸‹å³è¾¾åˆ°å¯å•†ç”¨çº§ç›¸ä¼¼åº¦ã€‚å…¶çªç ´ç‚¹åœ¨äºï¼š\n\n1. **GPT + SoVITS åŒåˆ†æ”¯æ¶æ„**ï¼šé¦–æ¬¡å°† Transformer è¯­è¨€å»ºæ¨¡ï¼ˆGPTï¼‰ä¸ vocoder çš„éŸ³ç´ -é¢‘è°±è§£è€¦ï¼ˆSoVITSï¼‰è¿›è¡Œç«¯åˆ°ç«¯ååŒè®­ç»ƒï¼Œè§£å†³äº†ä¼ ç»Ÿ TTS ä¸­â€œè¯­ä¹‰ä¸€è‡´ä½†è¯­éŸ³å¤±çœŸâ€æˆ–â€œéŸ³è‰²é€¼çœŸä½†è¯­è°ƒæœºæ¢°â€çš„æ ¹æœ¬çŸ›ç›¾ã€‚\n2. **è·¨è¯­è¨€é›¶æ ·æœ¬æ¨ç†**ï¼šæ¨¡å‹åœ¨ä¸­æ–‡è®­ç»ƒåï¼Œå¯ç›´æ¥ç”Ÿæˆè‹±æ–‡ã€æ—¥æ–‡ç­‰éè®­ç»ƒè¯­è¨€çš„è¯­éŸ³ï¼ˆä»…éœ€æ–‡æœ¬å¯¹é½ï¼‰ï¼Œæ— éœ€è¯­è¨€ç‰¹å®šå¾®è°ƒâ€”â€”è¿™åœ¨ VITS ç³»åˆ—ä¸­å±é¦–åˆ›ã€‚\n3. **WebUI å…¨é“¾è·¯é—­ç¯**ï¼šé›†æˆè¯­éŸ³åˆ†ç¦»ã€è‡ªåŠ¨åˆ‡åˆ†ã€ASR æ ‡æ³¨ã€æ•°æ®å¢å¼ºï¼Œä½¿â€œ1åˆ†é’Ÿè®­ç»ƒâ€ä¸å†æ˜¯æŠ€æœ¯å™±å¤´ï¼Œè€Œæ˜¯å¯å¤ç°çš„å·¥ç¨‹æµç¨‹ã€‚\n\nå¯¹æ¯” RVCï¼ˆä»…éŸ³è‰²è¿ç§»ï¼‰ã€Fish-TTSï¼ˆéœ€ 5min+ æ•°æ®ï¼‰ã€VITS-Finetuneï¼ˆæ— è·¨è¯­è¨€ï¼‰ï¼ŒGPT-SoVITS åœ¨**ä½æ•°æ®é‡ + é«˜æ³›åŒ–æ€§ + å®æ—¶æ¨ç†**ä¸‰è§’çŸ›ç›¾ä¸­æ‰¾åˆ°äº†æœ€ä¼˜å¹³è¡¡ç‚¹ã€‚\n\n---\n\n### ğŸ”¥ æŠ€æœ¯äº®ç‚¹\n\n1. **GPT å‰ç½®ç¼–ç å™¨ï¼šè¯­ä¹‰é©±åŠ¨çš„éŸ³ç´ åºåˆ—ç”Ÿæˆ**\n   - å°†æ–‡æœ¬è¾“å…¥ GPT-2/3 æ¨¡å‹ï¼ˆè½»é‡ç‰ˆï¼‰ï¼Œè¾“å‡ºä¸º**ç¦»æ•£éŸ³ç´ æ ‡ç­¾åºåˆ—**ï¼ˆéè¿ç»­å‘é‡ï¼‰ï¼Œè€Œéç›´æ¥é¢„æµ‹ Mel é¢‘è°±ã€‚\n   - å…³é”®åˆ›æ–°ï¼šGPT è¾“å‡ºçš„ token åºåˆ—ä½œä¸º SoVITS çš„â€œè¯­ä¹‰æ§åˆ¶ç â€ï¼Œä½¿æ¨¡å‹èƒ½å­¦ä¹ â€œè¯­ä¹‰â†’éŸ³è‰²â€çš„æ˜ å°„ï¼Œè€Œéå•çº¯å£°å­¦å»ºæ¨¡ã€‚\n\n2. **SoVITS è§£è€¦å¼ Vocoder + å¯å¾®åˆ† VQ**\n   - åŸºäº VITS æ”¹é€ çš„ SoVITS æ¨¡å—ï¼Œå°†è¯­éŸ³ç¼–ç ä¸ºï¼š\n     - éŸ³ç´ åºåˆ—ï¼ˆæ¥è‡ª GPTï¼‰\n     - è¯´è¯äººåµŒå…¥ï¼ˆSpeaker Embeddingï¼Œ128-dimï¼‰\n     - æŒç»­æ—¶é—´ä¸åŸºé¢‘ï¼ˆF0ï¼‰åŒ…ç»œ\n   - ä½¿ç”¨**å¯å¾®åˆ†å‘é‡é‡åŒ–ï¼ˆDVQï¼‰** æ›¿ä»£ä¼ ç»Ÿ VQ-VAEï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒæ—¶æ¢¯åº¦å¯å›ä¼ è‡³ GPT å±‚ã€‚\n\n3. **åŠ¨æ€ä¸Šä¸‹æ–‡çª—å£å‹ç¼©**\n   - è®­ç»ƒæ—¶ä½¿ç”¨ 5sâ€“60s é•¿è¯­éŸ³ç‰‡æ®µï¼Œä½†æ¨ç†æ—¶ä»…éœ€å‰ 1â€“2s çš„å‚è€ƒéŸ³é¢‘æå– speaker embeddingã€‚\n   - åˆ©ç”¨**æ»‘åŠ¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSliding Attentionï¼‰** åœ¨ç¼–ç å™¨ä¸­èšç„¦äºâ€œæœ€å…·ä»£è¡¨æ€§çš„éŸ³æ®µâ€ï¼Œè€Œéæ•´æ®µå¹³å‡ï¼Œæ˜¾è‘—æå‡çŸ­æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚\n\n4. **RTF < 0.03 å®ç°ï¼šå¹¶è¡Œè§£ç  + æ¨¡å‹è’¸é¦**\n   - GPT è¾“å‡ºé‡‡ç”¨ batched autoregressive è§£ç ï¼ˆéé€ tokenï¼‰ï¼ŒSoVITS ä½¿ç”¨**æµå¼ç¥ç»ç½‘ç»œ**ï¼ˆstreaming neural vocoderï¼‰å®ç° 1:1 å®æ—¶åˆæˆã€‚\n   - æ¨¡å‹é€šè¿‡çŸ¥è¯†è’¸é¦å‹ç¼©ï¼šå¤§æ¨¡å‹ï¼ˆGPT-3.5 çº§ï¼‰æŒ‡å¯¼å°æ¨¡å‹ï¼ˆGPT-2 å°ç‰ˆï¼‰ç”ŸæˆéŸ³ç´ åºåˆ—ï¼Œæ¨ç†ä»…éœ€ 0.7B å‚æ•°ã€‚\n\n---\n\n### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ\n\n```\n[Text Input] \n      â†“\n[GPT Encoder] â†’ (Discrete Phoneme Tokens)  \n      â†“                          â†˜\n[Speaker Embedder] â† [1-min Audio Ref]   [F0/Dur Predictor]\n      â†“                          â†—\n[SoVITS Decoder] â†â”€â”€â”€[DVQ Latents + F0 + Dur]\n      â†“\n[Waveform Synthesis] â†’ [Output WAV]\n```\n\n#### æ ¸å¿ƒæ¨¡å—èŒè´£ï¼š\n\n| æ¨¡å— | èŒè´£ |\n|------|------|\n| **GPT Encoder** | å°†è¾“å…¥æ–‡æœ¬æ˜ å°„ä¸ºç¦»æ•£éŸ³ç´ åºåˆ—ï¼ˆtokenizedï¼‰ï¼Œå­¦ä¹ è¯­ä¹‰-è¯­éŸ³çš„é«˜å±‚å…³è” |\n| **Speaker Embedder** | ä»å‚è€ƒéŸ³é¢‘æå–è¯´è¯äººåµŒå…¥ï¼Œä½¿ç”¨ WavLM + MLP å‹ç¼©è‡³ 128dï¼Œæ”¯æŒè·¨è¯­è¨€æ³›åŒ– |\n| **F0/Dur Predictor** | é¢„æµ‹åŸºé¢‘ä¸éŸ³ç´ æŒç»­æ—¶é—´ï¼ˆé GPT è¾“å‡ºï¼‰ï¼Œæå‡éŸµå¾‹è‡ªç„¶åº¦ |\n| **SoVITS Decoder** | èåˆéŸ³ç´ ã€åµŒå…¥ã€F0/Dur ç”Ÿæˆæ¢…å°”è°±ï¼Œä½¿ç”¨æ”¹è¿›çš„ Glow-TTS æ¶æ„ + DVQ |\n| **WaveNet Vocoder** | å°†æ¢…å°”è°±è½¬ä¸ºæ³¢å½¢ï¼ˆè½»é‡ç‰ˆ HiFi-GANï¼Œæ”¯æŒæµå¼ï¼‰ |\n\n#### æ•°æ®æµå‘ï¼š\n\n`æ–‡æœ¬ â†’ GPT ç¼–ç  â†’ éŸ³ç´ åºåˆ— + Speaker Embedding + F0/Dur â†’ SoVITS è§£ç å™¨ â†’ æ¢…å°”è°± â†’ WaveNet â†’ æ³¢å½¢`\n\n#### è®¾è®¡æ¨¡å¼ï¼š\n\n- **Pipeline + Composite Pattern**ï¼šå„æ¨¡å—ç‹¬ç«‹è®­ç»ƒã€å¯æ’æ‹”ï¼Œæ”¯æŒæ›¿æ¢ GPT ä¸º Whisper æˆ– BERTã€‚\n- **Observer Patternï¼ˆæ¨ç†æ—¶ï¼‰**ï¼šWebUI ç›‘å¬è®­ç»ƒè¿›åº¦ã€ASR ç»“æœï¼ŒåŠ¨æ€æ›´æ–°æ¨¡å‹åŠ è½½çŠ¶æ€ã€‚\n- **Strategy Pattern**ï¼šæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼ï¼ˆZero-shot / Few-shot / Cross-lingualï¼‰ï¼Œé€šè¿‡é…ç½®æ–‡ä»¶åˆ‡æ¢æŸå¤±å‡½æ•°ä¸é‡‡æ ·ç­–ç•¥ã€‚\n\n---\n\n### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ\n\n| ç»„ä»¶ | é€‰æ‹©åŸå›  | æ›¿ä»£æ–¹æ¡ˆ | æ³¨æ„äº‹é¡¹ |\n|------|----------|-----------|-----------|\n| **PyTorch 2.5.1** | æ”¯æŒ `torch.compile()` åŠ é€Ÿ GPT/SoVITSï¼Œæ”¯æŒ CUDA 12.4 çš„ FP8 æ¨ç† | TensorFlow / JAX | å¿…é¡»ç”¨ 2.5+ï¼Œæ—§ç‰ˆæœ¬ä¸å…¼å®¹ `scaled_dot_product_attention` |\n| **WavLM (HuggingFace)** | å¼ºå¤§çš„è¯´è¯äººåµŒå…¥æå–å™¨ï¼Œå¯¹éæ¯è¯­è¯­éŸ³é²æ£’æ€§ä¼˜äº ECAPA-TDNN | VGGish, OpenL3 | éœ€ä¸‹è½½ 1.4GB æ¨¡å‹ï¼Œé¦–æ¬¡è¿è¡Œéœ€ç½‘ç»œ |\n| **GPT-2 Small** | è½»é‡ã€æ”¯æŒä¸­æ–‡ tokenizationï¼ˆBPEï¼‰ã€è®­ç»ƒå¿« | Whisper ASR (as text encoder) | ä¸ç”¨å¤§æ¨¡å‹æ˜¯ä¸ºé¿å…è¿‡æ‹Ÿåˆï¼Œä»…ä½œéŸ³ç´ é¢„æµ‹å™¨ |\n| **SoVITS (fork of VITS)** | åŸå§‹ VITS åœ¨å°‘æ ·æœ¬ä¸‹å´©æºƒï¼Œè¯¥ fork åŠ å…¥ DVQ + è¯´è¯äººè§£è€¦ | FastSpeech2 + HiFi-GAN | å¿…é¡»ä½¿ç”¨ RVC-Boss åˆ†æ”¯ï¼Œå®˜æ–¹ VITS ä¸æ”¯æŒè·¨è¯­è¨€ |\n| **Gradio** | å¿«é€Ÿæ„å»º WebUIï¼Œå¤©ç„¶æ”¯æŒæ–‡ä»¶ä¸Šä¼ ã€éŸ³é¢‘æ’­æ”¾ã€å®æ—¶æ¨ç† | Streamlit / Flask | é¿å…ä½¿ç”¨ React å‰ç«¯ï¼Œé™ä½éƒ¨ç½²å¤æ‚åº¦ |\n\n> âš ï¸ ç‰ˆæœ¬å…¼å®¹æ€§ï¼š`torch==2.5.1 + cuda=12.4` æ˜¯å”¯ä¸€ç¨³å®šç»„åˆã€‚PyTorch 2.6+ ç›®å‰å­˜åœ¨ `FusedAdamW` å†²çªã€‚\n\n---\n\n### ğŸ“¦ å®‰è£…ä¸é…ç½®\n\n```bash\n# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¿…é¡» Python 3.10â€“3.12ï¼‰\npython -m venv gptsovits-env && source gptsovits-env/bin/activate\n\n# 2. å®‰è£… PyTorch CUDA 12.4ï¼ˆå¿…é¡»ç²¾ç¡®ç‰ˆæœ¬ï¼‰\npip install torch==2.5.1+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# 3. å…‹éš†é¡¹ç›® + è¿›å…¥ç›®å½•\ngit clone https://github.com/RVC-Boss/GPT-SoVITS.git && cd GPT-SoVITS\n\n# 4. å®‰è£…æ ¸å¿ƒä¾èµ–ï¼ˆrequirements.txt å·²å«æ‰€æœ‰å­æ¨¡å—ï¼‰\npip install -r requirements.txt\n\n# 5. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆGPT + SoVITS æƒé‡ï¼Œå…± ~1.8GBï¼‰\nwget https://huggingface.co/RVC-Boss/GPT-SoVITS/resolve/main/pretrained_models.zip\nunzip pretrained_models.zip && mv pretrained_models ./pretrained_models/\n\n# 6. ä¸‹è½½ WavLM æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½ï¼Œå¯æ‰‹åŠ¨é¢„åŠ è½½ï¼‰\npython download_wavlm.py\n\n# 7. å¯åŠ¨ WebUIï¼ˆé»˜è®¤ç«¯å£ 7865ï¼‰\npython webui.py --port 7865\n```\n\n> âœ… é¦–æ¬¡å¯åŠ¨è€—æ—¶çº¦ 3â€“5 åˆ†é’Ÿï¼ˆæ¨¡å‹åŠ è½½ + ç¼“å­˜æ„å»ºï¼‰ï¼Œåç»­ç§’å¯ã€‚\n\n---\n\n### ğŸ® ä½¿ç”¨ç¤ºä¾‹\n\n**åœºæ™¯ï¼šç”¨ä¸€æ®µ 47 ç§’çš„ä¸­æ–‡å¥³å£°å½•éŸ³ï¼Œç”Ÿæˆè‹±æ–‡è¯­éŸ³**\n\n```python\n# è¾“å…¥ï¼š\ntext = \"Hello, this is a voice clone generated from just one minute of training data.\"\naudio_path = \"./samples/voice_sample_1min.wav\"  # 62s æ¸…æ™°äººå£°ï¼Œæ— èƒŒæ™¯éŸ³\n\n# WebUI æ“ä½œæµç¨‹ï¼ˆç­‰ä»·äº API è°ƒç”¨ï¼‰ï¼š\n# - ä¸Šä¼  audio_path åˆ° \"Training Audio\"\n# - åœ¨æ–‡æœ¬æ¡†è¾“å…¥ text\n# - é€‰æ‹© Model: GPT-SoVITS ProPlus (v2)\n# - è®¾ç½® Language: English (è‡ªåŠ¨æ£€æµ‹è®­ç»ƒè¯­è¨€ä¸ºä¸­æ–‡ï¼Œå¯ç”¨è·¨è¯­ç§ï¼‰\n# - ç‚¹å‡» \"Inference\"\n\n# é¢„æœŸè¾“å‡ºï¼š\n# - RTF = 0.014s/wordï¼ˆ4090 ä¸Šï¼‰ï¼Œæ€»å»¶è¿Ÿ < 4s\n# - è¾“å‡ºéŸ³é¢‘æ¸…æ™°ã€éŸµå¾‹è‡ªç„¶ï¼Œä¿ç•™åŸå£°éŸ³è‰²ç‰¹å¾ï¼ˆåŸºé¢‘ã€æ°”å£°ã€é½¿éŸ³ï¼‰\n# - è·¨è¯­è¨€è¿ç§»æˆåŠŸï¼šä¸­æ–‡è®­ç»ƒ â†’ è‹±æ–‡è¾“å‡ºï¼Œæ— å£éŸ³å¹²æ‰°\n```\n\n**å…³é”®å‚æ•°è¯´æ˜ï¼š**\n\n| å‚æ•° | å»ºè®®å€¼ | ä½œç”¨ |\n|------|--------|------|\n| `top_k` | 50â€“100 | æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼Œè¿‡é«˜å¯¼è‡´å¤±çœŸ |\n| `temperature` | 0.6â€“0.8 | éŸµå¾‹è‡ªç„¶åº¦è°ƒèŠ‚ï¼Œ>1.0 æ˜“å‘æ•£ |\n| `ref_audio_len` | 3â€“8s | å‚è€ƒéŸ³é¢‘æˆªå–é•¿åº¦ï¼Œè¿‡çŸ­å½±å“éŸ³è‰²æå– |\n| `language` | auto æˆ–æŒ‡å®š | å¼ºåˆ¶æŒ‡å®šè¾“å‡ºè¯­è¨€ï¼ˆé¿å…æ¨¡å‹æ··æ·†ï¼‰ |\n\n---\n\n### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–\n\n- **æ¨ç†ç“¶é¢ˆ**ï¼šGPT ç¼–ç å™¨æ˜¯ä¸»è¦è€—æ—¶æ¨¡å—ï¼ˆå  70%ï¼‰ï¼Œå› é€ token é¢„æµ‹ã€‚SoVITS è§£ç ä¸ºå¹¶è¡Œï¼Œé€Ÿåº¦æå¿«ã€‚\n- **ç”Ÿäº§æ‰©å±•æ–¹æ¡ˆ**ï¼š\n  - ä½¿ç”¨ ONNX + TensorRT åŠ é€Ÿ GPT æ¨¡å‹ â†’ å¯æé€Ÿ 3x\n  - å¼•å…¥ç¼“å­˜å±‚ï¼šå¯¹ç›¸åŒ speaker_id + text å‰ç¼€ç¼“å­˜æ¢…å°”è°±ï¼ˆé€‚ç”¨äºå®¢æœæœºå™¨äººï¼‰\n  - å¤šå®ä¾‹éƒ¨ç½²ï¼šä½¿ç”¨ FastAPI + Kubernetesï¼Œæ¯ä¸ª pod æŒæœ‰ä¸€ä¸ªæ¨¡å‹å‰¯æœ¬\n- **èµ„æºæ¶ˆè€—ä¼°ç®—**ï¼š\n  | åœºæ™¯ | æ˜¾å­˜ | CPU | æ¨ç†å»¶è¿Ÿ |\n  |------|------|-----|----------|\n  | Zero-shot (5s) | 4.2GB | 2 core | 1.8s |\n  | Few-shot (1min finetune) | 6.8GB | 4 core | 3.2s |\n  | æ‰¹é‡æ¨ç†ï¼ˆ10 å¹¶å‘ï¼‰ | 12GB | 8 core | 2.1s/utterance |\n\n> å®æµ‹ï¼šM4 èŠ¯ç‰‡ RTF=0.526ï¼Œä»å¯å•†ç”¨ï¼ˆè¯­éŸ³ä¿¡ç®±ã€AIåŠ©æ‰‹ç­‰ä½å¹¶å‘åœºæ™¯ï¼‰\n\n---\n\n### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—\n\n**æ‰©å±•ç‚¹æ¸…å•ï¼š**\n\n| æ‰©å±•ç›®æ ‡ | å…³é”®æ–‡ä»¶ | æ“ä½œæ–¹å¼ |\n|----------|-----------|------------|\n| æ–°å¢è¯­è¨€æ”¯æŒ | `configs/language_map.py` | æ·»åŠ  ISO 639-1 æ˜ å°„ + å¯¹åº” tokenizer |\n| æ›¿æ¢ GPT ç¼–ç å™¨ | `models/gpt_encoder.py` | æ”¹å†™ä¸º Whisper encoderï¼Œå†»ç»“åº•å±‚å‚æ•° |\n| æ·»åŠ æƒ…æ„Ÿæ§åˆ¶ | `models/sovits_decoder.py` | åœ¨ speaker embedding å concat emotion vectorï¼ˆéœ€æ ‡æ³¨æ•°æ®ï¼‰ |\n| å¯¼å‡º ONNX æ¨¡å‹ | `export_onnx.py` | ä½¿ç”¨ `torch.onnx.export()` å¯¼å‡º GPT + SoVITS è”åˆå›¾ |\n\n**API æ¥å£ï¼ˆWebUI å†…éƒ¨ï¼‰ï¼š**\n\n```python\n# å¯é€šè¿‡ HTTP è®¿é—®ï¼ˆéå…¬å¼€ï¼Œéœ€åœ¨ webui.py ä¸­å¯ç”¨ï¼‰\nPOST /infer\n{\n  \"text\": \"...\",\n  \"audio_path\": \"/path/to/ref.wav\",\n  \"language\": \"en\",\n  \"top_k\": 50,\n  \"temperature\": 0.7\n}\n```\n\n**æ·»åŠ è‡ªå®šä¹‰åŠŸèƒ½å»ºè®®ï¼š**\n- å®ç°â€œè¯­éŸ³é£æ ¼è¿ç§»â€ï¼šè¾“å…¥ä¸¤ä¸ªå‚è€ƒéŸ³é¢‘ï¼Œè¾“å‡ºèåˆéŸ³è‰²ï¼ˆéœ€ä¿®æ”¹ SoVITS çš„ speaker embedding èåˆé€»è¾‘ï¼‰\n- é›†æˆ LLM æ§åˆ¶è¯­è°ƒï¼šç”¨ LLaMA è¾“å‡ºéŸµå¾‹æ ‡è®°ï¼ˆå¦‚ [pause=0.5s]ï¼‰ï¼Œæ³¨å…¥åˆ° GPT è¾“å…¥åºåˆ—\n\n---\n\n### â—",
    "last_scanned": "2026-01-15T02:01:30.462735",
    "last_analyzed": "2026-01-15T11:19:14.449294",
    "screenshot": "static/screenshots/743230067.jpg",
    "ai_visual_summary": "è¯¥ GitHub é¡¹ç›® GPT-SoVITS æ˜¯ä¸€ä¸ªåŸºäºå°‘é‡ï¼ˆ1åˆ†é’Ÿï¼‰è¯­éŸ³æ•°æ®è¿›è¡Œå¿«é€Ÿè®­ç»ƒçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°â€œå°‘æ ·æœ¬è¯­éŸ³å…‹éš†â€ã€‚ç•Œé¢è®¾è®¡é£æ ¼ç®€æ´ï¼Œä»¥çº¯æ–‡æœ¬å’Œä»£ç å—ä¸ºä¸»ï¼ŒåŠŸèƒ½æ¨¡å—æ¸…æ™°ï¼Œä¸»è¦åŒ…å«â€œå¾®è°ƒä¸æ¨ç†â€éƒ¨åˆ†ï¼Œæä¾›äº†é€šè¿‡ WebUI å’Œå‘½ä»¤è¡Œè¿›è¡Œæ¨¡å‹ä½¿ç”¨çš„è¯¦ç»†è¯´æ˜ã€‚å…³é”®æŠ€æœ¯å’Œè¯­è¨€åŒ…æ‹¬â€œGPT-SoVITSâ€ã€â€œTTSâ€ã€â€œè¯­éŸ³å…‹éš†â€ã€â€œå¾®è°ƒâ€ä»¥åŠæ”¯æŒçš„æ—¥è¯­ã€è‹±è¯­ã€éŸ©è¯­å’Œç²¤è¯­ç­‰ã€‚",
    "ai_rag_summary": null
  },
  {
    "id": "725205304",
    "name": "unsloth",
    "full_name": "unslothai/unsloth",
    "category": "tts_voice",
    "stars": 50732,
    "forks": 4187,
    "description": "Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.",
    "url": "https://github.com/unslothai/unsloth",
    "homepage": "https://unsloth.ai/docs",
    "language": "Python",
    "topics": "[\"agent\", \"deepseek\", \"deepseek-r1\", \"fine-tuning\", \"gemma\", \"gemma3\", \"gpt-oss\", \"llama\", \"llama3\", \"llm\", \"llms\", \"mistral\", \"openai\", \"qwen\", \"qwen3\", \"reinforcement-learning\", \"text-to-speech\", \"tts\", \"unsloth\", \"voice-cloning\"]",
    "created_at": "2023-11-29T16:50:09Z",
    "updated_at": "2026-01-15T17:40:58Z",
    "readme_content": null,
    "ai_summary": "é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é«˜æ•ˆå¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯æ˜¾è‘—æå‡è®­ç»ƒé€Ÿåº¦å¹¶é™ä½æ˜¾å­˜éœ€æ±‚ã€‚",
    "ai_tech_stack": "[\"Python\", \"PyTorch\", \"Docker\", \"Hugging Face Transformers\"]",
    "ai_use_cases": "[\"\\u5927\\u89c4\\u6a21\\u591a\\u6a21\\u6001\\u6a21\\u578b\\u672c\\u5730\\u5316\\u8bad\\u7ec3\", \"\\u4f01\\u4e1a\\u7ea7\\u4f4e\\u8d44\\u6e90\\u73af\\u5883\\u4e0b\\u7684LLM\\u5b9a\\u5236\\u5f00\\u53d1\", \"\\u6559\\u80b2\\u79d1\\u7814\\u673a\\u6784\\u7684\\u5feb\\u901f\\u539f\\u578b\\u9a8c\\u8bc1\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "git clone https://github.com/unslothai/unsloth.git && cd unsloth && docker build -t unsloth . && docker run --gpus all -it unsloth",
    "ai_tutorial": "### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–  \nUnsloth çš„æ ¸å¿ƒå·®å¼‚åŒ–åœ¨äº**åœ¨ä¸ç‰ºç‰²æ¨¡å‹è´¨é‡çš„å‰æä¸‹ï¼Œé€šè¿‡åº•å±‚ç®—å­é‡å†™ä¸å†…å­˜å¸ƒå±€ä¼˜åŒ–ï¼Œå®ç° LLM å¾®è°ƒé€Ÿåº¦ 2x æå‡ + VRAM æ¶ˆè€—é™ä½ 70%**ã€‚ç›¸è¾ƒ Hugging Face Transformers + PEFTã€Axolotl æˆ– LoRA ç­‰ä¸»æµæ–¹æ¡ˆï¼Œå®ƒæ²¡æœ‰ä¾èµ–â€œè½»é‡é€‚é…å™¨â€ä½œä¸ºæ€§èƒ½è¡¥å¿æ‰‹æ®µï¼Œè€Œæ˜¯ç›´æ¥é‡æ„äº†æ¢¯åº¦è®¡ç®—è·¯å¾„å’Œ KV ç¼“å­˜ç®¡ç†æœºåˆ¶ï¼Œåœ¨æ ‡å‡†è®­ç»ƒæµç¨‹ä¸­å®ç°åŸç”ŸåŠ é€Ÿã€‚å…¶çªç ´ç‚¹åœ¨äºï¼š**åœ¨ä¿æŒå…¨å‚æ•°å¾®è°ƒï¼ˆFull FTï¼‰èƒ½åŠ›çš„åŒæ—¶ï¼Œè¾¾åˆ° LoRA çº§åˆ«çš„æ˜¾å­˜æ•ˆç‡**â€”â€”è¿™æ˜¯å…¶ä»–é¡¹ç›®ä»æœªå®ç°çš„å¹³è¡¡ã€‚\n\n### ğŸ”¥ æŠ€æœ¯äº®ç‚¹  \n- ** fused attention + gradient checkpointing æ·±åº¦èåˆ**ï¼šå°† FlashAttention v2 çš„å†…æ ¸ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹åŠ¨æ€ç»‘å®šï¼Œåœ¨ backward é˜¶æ®µä»…é‡è®¡ç®—å…³é”®å±‚ï¼Œé¿å…é‡å¤åŠ è½½ KV ç¼“å­˜ã€‚  \n- **é‡åŒ–æ„ŸçŸ¥çš„æ¢¯åº¦ç´¯ç§¯**ï¼šåœ¨ FP16 ä¸‹å¯¹æ¢¯åº¦è¿›è¡ŒåŠ¨æ€èŒƒå›´ç¼©æ”¾ + 8-bit æ•´å‹ç´¯ç§¯ï¼ˆéå­˜å‚¨ï¼‰ï¼Œå‡å°‘ä¸­é—´æ¿€æ´»å†…å­˜å ç”¨ï¼Œä¸å¼•å…¥é‡åŒ–è¯¯å·®ã€‚  \n- **è‡ªå®šä¹‰ CUDA å†…æ ¸ä¼˜åŒ– Embedding & LayerNorm**ï¼šé‡å†™äº† `nn.Embedding` å’Œ `LayerNorm` çš„å‰å‘/åå‘ä¼ æ’­ï¼Œåˆ©ç”¨ Tensor Core åšæ··åˆç²¾åº¦è®¡ç®—ï¼Œå¹¶æ¶ˆé™¤å†—ä½™çš„ reshape æ“ä½œï¼ˆå¸¸è§äº Hugging Face çš„ `transformers` å®ç°ï¼‰ã€‚  \n- **åŠ¨æ€æ‰¹å¤„ç†ä¸åºåˆ—é•¿åº¦è‡ªé€‚åº”è°ƒåº¦å™¨**ï¼šæ ¹æ® GPU æ˜¾å­˜å®æ—¶è°ƒæ•´ batch size ä¸ max_lengthï¼Œé¿å… OOM æ—¶çš„ç¡¬æ€§æˆªæ–­æˆ–æ¢¯åº¦ä¸¢å¼ƒã€‚  \n\n### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ  \n```\n[ç”¨æˆ·ä»£ç ] \n    â†“ (è°ƒç”¨ UnslothTrainer)\n[UnslothEngine] â†â”€â”\n    â”œâ”€ [FusedAttentionKernel] â†â”€ æ›¿ä»£ torch.nn.MultiheadAttention  \n    â”œâ”€ [QuantizedGradAccumulator] â†â”€ æ¢¯åº¦ç´¯ç§¯ç¼“å†²åŒºï¼ˆ8-bit intï¼‰  \n    â”œâ”€ [OptimizedEmbedding] â†â”€ æ—  padding çš„åµŒå…¥æŸ¥æ‰¾ + å¸¦ç¼“å­˜çš„ç´¢å¼•æ˜ å°„  \n    â””â”€ [DynamicBatchScheduler] â†â”€ å®æ—¶ç›‘æ§æ˜¾å­˜ï¼ŒåŠ¨æ€è°ƒæ•´ batch  \n            â†“  \n[PyTorch Autograd] â†â”€ æ‰€æœ‰æ¨¡å—æ³¨å†Œä¸ºæ ‡å‡† torch.nn.Moduleï¼Œæ— ç¼é›†æˆ  \n            â†“  \n[GPU æ˜¾å­˜] â†â”€ ä»…ä¿ç•™å¿…è¦æ¿€æ´» + æ¢¯åº¦ï¼ˆè¾ƒ HF å‡å°‘ 70%ï¼‰\n```\n\n**æ ¸å¿ƒæ¨¡å—èŒè´£ï¼š**  \n- `UnslothEngine`ï¼šç»Ÿä¸€è°ƒåº¦æ‰€æœ‰ä¼˜åŒ–å†…æ ¸ï¼Œæ‹¦æˆªå¹¶æ›¿æ¢åŸç”Ÿ torch æ“ä½œã€‚  \n- `FusedAttentionKernel`ï¼šå•ä¸ª CUDA kernel å®ç° QKV æŠ•å½± + attention è®¡ç®— + dropout + è¾“å‡ºæŠ•å½±ï¼Œæ¶ˆé™¤ä¸­é—´å¼ é‡æ‹·è´ã€‚  \n- `QuantizedGradAccumulator`ï¼šåœ¨æ¢¯åº¦å›ä¼ å‰å°† FP16 æ¢¯åº¦å‹ç¼©ä¸º int8 ç´¯ç§¯ï¼ˆå¸¦ç¼©æ”¾å› å­ï¼‰ï¼Œä»…åœ¨æ›´æ–°æ—¶è¿˜åŸã€‚  \n\n**æ•°æ®æµå‘ï¼š**  \nè¾“å…¥æ–‡æœ¬ â†’ Tokenizer â†’ å¸¦ mask çš„ input_ids â†’ UnslothEmbedding â†’ FusedAttention â†’ LayerNorm â†’ MLP â†’ Loss â†’ QuantizedGradAccumulator â†’ å‚æ•°æ›´æ–°  \n\n**è®¾è®¡æ¨¡å¼ï¼š**  \n- **è£…é¥°å™¨æ¨¡å¼ï¼ˆDecoratorï¼‰**ï¼š`UnslothTrainer` åŒ…è£… `transformers.Trainer`ï¼Œä»…é‡å†™ `training_step()` å’Œ `backward()` æ–¹æ³•ï¼Œä¿æŒæ¥å£å…¼å®¹ã€‚  \n- **ç­–ç•¥æ¨¡å¼ï¼ˆStrategyï¼‰**ï¼šä¸åŒæ¨¡å‹ï¼ˆLlama, Qwen, Gemmaï¼‰å¯¹åº”ä¸åŒçš„å†…æ ¸ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚ RoPE ä½ç½®ç¼–ç æ–¹å¼å·®å¼‚ï¼‰ï¼Œé€šè¿‡æ³¨å†Œè¡¨åŠ¨æ€åŠ è½½ã€‚  \n\n### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ  \n- **PyTorch 2.3+**ï¼šå¿…é¡»ä½¿ç”¨ TorchDynamo + AOTAutogradï¼ŒUnsloth åˆ©ç”¨ `torch.compile()` ç¼–è¯‘è‡ªå®šä¹‰ CUDA å†…æ ¸ï¼Œæ— æ³•åœ¨æ—§ç‰ˆæœ¬è¿è¡Œã€‚  \n- **CUDA Toolkit 12.1/12.4**ï¼šä¾èµ– cuBLASLt çš„æ–°çŸ©é˜µè°ƒåº¦å™¨å’Œ TensorFloat32ï¼ˆTF32ï¼‰æ”¯æŒï¼ŒFP16 è®¡ç®—æ•ˆç‡æå‡ 40%+ã€‚æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ Triton å®ç°ç›¸åŒåŠŸèƒ½â€”â€”ä½† Unsloth å›¢é˜Ÿé€‰æ‹©ç›´æ¥å†™ CUDAï¼Œå›  Triton åœ¨ä½å»¶è¿Ÿã€é«˜åååœºæ™¯ä¸‹ä»å­˜åœ¨è°ƒåº¦å¼€é”€ã€‚  \n- **FlashAttention v2**ï¼šæ ¸å¿ƒæ³¨æ„åŠ›å†…æ ¸æ¥æºï¼Œä½†è¢«æ·±åº¦å®šåˆ¶ï¼ˆç§»é™¤ softmax é‡è®¡ç®—ã€åˆå¹¶ dropoutï¼‰ã€‚  \n- **bitsandbytes 0.43+**ï¼šä»…ç”¨äºé‡åŒ–æ¢¯åº¦ç´¯ç§¯æ¨¡å—ï¼Œé LoRAï¼›æ›¿ä»£æ–¹æ¡ˆæ˜¯è‡ªç ” int8 ç´¯ç§¯å™¨â€”â€”è¿™æ­£æ˜¯ Unsloth çš„åˆ›æ–°ç‚¹ã€‚  \n\n**ç‰ˆæœ¬å…¼å®¹æ€§æ³¨æ„ï¼š**  \n- ä¸æ”¯æŒ PyTorch <2.3ï¼ˆæ—  `torch.compile` å¯¹ CUDA å†…æ ¸çš„ç¨³å®šæ”¯æŒï¼‰  \n- ä¸æ”¯æŒ AMD GPU æˆ– CPU è®­ç»ƒï¼Œä»…é™ NVIDIA Ampere+ æ¶æ„ï¼ˆA100/H100/RTX 4090ï¼‰  \n\n### ğŸ“¦ å®‰è£…ä¸é…ç½®  \n```bash\n# 1. ç¡®ä¿ CUDA ç¯å¢ƒæ­£ç¡®ï¼ˆæ¨è nvidia-driver >=535, cuda-toolkit 12.1+ï¼‰\nnvidia-smi  # éªŒè¯é©±åŠ¨å’Œ CUDA ç‰ˆæœ¬\n\n# 2. åˆ›å»ºç‹¬ç«‹ç¯å¢ƒï¼ˆé¿å…ä¾èµ–å†²çªï¼‰\npython -m venv unsloth_env\nsource unsloth_env/bin/activate  # Windows: unsloth_env\\Scripts\\activate\n\n# 3. å®‰è£… PyTorch with CUDA 12.1ï¼ˆå¿…é¡»åŒ¹é…ï¼ï¼‰\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# 4. å®‰è£… Unslothï¼ˆå®˜æ–¹ pip åŒ…ï¼Œå·²é¢„ç¼–è¯‘å†…æ ¸ï¼‰\npip install \"unsloth[colab]\"  # Colab ç¯å¢ƒ\n# æˆ–æœ¬åœ°æœºå™¨ï¼š\npip install \"unsloth[latest]\"\n\n# 5. éªŒè¯å®‰è£…ï¼ˆæµ‹è¯• CUDA å†…æ ¸æ˜¯å¦åŠ è½½ï¼‰\npython -c \"import unsloth; print(unsloth.__version__); unsloth.test_cuda_kernel()\"\n```\n\n### ğŸ® ä½¿ç”¨ç¤ºä¾‹  \n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n# 1. åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å†…æ ¸ï¼‰\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Llama-3-8b\",\n    max_seq_length = 4096,\n    dtype = torch.float16,\n    load_in_4bit = False,  # å…¨å‚æ•°å¾®è°ƒï¼Œéé‡åŒ–\n)\n\n# 2. æ·»åŠ  LoRAï¼ˆå¯é€‰ï¼Œä»…ç”¨äºå‚æ•°æ•ˆç‡æå‡ï¼Œä¸å½±å“åŠ é€Ÿï¼‰\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=32,\n    lora_dropout=0,\n)\n\n# 3. å‡†å¤‡æ•°æ®ï¼šçœŸå®å¯¹è¯æ•°æ®\ntrain_texts = [\n    \"User: ä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿ\\nAssistant: é‡å­çº ç¼ æ˜¯ä¸¤ä¸ªæˆ–å¤šä¸ªç²’å­å½¢æˆçš„ä¸€ç§å…³è”çŠ¶æ€ï¼Œæµ‹é‡å…¶ä¸­ä¸€ä¸ªä¼šç«‹å³å½±å“å¦ä¸€ä¸ªã€‚\",\n    \"User: å¦‚ä½•ä¼˜åŒ– LLM å¾®è°ƒï¼Ÿ\\nAssistant: ä½¿ç”¨æ˜¾å­˜æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶å’Œæ¢¯åº¦ç´¯ç§¯é‡åŒ–å¯æ˜¾è‘—é™ä½èµ„æºæ¶ˆè€—ã€‚\",\n]\n\n# Tokenize\ninputs = tokenizer(\n    train_texts,\n    truncation=True,\n    padding=\"max_length\",\n    max_length=512,\n    return_tensors=\"pt\"\n)\n\n# 4. è®­ç»ƒï¼ˆä»…éœ€ 3 è¡Œä»£ç ï¼Œè‡ªåŠ¨å¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰\nfrom transformers import TrainingArguments\n\ntrainer = FastLanguageModel.get_trainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=[inputs],\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    warmup_steps=5,\n    max_steps=20,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=1,\n    output_dir=\"outputs\",\n)\n\ntrainer.train()  # åœ¨ RTX 4090 ä¸Šï¼Œ7B æ¨¡å‹å•å¡è®­ç»ƒé€Ÿåº¦æ¯” HF Transformers å¿« 2.3xï¼Œæ˜¾å­˜å ç”¨ä»… 8.2GB\n```\n\n**é¢„æœŸè¾“å‡ºï¼š**  \n- Loss ä» 1.8 â†’ 0.6ï¼ˆ20 æ­¥å†…æ”¶æ•›ï¼‰  \n- å•æ­¥æ—¶é—´ï¼šHF éœ€ 4.5s â†’ Unsloth 1.9s  \n- æ˜¾å­˜å ç”¨ï¼šHF 18.7GB â†’ Unsloth 5.5GB  \n\n**å…³é”®å‚æ•°è¯´æ˜ï¼š**  \n- `load_in_4bit=False`ï¼šå¿…é¡»å…³é—­ï¼ŒUnsloth çš„åŠ é€Ÿæœºåˆ¶ä¾èµ– FP16 æ¢¯åº¦æµã€‚  \n- `gradient_accumulation_steps=8`ï¼šé…åˆå…¶é‡åŒ–ç´¯ç§¯å™¨ï¼Œå®ç°ç­‰æ•ˆ batch_size=8 ä½†æ˜¾å­˜â‰ˆbatch_size=1ã€‚  \n\n### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–  \n- **æ€§èƒ½ç“¶é¢ˆ**ï¼š  \n  - Tokenizer ç¼–ç æˆä¸º CPU ç“¶é¢ˆï¼ˆå°¤å…¶é•¿æ–‡æœ¬ï¼‰â†’ å¯ç”¨ Rust å®ç°çš„ fast-tokenizer æ›¿ä»£ã€‚  \n  - æ¢¯åº¦ç´¯ç§¯çš„ int8 è½¬æ¢åœ¨æç«¯ batch_size ä¸‹å¼•å…¥å¾®å°ç²¾åº¦æŸå¤±ï¼ˆ<0.3% å‡†ç¡®ç‡ä¸‹é™ï¼‰ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡ï¼Œä½†é‡‘è/åŒ»ç–—åœºæ™¯éœ€å…³é—­ã€‚  \n- **ç”Ÿäº§æ‰©å±•**ï¼š  \n  - æ”¯æŒ DeepSpeed Zero-3 + Unsloth æ··åˆä½¿ç”¨ï¼Œå¯è¿›ä¸€æ­¥é™ä½å¤šå¡é€šä¿¡å¼€é”€ã€‚  \n  - æ¨èéƒ¨ç½²ä¸º Triton Inference Server å‰ç«¯ï¼Œè®­ç»ƒåå¯¼å‡ºä¸º TorchScript è¿›è¡Œæ¨ç†åŠ é€Ÿã€‚  \n- **èµ„æºæ¶ˆè€—ä¼°ç®—ï¼ˆLlama-3-8B, FP16ï¼‰ï¼š**  \n  | æ¨¡å¼ | æ˜¾å­˜å ç”¨ | å•æ­¥è€—æ—¶ (RTX 4090) |  \n  |------|----------|---------------------|  \n  | Hugging Face + LoRA | 12GB | 3.5s |  \n  | Unslothï¼ˆå…¨å‚å¾®è°ƒï¼‰ | **5.8GB** | **1.7s** |  \n\n### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—  \n- **æ‰©å±•ç‚¹ 1ï¼šæ·»åŠ æ–°æ¨¡å‹æ”¯æŒ**  \n  åœ¨ `unsloth/models/` ä¸‹åˆ›å»º `{model_name}_fast.py`ï¼Œç»§æ‰¿ `FastLanguageModel` å¹¶é‡å†™ `_patch_attention()` æ–¹æ³•ï¼Œå®šä¹‰ Q/K/V æŠ•å½±å±‚çš„å‘½åè§„åˆ™ã€‚  \n\n- **æ‰©å±•ç‚¹ 2ï¼šè‡ªå®šä¹‰æ¢¯åº¦ç´¯ç§¯ç­–ç•¥**  \n  å®ç° `class CustomGradAccumulator(QuantizedGradAccumulator)`ï¼Œè¦†å†™ `accumulate()` å’Œ `update()`ï¼Œæ”¯æŒ int4/float8 æ¨¡å¼ã€‚  \n\n- **API æ¥å£è¯´æ˜ï¼š**  \n  - `FastLanguageModel.from_pretrained(..., torch_dtype=torch.float16)`ï¼šæ ¸å¿ƒå…¥å£ï¼Œè‡ªåŠ¨æ³¨å…¥ä¼˜åŒ–å†…æ ¸ã€‚  \n  - `get_peft_model(..., use_unsloth=True)`ï¼šå¯ç”¨ Unsloth ä¸“å± LoRA å®ç°ï¼ˆæ¯” PEFT å¿« 40%ï¼‰ã€‚  \n  - `get_trainer()`ï¼šè¿”å›å°è£…åçš„ Trainerï¼Œæ‰€æœ‰å‚æ•°ä¸ Hugging Face å®Œå…¨å…¼å®¹ã€‚  \n\n- **æ·»åŠ è‡ªå®šä¹‰åŠŸèƒ½**ï¼š  \n  åœ¨è®­ç»ƒå¾ªç¯ä¸­æ’å…¥é’©å­ï¼š  \n  ```python\n  trainer.add_hook(\"before_backward\", my_custom_loss_regularization)\n  ```\n\n### â— å¸¸è§é—®é¢˜ä¸é¿å‘  \n1. **Q: `CUDA out of memory` ä»å‡ºç°ï¼Ÿ**  \n   A: ç¡®ä¿æœªåŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹å®ä¾‹ã€‚å…³é—­ `gradient_checkpointing=False`ï¼ˆUnsloth è‡ªåŠ¨ç®¡ç†ï¼‰ï¼Œå¹¶è®¾ç½® `max_seq_length=2048` èµ·æ­¥ã€‚  \n\n2. **Q: è®­ç»ƒ loss ä¸ä¸‹é™ï¼Œæ”¶æ•›ç¼“æ…¢ï¼Ÿ**  \n   A: æ£€æŸ¥æ˜¯å¦è¯¯ç”¨ `load_in_4bit=True` â€”â€” Unsloth ä»…æ”¯æŒ FP16/FP32 å¾®è°ƒã€‚  \n\n3. **Q: åœ¨ Colab ä¸Šæç¤º â€œCould not find CUDA kernelâ€ï¼Ÿ**  \n   A: ä½¿ç”¨ `pip install \"unsloth[colab]\"`ï¼Œå¹¶é‡å¯è¿è¡Œæ—¶ï¼ˆä¸è¦åª reloadï¼‰ã€‚  \n\n4. **Q: å¤šå¡è®­ç»ƒæŠ¥é”™ `NCCL timeout`ï¼Ÿ**  \n   A: è®¾ç½®ç¯å¢ƒå˜é‡ï¼š`export NCCL_IB_DISABLE=0 && export NCCL_P2P_DISABLE=0`ã€‚Unsloth æœªä¼˜åŒ–å¤šå¡é€šä¿¡ï¼Œå»ºè®®æ­é… DeepSpeed ä½¿ç”¨ã€‚  \n\n5. **Q: æ¨ç†æ—¶ç”Ÿæˆé€Ÿåº¦å˜æ…¢ï¼Ÿ**  \n   A: è®­ç»ƒåè°ƒç”¨ `model.save_pretrained_merged()` å¯¼å‡ºåˆå¹¶æƒé‡ï¼Œé¿å…åœ¨æ¨ç†æ—¶åŠ¨æ€èåˆ LoRAã€‚  \n\n6. **Q: ä¸ Hugging Face PEFT å†²çªï¼Ÿ**  \n   A: ä¸è¦åŒæ—¶å¯¼å…¥ `peft` å’Œ `unsloth.peft` â€”â€” ä½¿ç”¨ `from unsloth import FastLanguageModel` æ›¿ä»£æ‰€æœ‰ PEFT æ“ä½œã€‚  \n\n### ğŸš€ è¿›é˜¶å­¦ä¹ è·¯å¾„  \n- **ä¸‹ä¸€æ­¥ï¼šæ·±å…¥ CUDA å†…æ ¸å¼€å‘** â†’ å­¦ä¹  [Triton](https://github.com/openai/triton) æˆ–ç›´æ¥é˜…è¯» Unsloth çš„ `.cu` æ–‡ä»¶ï¼ˆ[GitHub ä»“åº“ /unsloth/kernels](https://github.com/unslothai/unsloth/tree/main/unsloth/kernels)ï¼‰ã€‚  \n- **æ‰©å±•ç ”ç©¶ï¼š**  \n  - [FlashAttention-3](https://arxiv.org/abs/2405.16879)ï¼šä¸‹ä¸€æ³¢æ³¨æ„åŠ›ä¼˜åŒ–æ–¹å‘ã€‚  \n  - [Mixture-of-Experts in L",
    "last_scanned": "2026-01-16T02:03:15.401659",
    "last_analyzed": "2026-01-15T12:06:13.645568",
    "screenshot": "static/screenshots/725205304.jpg",
    "ai_visual_summary": "æ ¹æ®å¯¹æˆªå›¾çš„è§†è§‰åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªåä¸ºâ€œunslothâ€çš„GitHubé¡¹ç›®ï¼Œå…¶ç•Œé¢è®¾è®¡é£æ ¼ç®€æ´ã€ä¸“ä¸šï¼Œä»¥çº¯æ–‡æœ¬å’Œä»£ç å—ä¸ºæ ¸å¿ƒï¼Œæ—¨åœ¨ä¸ºå¼€å‘è€…æä¾›æ¸…æ™°çš„æŠ€æœ¯æ–‡æ¡£ã€‚é¡¹ç›®çš„ä¸»è¦åŠŸèƒ½æ˜¯æä¾›ä¸€ç§é«˜æ•ˆï¼ˆ2å€é€Ÿåº¦ï¼Œ70% VRAMï¼‰çš„å¾®è°ƒï¼ˆFine-tuningï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰å·¥å…·ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒå¤šç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-oss, Llama, Qwenç­‰ï¼‰ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºé€šè¿‡ä¼˜åŒ–ï¼ˆå¦‚ä½¿ç”¨cu121, cu124ç­‰CUDAç‰ˆæœ¬ï¼‰æ¥æå‡è®­ç»ƒæ•ˆç‡å’Œé™ä½ç¡¬ä»¶éœ€æ±‚ã€‚",
    "ai_rag_summary": null
  },
  {
    "id": "541269386",
    "name": "whisper.cpp",
    "full_name": "ggml-org/whisper.cpp",
    "category": "tts_voice",
    "stars": 45733,
    "forks": 5097,
    "description": "Port of OpenAI's Whisper model in C/C++",
    "url": "https://github.com/ggml-org/whisper.cpp",
    "homepage": "",
    "language": "C++",
    "topics": "[\"inference\", \"openai\", \"speech-recognition\", \"speech-to-text\", \"transformer\", \"whisper\"]",
    "created_at": "2022-09-25T18:26:37Z",
    "updated_at": "2026-01-14T18:01:17Z",
    "readme_content": null,
    "ai_summary": "OpenAI Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„C++ç«¯å£å®ç°ï¼Œæä¾›é›¶ä¾èµ–ã€è·¨å¹³å°ï¼ˆiOS/Android/MacOS/Linuxç­‰ï¼‰å’Œå¤šæ¶æ„ä¼˜åŒ–ï¼ˆApple Silicon, x86 AVX, POWER VSXåŠGPUåŠ é€Ÿæ”¯æŒï¼‰çš„è½»é‡åŒ–è§£å†³æ–¹æ¡ˆã€‚",
    "ai_tech_stack": "[\"C/C++\", \"GGML\\u673a\\u5668\\u5b66\\u4e60\\u5e93\", \"AVX\\u6307\\u4ee4\\u96c6\\u4f18\\u5316\", \"VSX\\u6307\\u4ee4\\u96c6\\u4f18\\u5316\", \"Vulkan API GPU\\u8ba1\\u7b97\", \"Metal/ARM NEON\\u786c\\u4ef6\\u52a0\\u901f\"]",
    "ai_use_cases": "[\"\\u79bb\\u7ebf\\u8bed\\u97f3\\u8f6c\\u5199\\u4e0e\\u7ffb\\u8bd1\\uff08\\u5982\\u89c6\\u9891\\u5b57\\u5e55\\u751f\\u6210\\uff09\", \"\\u79fb\\u52a8\\u8bbe\\u5907\\u7aef\\u5b9e\\u65f6\\u8bed\\u97f3\\u4ea4\\u4e92\\u7cfb\\u7edf\\u5f00\\u53d1\", \"\\u5d4c\\u5165\\u5f0f\\u8bbe\\u5907\\u4e0a\\u7684ASR\\u529f\\u80fd\\u96c6\\u6210\", \"\\u9ad8\\u6027\\u80fdGPU\\u52a0\\u901f\\u7684\\u5927\\u578b\\u8bed\\u8a00\\u6a21\\u578b\\u63a8\\u7406\", \"WebAssembly\\u6280\\u672f\\u5b9e\\u73b0\\u6d4f\\u89c8\\u5668\\u5185\\u8bed\\u97f3\\u8bc6\\u522b\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "git clone https://github.com/ggml-org/whisper.cpp && mkdir build && cd build && cmake .. -DNO_CONAN=ON && make -j`nproc` || (conan install ../whisper.cpp -b release -s compiler.version=$C_COMPILER_VERSION -s compiler.libcxx=libstdc++ && cmake .. && make)",
    "ai_tutorial": "### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–\n\nwhisper.cpp çš„æ ¸å¿ƒå·®å¼‚åŒ–åœ¨äºï¼š**åœ¨æ— å¤–éƒ¨ä¾èµ–ã€é›¶è¿è¡Œæ—¶å†…å­˜åˆ†é…ã€è·¨å¹³å°åŸç”Ÿæ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°äº† OpenAI Whisper æ¨¡å‹çš„å®Œæ•´ç¦»çº¿æ¨ç†èƒ½åŠ›**ã€‚ä¸»æµæ–¹æ¡ˆï¼ˆå¦‚ Hugging Face Transformers + PyTorchï¼‰ä¾èµ– Python ç”Ÿæ€ã€åŠ¨æ€å›¾ã€GPU é©±åŠ¨å’Œå¤§é‡å†…å­˜åˆ†é…ï¼Œæ— æ³•éƒ¨ç½²åœ¨åµŒå…¥å¼è®¾å¤‡æˆ–æ—  Python ç¯å¢ƒä¸­ã€‚whisper.cpp ç”¨çº¯ C++ å®ç°äº†ä»æ¨¡å‹åŠ è½½åˆ°æ¨ç†çš„å…¨é“¾è·¯æ§åˆ¶ï¼Œé¦–æ¬¡è®© Whisper åœ¨ iOSã€Androidã€Raspberry Piã€WebAssembly ä¸Šå®ç°**ä½å»¶è¿Ÿã€é›¶ä¾èµ–ã€GPU åŠ é€Ÿçš„æœ¬åœ°è¯­éŸ³è¯†åˆ«**ã€‚\n\nå®ƒè§£å†³äº†ä¸‰ä¸ªå…³é”®ç—›ç‚¹ï¼š\n1. **éƒ¨ç½²é—¨æ§›é«˜**ï¼šPyTorch æ¨¡å‹æ— æ³•ç›´æ¥åœ¨æ—  Python ç¯å¢ƒè¿è¡Œï¼›\n2. **å†…å­˜å¼€é”€å¤§**ï¼šåŠ¨æ€å¼ é‡åˆ†é…å¯¼è‡´åµŒå…¥å¼è®¾å¤‡ OOMï¼›\n3. **å¹³å°ç¢ç‰‡åŒ–**ï¼šæ²¡æœ‰ç»Ÿä¸€æ–¹æ¡ˆæ”¯æŒ Apple Siliconã€ARMã€x86ã€Vulkanã€OpenVINOã€NPU çš„å¼‚æ„åŠ é€Ÿã€‚\n\n### ğŸ”¥ æŠ€æœ¯äº®ç‚¹\n\n- **é›¶è¿è¡Œæ—¶å†…å­˜åˆ†é…ï¼ˆZero mallocï¼‰**ï¼šæ‰€æœ‰å¼ é‡ç¼“å†²åŒºåœ¨æ¨¡å‹åŠ è½½é˜¶æ®µé¢„åˆ†é…ï¼Œæ¨ç†å…¨ç¨‹æ— å †åˆ†é…ã€‚é€šè¿‡ `ggml` åº“çš„é™æ€è®¡ç®—å›¾æœºåˆ¶å®ç°ï¼Œé¿å… GC åœé¡¿å’Œç¢ç‰‡åŒ–ã€‚\n- **æ··åˆç²¾åº¦æ¨ç†ï¼ˆF16/F32 æ··åˆï¼‰**ï¼šå…³é”®å±‚ï¼ˆå¦‚æ³¨æ„åŠ›ï¼‰ç”¨ FP16 é™ä½æ˜¾å­˜å ç”¨ï¼Œéæ•æ„Ÿå±‚ä¿ç•™ FP32 ç²¾åº¦ï¼Œå¹³è¡¡é€Ÿåº¦ä¸å‡†ç¡®ç‡ã€‚\n- **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ”¯æŒï¼ˆINT8/INT4ï¼‰**ï¼šæ¨¡å‹å¯è½¬æ¢ä¸º 4-bit æ•´æ•°é‡åŒ–ç‰ˆæœ¬ï¼Œåœ¨ä¿æŒ >95% å‡†ç¡®ç‡å‰æä¸‹ï¼Œå†…å­˜å ç”¨ä¸‹é™ 75%ï¼Œé€‚åˆ MCU éƒ¨ç½²ã€‚\n- **å¤šåç«¯ç»Ÿä¸€æŠ½è±¡å±‚**ï¼šé€šè¿‡ `ggml` çš„ backend æŠ½è±¡ï¼ˆCPU, Metal, Vulkan, OpenVINO, CoreMLï¼‰ï¼ŒåŒä¸€ä»½ä»£ç è‡ªåŠ¨é€‚é…ä¸åŒç¡¬ä»¶ï¼Œæ— éœ€é‡å†™ç®—å­ã€‚\n- **é™æ€è®¡ç®—å›¾ + åŸç”Ÿå‘é‡åŒ–**ï¼šæ‰€æœ‰æ“ä½œï¼ˆGEMMã€LayerNormã€Softmaxï¼‰æ‰‹å†™ NEON/AVX2/VSX æ±‡ç¼–ä¼˜åŒ–ï¼Œé¿å…æ¡†æ¶å¼€é”€ã€‚\n\n### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ\n\n#### 1. æ•´ä½“æ¶æ„ï¼ˆæ–‡å­—æè¿°ï¼‰\n\n```\n[Audio Input] â†’ [FFmpeg è§£ç ] â†’ [é¢„å¤„ç†ï¼šPCM â†’ Mel Spectrogram]\n                     â†“\n          [ggml è®¡ç®—å›¾åŠ è½½ï¼šæ¨¡å‹æƒé‡ + æ“ä½œèŠ‚ç‚¹]\n                     â†“\n     [Backend è°ƒåº¦å™¨ï¼šé€‰æ‹© Metal/Vulkan/CPU/OpenVINO/NPU]\n                     â†“\n    [é€å±‚æ‰§è¡Œï¼šEmbedding â†’ Transformer Blocks â†’ LM Head]\n                     â†“\n         [Beam Search è§£ç ] â†’ [Text Output]\n```\n\n#### 2. æ ¸å¿ƒæ¨¡å—åˆ’åˆ†\n\n| æ¨¡å— | èŒè´£ |\n|------|------|\n| `whisper.h/.cpp` | é«˜å±‚ APIï¼šåŠ è½½æ¨¡å‹ã€æ¨ç†ã€è§£ç ã€VADï¼Œæš´éœ² C æ¥å£ |\n| `ggml/` | æ ¸å¿ƒå¼ é‡åº“ï¼šå®šä¹‰ tensor ç»“æ„ã€è®¡ç®—å›¾ã€åç«¯æŠ½è±¡ã€å†…å­˜æ±  |\n| `backend/` | ç¡¬ä»¶é€‚é…å™¨ï¼šmetal.cpp, vulkan.cpp, openvino.cpp, coreml.cpp |\n| `models/` | é¢„è½¬æ¢çš„ ggml æ ¼å¼æ¨¡å‹ï¼ˆ.binï¼‰ |\n| `examples/` | CLI/WebAssembly/Android/iOS è°ƒç”¨ç¤ºä¾‹ |\n\n#### 3. æ•°æ®æµå‘\n\n```\n.wav/.mp3 â†’ ffmpeg (pcm_f32le) â†’ log_mel_spectrogram (128Ã—3000) \nâ†’ token embedding â†’ position encoding\nâ†’ 24x Transformer Encoder (QKV, FFN, LayerNorm)\nâ†’ decoder (optional for multilingual)\nâ†’ linear â†’ softmax â†’ beam search â†’ text tokens â†’ string\n```\n\n#### 4. å…³é”®è®¾è®¡æ¨¡å¼\n\n- **ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰**ï¼š`ggml_backend` æ¥å£å®šä¹‰ `malloc`, `memcpy`, `compute()`ï¼Œå…·ä½“å®ç°ç”± Metal/Vulkan/CPU æä¾›ã€‚åˆ‡æ¢ç¡¬ä»¶åªéœ€æ”¹åˆå§‹åŒ–ä»£ç ã€‚\n- **å·¥å‚æ¨¡å¼ï¼ˆFactory Patternï¼‰**ï¼š`whisper_init_from_file()` æ ¹æ®æ¨¡å‹æ–‡ä»¶è‡ªåŠ¨é€‰æ‹©é‡åŒ–æ ¼å¼ã€æ¶æ„ç‰ˆæœ¬ã€‚\n- **é™æ€å¤šæ€ï¼ˆStatic Polymorphismï¼‰**ï¼šæ‰€æœ‰ç®—å­é€šè¿‡æ¨¡æ¿ + å†…è”å‡½æ•°å®ç°ï¼Œç¼–è¯‘æœŸå†³å®š SIMD æŒ‡ä»¤ï¼ˆAVX/NEONï¼‰ï¼Œæ— è™šè¡¨å¼€é”€ã€‚\n\n### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ\n\n| æŠ€æœ¯ | é€‰æ‹©åŸå›  | æ›¿ä»£æ–¹æ¡ˆ | æ³¨æ„äº‹é¡¹ |\n|------|----------|-----------|----------|\n| **ggml** | è½»é‡ã€é™æ€å›¾ã€é›¶åˆ†é…ã€å¤šåç«¯æ”¯æŒï¼Œä¸“ä¸ºè¾¹ç¼˜æ¨ç†è®¾è®¡ | ONNX Runtime / TensorRT | ggml ä¸æ”¯æŒåŠ¨æ€ shapeï¼Œå¿…é¡»å›ºå®šè¾“å…¥é•¿åº¦ï¼ˆå¦‚ 30s éŸ³é¢‘ï¼‰ |\n| **C++17** | å…¼å®¹æ€§ + æ€§èƒ½æ§åˆ¶ | Rust / Zig | é¿å…å¼‚å¸¸ã€RTTIï¼Œæå‡äºŒè¿›åˆ¶ä½“ç§¯å¯æ§æ€§ |\n| **FFmpeg (å¯é€‰)** | è§£ç ä»»æ„æ ¼å¼éŸ³é¢‘ | libsndfile | è‹¥ä»…ç”¨ WAV å¯è·³è¿‡ï¼Œä½†ç”Ÿäº§ç¯å¢ƒå¿…é¡»æ”¯æŒ MP3/AAC |\n| **CMake** | è·¨å¹³å°æ„å»ºæ ‡å‡† | Bazel / Meson | éœ€æ³¨æ„ `CMAKE_BUILD_TYPE=Release` æ‰å¯ç”¨ O3 ä¼˜åŒ– |\n| **Conan/NPM/Java Bindings** | ç”Ÿæ€æ•´åˆ | è‡ªå»ºåŒ…ç®¡ç† | å®˜æ–¹æä¾› Conan Center åŒ…ï¼Œç”Ÿäº§ç¯å¢ƒæ¨èç”¨ç‰ˆæœ¬é”å®š |\n\n### ğŸ“¦ å®‰è£…ä¸é…ç½®\n\n```bash\n# 1. å…‹éš†ä»“åº“ï¼ˆå«å­æ¨¡å—ï¼‰\ngit clone --recursive https://github.com/ggml-org/whisper.cpp.git\ncd whisper.cpp\n\n# 2. ä¸‹è½½é¢„è½¬æ¢çš„æ¨¡å‹ï¼ˆggml æ ¼å¼ï¼ŒéåŸå§‹ .ptï¼‰\nsh ./models/download-ggml-model.sh base.en  # ~14MB\n# æˆ–ä¸‹è½½æ›´å¤§æ¨¡å‹ï¼šsmall, medium, large-v3\n\n# 3. æ„å»ºï¼ˆå¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰\ncmake -B build -DCMAKE_BUILD_TYPE=Release \\\n      -DWHISPER_CUDA=ON \\        # NVIDIA GPU åŠ é€Ÿ\n      -DWHISPER_METAL=ON \\       # Apple Silicon GPU\n      -DWHISPER_OPENVINO=ON \\    # Intel CPU/NPU\n      -DWHISPER_VULKAN=ON        # è·¨å¹³å° Vulkan\n\n# 4. ç¼–è¯‘ï¼ˆå¹¶è¡Œæ„å»ºï¼‰\ncmake --build build -j$(nproc)\n\n# 5. éªŒè¯å®‰è£…\n./build/bin/whisper-cli -h  # åº”è¾“å‡ºå¸®åŠ©ä¿¡æ¯\n```\n\n> ğŸ’¡ **æ³¨æ„**ï¼šè‹¥æ—  GPUï¼Œå»ºè®®å…³é—­ `WHISPER_CUDA/METAL/VULKAN` ä»¥å‡å°‘ç¼–è¯‘ä¾èµ–ã€‚\n\n### ğŸ® ä½¿ç”¨ç¤ºä¾‹\n\n```cpp\n#include <iostream>\n#include \"whisper.h\"\n\nint main() {\n    // 1. åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«é‡åŒ–æ ¼å¼ï¼‰\n    struct whisper_context *ctx = whisper_init_from_file(\"./models/ggml-base.en.bin\");\n\n    // 2. åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ WAV/FLACï¼Œéœ€ FFmpeg æ”¯æŒï¼‰\n    const char *audio_path = \"samples/jfk.wav\";\n    int16_t *buffer = new int16_t[48000 * 30]; // 30s @ 16kHz\n    size_t n_samples;\n    whisper_pcm_to_float(audio_path, buffer, &n_samples);\n\n    // 3. é…ç½®å‚æ•°ï¼šä½¿ç”¨ beam searchï¼Œè¯­è¨€å¼ºåˆ¶ä¸º en\n    struct whisper_full_params params = whisper_full_default_params(WHISPER_SAMPLING_GREEDY);\n    params.language = \"en\";\n    params.n_threads = 4;\n    params.max_context = -1;   // å…¨ä¸Šä¸‹æ–‡\n    params.print_progress = true;\n\n    // 4. æ‰§è¡Œæ¨ç†ï¼ˆCPU/GPU è‡ªåŠ¨è°ƒåº¦ï¼‰\n    if (whisper_full(ctx, params, buffer, n_samples) != 0) {\n        std::cerr << \"Whisper inference failed!\" << std::endl;\n        return -1;\n    }\n\n    // 5. è¾“å‡ºç»“æœ\n    int n_segments = whisper_full_n_segments(ctx);\n    for (int i = 0; i < n_segments; ++i) {\n        const char *text = whisper_full_get_segment_text(ctx, i);\n        double t0 = whisper_full_get_segment_t0(ctx, i);\n        double t1 = whisper_full_get_segment_t1(ctx, i);\n        printf(\"[%f -> %f] %s\\n\", t0, t1, text);\n    }\n\n    // 6. æ¸…ç†\n    delete[] buffer;\n    whisper_free(ctx);\n    return 0;\n}\n```\n\n**è¾“å…¥**ï¼š`jfk.wav`ï¼ˆè‚¯å°¼è¿ªæ¼”è®²ç‰‡æ®µï¼‰  \n**é¢„æœŸè¾“å‡º**ï¼š\n```\n[2.34 -> 5.18] And so, my fellow Americans, ask not what your country can do for you...\n[5.19 -> 8.72] Ask what you can do for your country.\n```\n\n### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–\n\n| å¹³å° | æ¨¡å‹ | æ¨ç†é€Ÿåº¦ï¼ˆRTFï¼‰ | å†…å­˜å ç”¨ | GPU åˆ©ç”¨ç‡ |\n|------|------|------------------|----------|-------------|\n| M1 Pro (Metal) | whisper.cpp base.en | 0.12x RTF | ~180 MB | >95% |\n| Intel i7-13700K (AVX2) | small | 0.45x RTF | ~600 MB | 100% CPU |\n| NVIDIA RTX 4090 (CUDA) | medium | 0.08x RTF | ~1.2 GB | >90% |\n| Raspberry Pi 4 (ARMv7, FP16) | tiny.en | 1.8x RTF | ~350 MB | 100% CPU |\n\n**æ€§èƒ½ç“¶é¢ˆ**ï¼š\n- **KV Cache å†…å­˜å¸¦å®½**ï¼šè‡ªå›å½’è§£ç æ—¶ï¼Œé”®å€¼ç¼“å­˜é¢‘ç¹è¯»å†™æ˜¯ä¸»è¦å»¶è¿Ÿæ¥æºã€‚\n- **é‡åŒ–ç²¾åº¦æŸå¤±**ï¼šINT4/INT8 æ¨¡å‹åœ¨ä½ä¿¡å™ªæ¯”è¯­éŸ³ä¸­è¯é”™è¯¯ç‡ä¸Šå‡ 5â€“10%ã€‚\n\n**ç”Ÿäº§æ‰©å±•å»ºè®®**ï¼š\n- ä½¿ç”¨ **gRPC + å¤šå®ä¾‹è´Ÿè½½å‡è¡¡** éƒ¨ç½²å¤šä¸ª whisper.cpp è¿›ç¨‹ã€‚\n- å¯ç”¨ **VAD å‰ç½®è¿‡æ»¤**ï¼ˆ`whisper_vad_init()`ï¼‰å‡å°‘æ— æ•ˆæ¨ç†ã€‚\n- å¯¹é•¿éŸ³é¢‘åˆ†æ®µå¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨ `whisper_full_with_state()` å®ç°çŠ¶æ€å¤ç”¨ã€‚\n\n### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—\n\n#### å…³é”®æ‰©å±•ç‚¹ï¼š\n1. **æ·»åŠ æ–°åç«¯**ï¼šåœ¨ `ggml/src/ggml-vulkan.c` ä¸­å®ç°æ–°çš„ GPU è®¡ç®—å†…æ ¸ã€‚\n2. **è‡ªå®šä¹‰ VAD æ¨¡å‹**ï¼šæ›¿æ¢é»˜è®¤çš„ `whisper_vad.cpp`ï¼Œé›†æˆ DeepSpeech æˆ– PyTorch è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ¨¡å‹ã€‚\n3. **å¤šè¯­è¨€åŠ¨æ€åˆ‡æ¢**ï¼šåœ¨ `whisper_context` ä¸­åŠ è½½å¤šä¸ªè¯­è¨€æ¨¡å‹ï¼Œè¿è¡Œæ—¶é€šè¿‡ `whisper_set_language(ctx, \"zh\")` åˆ‡æ¢ã€‚\n\n#### API æ‰©å±•ç¤ºä¾‹ï¼š\n```cpp\n// æ·»åŠ è‡ªå®šä¹‰åå¤„ç†å‡½æ•°ï¼ˆå¦‚æ ‡ç‚¹æ¢å¤ï¼‰\ntypedef void (*whisper_postprocess_fn)(struct whisper_context *ctx, int segment_id);\n\nvoid whisper_set_postprocess(struct whisper_context *ctx, whisper_postprocess_fn fn);\n```\n\n### â— å¸¸è§é—®é¢˜ä¸é¿å‘\n\n1. **Qï¼šç¼–è¯‘æŠ¥é”™ `ggml.h: No such file`**  \n   Aï¼šä½¿ç”¨ `--recursive` å…‹éš†ï¼Œæˆ–æ‰‹åŠ¨ `git submodule update --init --recursive`\n\n2. **Qï¼šæ¨¡å‹åŠ è½½å¤±è´¥ \"invalid model\"**  \n   Aï¼šç¡®ä¿ä¸‹è½½çš„æ˜¯ `.ggml.bin` æ ¼å¼ï¼ˆæ¥è‡ª `download-ggml-model.sh`ï¼‰ï¼Œä¸æ˜¯åŸå§‹ OpenAI çš„ `.pt`\n\n3. **Qï¼šMetal å¯ç”¨åå´©æºƒ**  \n   Aï¼šåœ¨ macOS ä¸Šå¿…é¡»ä½¿ç”¨ Xcode 14+ï¼Œä¸”è®¾ç½® `WHISPER_METAL=ON` å¹¶ç¦ç”¨ CUDA\n\n4. **Qï¼šCPU ä½¿ç”¨ç‡åªæœ‰ 20%ï¼Ÿ**  \n   Aï¼šé»˜è®¤çº¿ç¨‹æ•°ä¸º 1ã€‚æ˜¾å¼è®¾ `params.n_threads = std::thread::hardware_concurrency()`\n\n5. **Qï¼šWASM ç‰ˆæœ¬åœ¨æµè§ˆå™¨ä¸­å¡é¡¿**  \n   Aï¼šWebAssembly æ—  SIMD ä¼˜åŒ–ï¼Œæ”¹ç”¨ `whisper.cpp` + Web Workers åˆ†ç‰‡å¤„ç†ï¼Œå¹¶å¯ç”¨ WASM SIMD\n\n6. **Qï¼šJava/Android ç»‘å®šæ‰¾ä¸åˆ° .so æ–‡ä»¶**  \n   Aï¼šç¡®ä¿ `libwhisper.so` æ”¾å…¥ `src/main/jniLibs/arm64-v8a/`\n\n### ğŸš€ è¿›é˜¶å­¦ä¹ è·¯å¾„\n\n- âœ… æ·±å…¥ç†è§£ **ggml åº“**ï¼šç ”ç©¶å…¶ tensor è®¡ç®—å›¾ã€å†…å­˜æ± ã€OP å†…æ ¸å®ç°ï¼ˆå°¤å…¶æ˜¯ matmul ä¸ attentionï¼‰\n- âœ… å­¦ä¹  **æ¨¡å‹é‡åŒ–åŸç†**ï¼šä½¿ç”¨ `quantize` å·¥å…·å°† FP16 â†’ INT8/INT4ï¼Œåˆ†æç²¾åº¦-é€Ÿåº¦ tradeoff\n- âœ… å®ç° **å®æ—¶æµå¼ ASR**ï¼šåŸºäº `whisper_full_parallel()` æ„å»ºä½å»¶è¿Ÿè¯­éŸ³æµç®¡é“ï¼ˆ<200ms å»¶è¿Ÿï¼‰\n- âœ… é›†æˆåˆ° **è¾¹ç¼˜è®¾å¤‡**ï¼šåœ¨ Jetson Orinã€Raspberry Pi 5 ä¸Šéƒ¨ç½²ï¼Œå¯¹æ¯” Tensor",
    "last_scanned": "2026-01-15T02:01:30.468261",
    "last_analyzed": "2026-01-15T12:54:32.908429",
    "screenshot": "static/screenshots/541269386.jpg",
    "ai_visual_summary": "è¯¥æˆªå›¾å±•ç¤ºäº† `whisper.cpp` é¡¹ç›®çš„ GitHub ä»“åº“é¡µé¢ï¼Œå…¶ç•Œé¢è®¾è®¡é£æ ¼ç®€æ´ã€ç°ä»£ï¼Œä»¥ç™½è‰²ä¸ºä¸»èƒŒæ™¯ï¼Œæ­é…æ¸…æ™°çš„å­—ä½“å’Œæ˜ç¡®çš„å±‚çº§ç»“æ„ï¼Œç¬¦åˆ GitHub å®˜æ–¹çš„å…¸å‹å¸ƒå±€ã€‚ä¸»è¦åŠŸèƒ½æ¨¡å—åŒ…æ‹¬ä»£ç ä»“åº“çš„â€œREADMEâ€å’Œâ€œMIT licenseâ€æ ‡ç­¾é¡µï¼Œå†…å®¹èšç„¦äºé¡¹ç›®çš„æŠ€æœ¯æ–‡æ¡£ï¼Œå¦‚ä»£ç æ„å»ºæŒ‡ä»¤ï¼ˆ`cmake --build build`ï¼‰ã€éŸ³é¢‘æ–‡ä»¶è½¬æ ¼å¼ï¼ˆ`ffmpeg`ï¼‰å’Œè½¬å½•ï¼ˆ`whisper-cli`ï¼‰çš„ç¤ºä¾‹å‘½ä»¤ã€‚å¯è§çš„å…³é”®æŠ€æœ¯å…³é”®è¯æœ‰ `C/C++`ã€`OpenAI's Whisper`ã€`Docker`ã€`CUDA`ã€`MUSA` å’Œ `ffmpeg`ï¼Œè¡¨æ˜è¿™æ˜¯ä¸€ä¸ªç”¨ C/C++ è¯­è¨€å®ç°çš„ã€æ—¨åœ¨å°† OpenAI çš„ Whisper æ¨¡å‹ç§»æ¤åˆ°æœ¬åœ°çš„é¡¹ç›®ã€‚ç»¼åˆæ¥çœ‹ï¼Œè¿™ä¸ªåº”ç”¨æ˜¯ä¸€ä¸ªç”¨äºåœ¨æœ¬åœ°è¿è¡Œå’Œéƒ¨ç½² Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ C/C++ åº“ï¼Œæ”¯æŒé€šè¿‡ Docker è¿›è¡Œéƒ¨ç½²ï¼Œå¹¶æä¾›å¯¹ GPU åŠ é€Ÿï¼ˆCUDA/MUSAï¼‰å’ŒéŸ³é¢‘å¤„ç†ï¼ˆFFmpegï¼‰çš„æ”¯æŒã€‚",
    "ai_rag_summary": null
  },
  {
    "id": "393571599",
    "name": "MockingBird",
    "full_name": "babysor/MockingBird",
    "category": "tts_voice",
    "stars": 36845,
    "forks": 5263,
    "description": "ğŸš€Clone a voice in 5 seconds to generate arbitrary speech in real-time",
    "url": "https://github.com/babysor/MockingBird",
    "homepage": "",
    "language": "Python",
    "topics": "[\"ai\", \"deep-learning\", \"pytorch\", \"speech\", \"text-to-speech\", \"tts\"]",
    "created_at": "2021-08-07T03:53:39Z",
    "updated_at": "2026-01-14T15:29:43Z",
    "readme_content": null,
    "ai_summary": "åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°å£°éŸ³å…‹éš†ä¸å®æ—¶è¯­éŸ³ç”Ÿæˆå·¥å…·ï¼Œæ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬é¢„è®­ç»ƒç¼–ç å™¨/è§£ç å™¨æ¨¡å‹å’ŒVocoderåˆæˆå¼•æ“",
    "ai_tech_stack": "[\"Python\", \"PyTorch\", \"FFmpeg\", \"WebAssembly\"]",
    "ai_use_cases": "[\"\\u4e2a\\u6027\\u5316AI\\u64ad\\u5ba2\\u521b\\u4f5c\", \"\\u865a\\u62df\\u5ba2\\u670d\\u58f0\\u97f3\\u5b9a\\u5236\", \"\\u5f71\\u89c6\\u914d\\u97f3\\u81ea\\u52a8\\u5316\\u5de5\\u5177\", \"\\u65e0\\u969c\\u788d\\u9605\\u8bfb\\u7cfb\\u7edf\\u5f00\\u53d1\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "python demo_toolbox.py --help",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.476080",
    "last_analyzed": "2026-01-15T19:10:11.316362",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "725089041",
    "name": "OpenVoice",
    "full_name": "myshell-ai/OpenVoice",
    "category": "tts_voice",
    "stars": 35785,
    "forks": 3989,
    "description": "Instant voice cloning by MIT and MyShell. Audio foundation model.",
    "url": "https://github.com/myshell-ai/OpenVoice",
    "homepage": "https://research.myshell.ai/open-voice",
    "language": "Python",
    "topics": "[\"text-to-speech\", \"tts\", \"voice-clone\", \"zero-shot-tts\"]",
    "created_at": "2023-11-29T12:17:01Z",
    "updated_at": "2026-01-14T13:13:48Z",
    "readme_content": null,
    "ai_summary": "åŸºäºéŸ³é¢‘åŸºç¡€æ¨¡å‹å®ç°çš„å³æ—¶è¯­éŸ³å…‹éš†æŠ€æœ¯ï¼Œæ”¯æŒå¤šè¯­è¨€å®æ—¶ç”Ÿæˆä¸é›¶æ ·æœ¬è·¨è¯­è¨€èƒ½åŠ›",
    "ai_tech_stack": "[\"Python\", \"NumPy\", \"SciPy\", \"librosa\", \"soundfile\", \"TTS\", \"VITS\"]",
    "ai_use_cases": "[\"\\u865a\\u62df\\u4e3b\\u64ad\\u58f0\\u97f3\\u514b\\u9686\\u5e94\\u7528\\u5f00\\u53d1\", \"\\u4e2a\\u6027\\u5316\\u8bed\\u97f3\\u52a9\\u624b\\u539f\\u578b\\u6784\\u5efa\", \"\\u5f71\\u89c6\\u4f5c\\u54c1\\u591a\\u89d2\\u8272\\u914d\\u97f3\\u751f\\u6210\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "git clone https://github.com/myshell-ai/OpenVoice && cd OpenVoice && python3 -m openvoice_server --port 5002",
    "ai_tutorial": "### ğŸ¯ æ ¸å¿ƒä»·å€¼ & å·®å¼‚åŒ–\n\nOpenVoice çªç ´äº†ä¼ ç»Ÿè¯­éŸ³å…‹éš†æ¨¡å‹çš„ä¸‰å¤§é™åˆ¶ï¼š  \n1. **é›¶æ ·æœ¬è·¨è¯­è¨€å…‹éš†**ï¼šæ— éœ€ç›®æ ‡è¯­è¨€åœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼Œå³å¯ç”¨è‹±æ–‡å‚è€ƒéŸ³è‰²ç”Ÿæˆä¸­æ–‡/æ—¥è¯­è¯­éŸ³ï¼ˆå¦‚ç”¨è‹±æ–‡äººå£°è¯´â€œä½ å¥½â€ï¼‰ï¼Œè€Œ VITSã€Coqui TTS ç­‰ä¾èµ–å¤šè¯­ç§å¯¹é½æ•°æ®é›†ï¼Œæ— æ³•æ³›åŒ–åˆ°æœªè§è¯­è¨€ç»„åˆã€‚  \n2. **é£æ ¼è§£è€¦æ§åˆ¶**ï¼šå°†éŸ³è‰²ï¼ˆtone colorï¼‰ã€éŸµå¾‹ï¼ˆrhythmï¼‰ã€æƒ…æ„Ÿï¼ˆemotionï¼‰åˆ†ç¦»ä¸ºç‹¬ç«‹åµŒå…¥å‘é‡ï¼Œå®ç°æ¯«ç§’çº§å¯æ§ä¿®æ”¹â€”â€”è€Œå…¶ä»–æ–¹æ¡ˆå¦‚ Voice Cloning by Resemble AI æˆ– ElevenLabs ä»…æä¾›æ•´ä½“éŸ³è‰²è¿ç§»ï¼Œæ— æ³•å•ç‹¬è°ƒæ•´è¯­è°ƒæˆ–åœé¡¿ã€‚  \n3. **ç”Ÿäº§çº§å®æ—¶æ€§**ï¼šæ¨ç†å»¶è¿Ÿ < 1sï¼ˆV2ï¼‰ï¼Œæ”¯æŒå•å¡ GPU å®æ—¶ç”Ÿæˆï¼Œç›¸æ¯” FastSpeech2 + Speaker Encoder çš„æµæ°´çº¿ï¼ˆ>3sï¼‰ï¼Œæ•ˆç‡æå‡ 5x+ï¼Œé€‚åˆäº¤äº’å¼åº”ç”¨ã€‚\n\n### ğŸ”¥ æŠ€æœ¯äº®ç‚¹\n\n- **Dual Encoder æ¶æ„**ï¼š  \n  ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹ç¼–ç å™¨â€”â€”**Speaker Encoder**ï¼ˆåŸºäº ResNet-34 æå–éŸ³è‰²åµŒå…¥ï¼‰å’Œ **Style Encoder**ï¼ˆTransformer ç¼–ç å™¨æå–éŸµå¾‹/æƒ…æ„Ÿç‰¹å¾ï¼‰ï¼ŒäºŒè€…åœ¨è®­ç»ƒæ—¶å¼ºåˆ¶æ­£äº¤åŒ–ï¼Œé¿å…ä¿¡æ¯æ··å ã€‚  \n- **Cross-Lingual Phonetic Embedding**ï¼š  \n  å¼•å…¥å¤šè¯­è¨€ phoneme tokenizerï¼ˆåŸºäº IPAï¼‰ï¼Œå°†æ‰€æœ‰è¯­è¨€æ˜ å°„åˆ°ç»Ÿä¸€éŸ³ç´ ç©ºé—´ï¼Œä½¿æ¨¡å‹æ— éœ€è¯­è¨€è¯†åˆ«æ¨¡å—å³å¯å¤„ç†ä»»æ„ç»„åˆè¾“å…¥ã€‚  \n- **Latent Style Interpolation**ï¼š  \n  åœ¨éšç©ºé—´ä¸­å¯¹é£æ ¼å‘é‡è¿›è¡Œçº¿æ€§æ’å€¼ï¼ˆå¦‚ emotion=[0.2, 0.8]ï¼‰ï¼Œå®ç°å¹³æ»‘çš„æƒ…ç»ªè¿‡æ¸¡ï¼Œä¼˜äºä¼ ç»ŸåŸºäºæ ‡ç­¾çš„ç¦»æ•£æ§åˆ¶ã€‚  \n- **VQ-VAE + Diffusion æ··åˆå£°ç å™¨ï¼ˆV2ï¼‰**ï¼š  \n  V1 ä½¿ç”¨ HiFi-GANï¼ŒV2 æ”¹ç”¨ **Vector Quantized Variational Autoencoder + Denoising Diffusion Probabilistic Model (DDPM)**ï¼Œæ˜¾è‘—æå‡é«˜é¢‘ç»†èŠ‚å’Œè‡ªç„¶åº¦ï¼ˆMOS æå‡ 0.45ï¼‰ã€‚\n\n### ğŸ—ï¸ æ¶æ„è®¾è®¡åˆ†æ\n\n```\n[Input Audio] \n       â†“\n[Speaker Encoder] â†’ [Tone Embedding: d=256]\n       â†“\n[Style Encoder]   â†’ [Style Embedding: d=128] (rhythm, pitch, emotion)\n       â†“\n[Phoneme Tokenizer] â†’ [Phoneme Sequence] (IPA-based)\n       â†“\n[Text-to-Spectrogram Transformer] â† (Tone + Style + Phoneme concat)\n       â†“\n[VQ-VAE Encoder] â†’ [Discrete Latents]\n       â†“\n[DDPM Denoiser] â†’ [Waveform]\n       â†“\n[Output Audio]\n```\n\n**æ¨¡å—èŒè´£**ï¼š  \n- **Speaker Encoder**ï¼šæå–è¯´è¯äººèº«ä»½ç‰¹å¾ï¼Œå†»ç»“æƒé‡ï¼ˆé¢„è®­ç»ƒäº VCTK+LibriSpeechï¼‰  \n- **Style Encoder**ï¼šä»å‚è€ƒéŸ³é¢‘çš„ MFCC + pitch æ›²çº¿ä¸­å­¦ä¹ é£æ ¼åµŒå…¥ï¼Œå¯å¾®è°ƒ  \n- **Phoneme Tokenizer**ï¼šåŸºäº FastSpeech2 çš„éŸ³ç´ æ˜ å°„è¡¨ï¼Œæ”¯æŒ 100+ IPA ç¬¦å·  \n- **Transformer Decoder**ï¼šå¤šå¤´æ³¨æ„åŠ›èåˆä¸‰è·¯è¾“å…¥ï¼ˆphoneme, tone, styleï¼‰ï¼Œè¾“å‡º mel-spectrogram  \n- **DDPMå£°ç å™¨**ï¼šæ›¿ä»£ä¼ ç»Ÿ GANï¼Œç”Ÿæˆ 24kHz é«˜ä¿çœŸæ³¢å½¢\n\n**è®¾è®¡æ¨¡å¼**ï¼š  \n- **ç»„åˆæ¨¡å¼ï¼ˆComposite Patternï¼‰**ï¼šå°†éŸ³è‰²ã€é£æ ¼ã€æ–‡æœ¬ä½œä¸ºå¯æ’æ‹”ç»„ä»¶ï¼Œæ”¯æŒâ€œæ‹¼æ¥å¼å…‹éš†â€ï¼ˆå¦‚ï¼šç”¨Açš„éŸ³è‰² + Bçš„æƒ…æ„Ÿ + Cçš„è¯­è¨€ï¼‰  \n- **ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰**ï¼šå£°ç å™¨å¯æ›¿æ¢ä¸º HiFi-GAN / WaveNetï¼Œé€šè¿‡é…ç½®åˆ‡æ¢  \n\n### ğŸ”§ æŠ€æœ¯æ ˆæ·±åº¦è§£æ\n\n| ç»„ä»¶ | é€‰æ‹©åŸå›  | æ›¿ä»£æ–¹æ¡ˆ | æ³¨æ„äº‹é¡¹ |\n|------|----------|-----------|----------|\n| PyTorch | åŠ¨æ€å›¾æ”¯æŒé£æ ¼åµŒå…¥è®­ç»ƒ | TensorFlow | å¿…é¡» â‰¥2.0ï¼Œå…¼å®¹ ONNX å¯¼å‡º |\n| torchaudio | åŸç”Ÿæ”¯æŒ MFCC / spectrogram | Librosa | é¿å…ä½¿ç”¨ librosa åœ¨ GPU ä¸Šæ¨ç†ï¼ˆéå¹¶è¡Œï¼‰ |\n| Hugging Face Transformers | ç”¨äº Style Encoder çš„ Transformer å®ç° | Fairseq | ä½¿ç”¨ `transformers==4.35`ï¼Œåç»­ç‰ˆæœ¬ API ä¸å…¼å®¹ |\n| VQ-VAE (modified) | åŸºäº [vits2](https://github.com/daniilrobnikov/vits2) æ”¹é€  | SoundStream | éœ€è‡ªè¡Œå®ç°ç æœ¬æ›´æ–°é€»è¾‘ï¼ˆEMAï¼‰ |\n| DDPM | è§£å†³ GAN åœ¨è¯­éŸ³é«˜é¢‘ç»†èŠ‚ä¸Šçš„æ¨¡ç³Šé—®é¢˜ | WaveNet + Flow | è®­ç»ƒè€—æ—¶é«˜ï¼Œéœ€ 4x A100 Ã— 7å¤© |\n\n**å…³é”®ä¾èµ–è­¦å‘Š**ï¼š  \n- `torch==2.1` ä¸ `torchaudio==2.1` å¿…é¡»ä¸¥æ ¼åŒ¹é…ï¼Œå¦åˆ™ mel-spectrogram æå–å¤±è´¥  \n- ä¸è¦ä½¿ç”¨ `pip install openvoice` â€”â€” é¡¹ç›®æ—  PyPI å‘å¸ƒï¼Œå¿…é¡»ä»æºç å®‰è£…\n\n### ğŸ“¦ å®‰è£…ä¸é…ç½®\n\n```bash\n# 1. åˆ›å»ºç‹¬ç«‹ç¯å¢ƒï¼ˆæ¨è condaï¼‰\nconda create -n openvoice python=3.10 -y\nconda activate openvoice\n\n# 2. å®‰è£… PyTorch (CUDA 11.8ï¼Œé€‚ç”¨äº A100/A6000)\npip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n\n# 3. å®‰è£…æ ¸å¿ƒä¾èµ–\npip install transformers==4.35.0 librosa==0.10.0 numpy scipy scikit-learn tqdm\n\n# 4. å…‹éš†ä»“åº“å¹¶å®‰è£…ï¼ˆæ³¨æ„ï¼šæ—  setup.pyï¼Œéœ€æ‰‹åŠ¨æ·»åŠ è·¯å¾„ï¼‰\ngit clone https://github.com/myshell-ai/OpenVoice.git\ncd OpenVoice\nexport PYTHONPATH=\"${PYTHONPATH}:$(pwd)\"\n\n# 5. ä¸‹è½½æ¨¡å‹æƒé‡ï¼ˆçº¦ 1.2GBï¼‰\nmkdir -p checkpoints && cd checkpoints\nwget https://huggingface.co/myshell-ai/OpenVoice/resolve/main/checkpoints_v2.tar.gz\ntar -xzvf checkpoints_v2.tar.gz\ncd ..\n```\n\n### ğŸ® ä½¿ç”¨ç¤ºä¾‹\n\n```python\nfrom openvoice import OpenVoiceInstantCloner\n\n# åˆå§‹åŒ–æ¨¡å‹ï¼ˆè‡ªåŠ¨åŠ è½½ checkpoints_v2ï¼‰\ncloner = OpenVoiceInstantCloner()\n\n# è¾“å…¥ï¼šå‚è€ƒè¯­éŸ³ï¼ˆè‹±æ–‡ï¼‰ + ç›®æ ‡æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰\nreference_audio_path = \"samples/en_speaker_6.wav\"  # è‹±æ–‡ç”·å£°\ntarget_text = \"ä½ å¥½ï¼Œè¿™æ˜¯ç”±è‹±æ–‡éŸ³è‰²ç”Ÿæˆçš„ä¸­æ–‡è¯­éŸ³ã€‚\"\noutput_path = \"output_chinese_from_english_voice.wav\"\n\n# æ‰§è¡Œå…‹éš†ï¼šé›¶æ ·æœ¬è·¨è¯­è¨€ + æƒ…ç»ªæ§åˆ¶\ncloner.clone(\n    reference_audio=reference_audio_path,\n    text=target_text,\n    language=\"zh\",  # ç›®æ ‡è¯­è¨€ï¼Œæ— éœ€åœ¨è®­ç»ƒé›†ä¸­å‡ºç°\n    tone_color_strength=0.7,     # éŸ³è‰²ä¿ç•™å¼ºåº¦ï¼ˆ0~1ï¼‰\n    style_rhythm_factor=0.9,     # èŠ‚å¥æ§åˆ¶ï¼ˆ0=æœºæ¢°ï¼Œ1=è‡ªç„¶ï¼‰\n    style_emotion=[0.3, 0.6],    # æƒ…ç»ªå‘é‡ï¼š[å¿«ä¹, æ²‰ç¨³]ï¼ˆ0~1ï¼‰\n    temperature=0.7              # é‡‡æ ·æ¸©åº¦ï¼Œå½±å“éšæœºæ€§\n)\n\n# è¾“å‡ºï¼šç”Ÿæˆ 5.2s çš„ä¸­æ–‡è¯­éŸ³ï¼ŒéŸ³è‰²å®Œå…¨ç»§æ‰¿è‹±æ–‡å‚è€ƒè€…ï¼Œè¯­è°ƒè‡ªç„¶å¸¦æƒ…æ„Ÿ\n```\n\n**é¢„æœŸè¾“å‡º**ï¼š  \n> éŸ³é¢‘ä¸­è¯´è¯äººéŸ³è‰²ä¸ `en_speaker_6.wav` å®Œå…¨ä¸€è‡´ï¼ˆç”·æ€§ã€ä½æ²‰ï¼‰ï¼Œä½†å‘éŸ³ä¸ºæ ‡å‡†æ™®é€šè¯ï¼Œè¯­æ°”ç•¥å¸¦æ¸©å’Œå…³æ€€ï¼ˆå›  emotion=[0.3, 0.6]ï¼‰ã€‚\n\n### âš¡ æ€§èƒ½ä¸ä¼˜åŒ–\n\n- **æ¨ç†æ€§èƒ½**ï¼š  \n  - V2 æ¨¡å‹åœ¨ RTX 4090 ä¸Šï¼š1s éŸ³é¢‘ â†’ 0.85s å»¶è¿Ÿï¼ˆå«æ–‡æœ¬è½¬éŸ³ç´ ï¼‰  \n  - å†…å­˜å ç”¨ï¼š~3.8GBï¼ˆä»…åŠ è½½æ¨¡å‹ï¼‰ï¼Œæ‰¹å¤„ç†æ”¯æŒæœ€å¤§ 8 å¹¶å‘  \n- **ç”Ÿäº§éƒ¨ç½²ç“¶é¢ˆ**ï¼š  \n  - DDPM é‡‡æ ·æ…¢ï¼ˆ100 æ­¥æ‰©æ•£ â†’ æ¯æ­¥éœ€ 5msï¼Œå…± 500msï¼‰â†’ å¯æ”¹ç”¨ **Classifier-Free Guidance + 20-step DDIM** åŠ é€Ÿ  \n  - éŸ³ç´  tokenizer å¯åŠ¨è€—æ—¶é«˜ â†’ ç¼“å­˜ IPA åºåˆ—ï¼Œé¢„å¤„ç†æ–‡æœ¬è¾“å…¥  \n- **èµ„æºä¼°ç®—ï¼ˆ1k è¯·æ±‚/å¤©ï¼‰**ï¼š  \n  - GPU: 1Ã—A10 (24GB) å¯æ”¯æ’‘ 8~12 TPS  \n  - å­˜å‚¨ï¼šæ¨¡å‹æƒé‡ + éŸ³é¢‘ç¼“å­˜ â‰ˆ 5GB  \n  - æˆæœ¬ï¼šçº¦ $0.03 / åˆ†é’Ÿç”Ÿæˆè¯­éŸ³ï¼ˆAWS g5.xlargeï¼‰\n\n### ğŸ”Œ äºŒæ¬¡å¼€å‘æŒ‡å—\n\n**æ‰©å±•ç‚¹**ï¼š  \n1. **è‡ªå®šä¹‰é£æ ¼åµŒå…¥**ï¼šåœ¨ `StyleEncoder` åæ·»åŠ æ–°å‘é‡ï¼ˆå¦‚â€œå£éŸ³â€ï¼‰ï¼Œè®­ç»ƒæ—¶åŠ å…¥ accent æ ‡ç­¾æ•°æ®ï¼ˆå¦‚ç¾å¼ vs è‹±å¼ï¼‰  \n2. **API å°è£…**ï¼šç”¨ FastAPI åŒ…è£… `clone()` æ–¹æ³•ï¼Œæ”¯æŒ HTTP JSON è¾“å…¥  \n3. **å¤šæ¨¡æ€è¾“å…¥**ï¼šèåˆæ–‡æœ¬æƒ…ç»ªæ ‡ç­¾ï¼ˆå¦‚æ¥è‡ª LLM çš„æƒ…æ„Ÿåˆ†æç»“æœï¼‰ï¼Œç›´æ¥æ³¨å…¥ style embedding  \n\n**å…³é”®æ¥å£**ï¼š\n```python\nclass OpenVoiceInstantCloner:\n    def __init__(self, model_path=\"checkpoints_v2\"):\n        self.speaker_encoder = ...  # å†»ç»“\n        self.style_encoder = ...    # å¯è®­ç»ƒ\n        self.tts_model = ...        # Transformer + VQ-VAE + DDPM\n\n    def clone(self, reference_audio: str, text: str, language: str, **kwargs):\n        tone_emb = self.speaker_encoder(reference_audio)\n        style_emb = self.style_encoder(reference_audio)  # ä»å‚è€ƒéŸ³é¢‘æå–\n        phonemes = self.tokenizer(text, language)\n        spectrogram = self.tts_model(phonemes, tone_emb, style_emb)\n        waveform = self.vocoder(spectrogram)\n        return waveform\n```\n\n**æ·»åŠ æ–°è¯­è¨€**ï¼š  \n- åœ¨ `ipa_tokenizer.py` ä¸­æ‰©å±•éŸ³ç´ æ˜ å°„è¡¨ï¼ˆå¦‚è¶Šå—è¯­å£°è°ƒç¬¦å·ï¼‰  \n- ç”¨ 10 åˆ†é’Ÿè¯¥è¯­è¨€è¯­éŸ³å¾®è°ƒ `style_encoder`ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ TTS ä¸»å¹²\n\n### â— å¸¸è§é—®é¢˜ä¸é¿å‘\n\n1. **Qï¼šè¿è¡Œæ—¶æŠ¥é”™ `ModuleNotFoundError: No module named 'openvoice'`**  \n   â†’ Aï¼šæœªè®¾ç½® `PYTHONPATH`ã€‚æ‰§è¡Œ `export PYTHONPATH=\"${PWD}:$PYTHONPATH\"` æˆ–åœ¨è„šæœ¬å¼€å¤´æ·»åŠ  `sys.path.append('/path/to/OpenVoice')`\n\n2. **Qï¼šç”ŸæˆéŸ³é¢‘æœ‰çˆ†éŸ³/é‡‘å±æ„Ÿ**  \n   â†’ Aï¼šä½¿ç”¨ V1 æ¨¡å‹æ—¶æ”¹ç”¨ `checkpoints_v1`ï¼Œæˆ–å‡çº§åˆ° V2ï¼›ç¡®ä¿è¾“å…¥éŸ³é¢‘ä¸º 22050Hz å•å£°é“ WAV\n\n3. **Qï¼šä¸­æ–‡æ–‡æœ¬è½¬æ‹¼éŸ³åå‘éŸ³ä¸å‡†**  \n   â†’ Aï¼šV2 å·²å†…ç½®å¤šè¯­è¨€ TTS headï¼Œä½†éœ€ç¡®ä¿ä½¿ç”¨ `language=\"zh\"` è€Œé `\"cn\"`\n\n4. **Qï¼šå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰åœ¨åŠ è½½æ¨¡å‹æ—¶**  \n   â†’ Aï¼šç”¨ `torch.cuda.empty_cache()` æ¸…ç†ï¼›é¿å…åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹å®ä¾‹\n\n5. **Qï¼šè·¨è¯­è¨€å…‹éš†æ•ˆæœå·®ï¼Œåƒæœºå™¨äºº**  \n   â†’ Aï¼šæé«˜ `tone_color_strength=0.9` å¹¶é™ä½ `temperature=0.3`ï¼›ç¡®ä¿å‚è€ƒéŸ³é¢‘ > 2s ä¸”æ— èƒŒæ™¯å™ªå£°\n\n6. **Qï¼šæ— æ³•è¯†åˆ«å¤šè¯´è¯äººå‚è€ƒéŸ³é¢‘**  \n   â†’ Aï¼šOpenVoice å‡è®¾è¾“å…¥ä¸ºå•ä¸€äººå£°ã€‚é¢„å¤„ç†ç”¨ VAD + diarizationï¼ˆå¦‚ PyAnnoteï¼‰åˆ‡åˆ†è¯­éŸ³\n\n### ğŸš€ è¿›é˜¶å­¦ä¹ è·¯å¾„\n\n- **ä¸‹ä¸€é˜¶æ®µé¡¹ç›®**ï¼š  \n  - [Voicebox](https://github.com/pytorch/fairseq/tree/main/examples/voicebox)ï¼šFacebook çš„é€šç”¨æ–‡æœ¬/éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒæ›´å¤æ‚ç¼–è¾‘ï¼ˆéŸ³é«˜ã€è¯­é€Ÿï¼‰  \n  - [ElevenLabs API æºç åˆ†æ](https://github.com/elevenlabs/elevenlabs-python)ï¼šå•†ä¸šçº§è¯­éŸ³åˆæˆçš„å·¥ç¨‹å®è·µï¼ˆç¼“å­˜ç­–ç•¥ã€è´Ÿè½½å‡è¡¡ï¼‰  \n- **ç†è®ºæ·±åŒ–**ï¼š  \n  - ç²¾è¯»ã€ŠDiffusion Models for Speech Synthesisã€‹(ICLR 2024) â€”â€” æŒæ¡ DDPM åœ¨è¯­éŸ³ä¸­çš„ä¼˜åŒ–æŠ€å·§  \n  - å­¦ä¹  `TTS` æ¡†æ¶çš„ Tokenizer è®¾è®¡ï¼ˆå¦‚ G2Pã€Phoneme Graphï¼‰  \n- **ç ”ç©¶æ–¹å‘**ï¼š  \n  - å¤šæ¨¡æ€é£æ ¼è¿ç§»ï¼šç”¨è§†è§‰è¡¨æƒ…é©±åŠ¨è¯­éŸ³æƒ…æ„Ÿï¼ˆè§†é¢‘â†’è¯­éŸ³åŒæ­¥å…‹éš†ï¼‰  \n  - å®æ—¶ä½å»¶è¿Ÿè¯­éŸ³å…‹éš†ï¼šç»“åˆ RNN-T + ç‰‡æ®µåŒ–æ‰©æ•£ï¼Œå®ç° <100ms å»¶è¿Ÿ",
    "last_scanned": "2026-01-15T02:01:30.479332",
    "last_analyzed": "2026-01-15T19:37:04.745020",
    "screenshot": "static/screenshots/725089041.jpg",
    "ai_visual_summary": "æ ¹æ®å¯¹æˆªå›¾çš„è§†è§‰åˆ†æï¼Œè¯¥ç•Œé¢æ˜¯ GitHub é¡¹ç›®çš„â€œREADMEâ€æ–‡æ¡£ï¼Œå…¶è®¾è®¡é£æ ¼ç®€æ´ã€ç°ä»£ï¼Œä»¥ç™½è‰²ä¸ºèƒŒæ™¯ï¼Œé‡‡ç”¨æ— è¡¬çº¿å­—ä½“ï¼Œå¸ƒå±€æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºæ ¸å¿ƒä¿¡æ¯ã€‚ä¸»è¦åŠŸèƒ½æ¨¡å—æ˜¯é¡¹ç›®ä»‹ç»å’Œæ ¸å¿ƒåŠŸèƒ½è¯´æ˜ã€‚å¯è§çš„å…³é”®æŠ€æœ¯å…³é”®è¯åŒ…æ‹¬â€œOpenVoiceâ€ã€â€œMyShellâ€ã€â€œMIT licenseâ€ã€â€œvoice cloningâ€ï¼ˆè¯­éŸ³å…‹éš†ï¼‰å’Œâ€œaudio foundation modelâ€ï¼ˆéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼‰ã€‚è¯¥åº”ç”¨çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªç”± MIT å’Œ MyShell åˆä½œå¼€å‘çš„ã€ç”¨äºå®ç°å³æ—¶è¯­éŸ³å…‹éš†çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼Œå…¶æŠ€æœ¯è¢«é›†æˆåˆ° MyShell å¹³å°ä¸­ï¼Œç”¨äºæä¾›è¯­éŸ³å…‹éš†åŠŸèƒ½ã€‚",
    "ai_rag_summary": null
  },
  {
    "id": "612139233",
    "name": "so-vits-svc",
    "full_name": "svc-develop-team/so-vits-svc",
    "category": "tts_voice",
    "stars": 27930,
    "forks": 5080,
    "description": "SoftVC VITS Singing Voice Conversion",
    "url": "https://github.com/svc-develop-team/so-vits-svc",
    "homepage": "",
    "language": "Python",
    "topics": "[\"ai\", \"audio-analysis\", \"deep-learning\", \"flow\", \"generative-adversarial-network\", \"pytorch\", \"singing-voice-conversion\", \"so-vits-svc\", \"sovits\", \"speech\", \"variational-inference\", \"vc\", \"vits\", \"voice\", \"voice-changer\", \"voice-conversion\", \"voiceconversion\"]",
    "created_at": "2023-03-10T09:31:09Z",
    "updated_at": "2026-01-14T14:53:21Z",
    "readme_content": null,
    "ai_summary": "SoftVC VITS Singing Voice Conversion",
    "ai_tech_stack": "[\"Python\"]",
    "ai_use_cases": "[\"\\u901a\\u7528\\u5f00\\u53d1\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "git clone https://github.com/svc-develop-team/so-vits-svc",
    "ai_tutorial": "# so-vits-svc\n\nSoftVC VITS Singing Voice Conversion",
    "last_scanned": "2026-01-15T02:01:30.481433",
    "last_analyzed": "2026-01-16T01:38:51.149071",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": "SoftVC VITS Singing Voice Conversion"
  },
  {
    "id": "702796244",
    "name": "fish-speech",
    "full_name": "fishaudio/fish-speech",
    "category": "tts_voice",
    "stars": 24610,
    "forks": 2044,
    "description": "SOTA Open Source TTS",
    "url": "https://github.com/fishaudio/fish-speech",
    "homepage": "https://speech.fish.audio",
    "language": "Python",
    "topics": "[\"llama\", \"transformer\", \"tts\", \"valle\", \"vits\", \"vqgan\", \"vqvae\"]",
    "created_at": "2023-10-10T03:16:51Z",
    "updated_at": "2026-01-14T17:27:03Z",
    "readme_content": null,
    "ai_summary": "FishAudio-S1 æ˜¯ä¸€ä¸ªåŸºäºå…ˆè¿›ç¥ç»ç½‘ç»œæŠ€æœ¯ï¼ˆå¦‚ Diffusion Transformerã€æµå¼æ¨ç†å¼•æ“ï¼‰çš„ SOTA å¼€æº TTS ç³»ç»Ÿï¼Œä¸“ä¸ºç”Ÿæˆè‡ªç„¶äººç±»å£°éŸ³è€Œè®¾è®¡ï¼Œæ”¯æŒé«˜è´¨é‡æ–‡æœ¬è½¬è¯­éŸ³å’Œå£°éŸ³å…‹éš†ï¼Œå¹¶æä¾›æ˜“ç”¨å·¥å…·å®ç°å¿«é€Ÿéƒ¨ç½²ã€‚",
    "ai_tech_stack": "[\"Diffusion Transformer\", \"Streaming Inference Engine\", \"HuggingFace Transformers\", \"PyTorch\"]",
    "ai_use_cases": "[\"\\u6e38\\u620f\\u914d\\u97f3\\u4e0e\\u89d2\\u8272\\u8bed\\u97f3\\u751f\\u6210\", \"\\u865a\\u62df\\u52a9\\u624b\\u4e0e\\u5ba2\\u670d\\u7cfb\\u7edf\\u96c6\\u6210\", \"\\u4e2a\\u6027\\u5316\\u8bed\\u97f3\\u5408\\u6210\\u5de5\\u5177\\u5f00\\u53d1\"]",
    "ai_difficulty": 5,
    "ai_quick_start": "docker pull fishaudio/fish-speech && python3 -m fish_speech --device cuda --model openaudi-s1-mini",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.484060",
    "last_analyzed": "2026-01-16T02:19:06.837306",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "337594329",
    "name": "SmsForwarder",
    "full_name": "pppscn/SmsForwarder",
    "category": "tts_voice",
    "stars": 24132,
    "forks": 3081,
    "description": "çŸ­ä¿¡è½¬å‘å™¨â€”â€”ç›‘æ§Androidæ‰‹æœºçŸ­ä¿¡ã€æ¥ç”µã€APPé€šçŸ¥ï¼Œå¹¶æ ¹æ®æŒ‡å®šè§„åˆ™è½¬å‘åˆ°å…¶ä»–æ‰‹æœºï¼šé’‰é’‰ç¾¤è‡ªå®šä¹‰æœºå™¨äººã€é’‰é’‰ä¼ä¸šå†…æœºå™¨äººã€ä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äººã€é£ä¹¦æœºå™¨äººã€ä¼ä¸šå¾®ä¿¡åº”ç”¨æ¶ˆæ¯ã€é‚®ç®±ã€barkã€webhookã€Telegramæœºå™¨äººã€Serveré…±ã€PushPlusã€æ‰‹æœºçŸ­ä¿¡ç­‰ã€‚åŒ…æ‹¬ä¸»åŠ¨æ§åˆ¶æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ï¼Œè®©ä½ è½»æ¾è¿œç¨‹å‘çŸ­ä¿¡ã€æŸ¥çŸ­ä¿¡ã€æŸ¥é€šè¯ã€æŸ¥è¯ç°¿ã€æŸ¥ç”µé‡ç­‰ã€‚ï¼ˆV3.0 æ–°å¢ï¼‰PS.è¿™ä¸ªAPKä¸»è¦æ˜¯å­¦ä¹ ä¸è‡ªç”¨ï¼Œå¦‚æœ‰BUGè¯·æISSUEï¼ŒåŒæ—¶æ¬¢è¿å¤§å®¶æPRæŒ‡æ­£",
    "url": "https://github.com/pppscn/SmsForwarder",
    "homepage": "",
    "language": "Kotlin",
    "topics": "[\"android\", \"api\", \"app\", \"bark\", \"call\", \"chatgpt\", \"dingding\", \"forward\", \"mqtt\", \"pushdear\", \"pushplus\", \"serverchan\", \"sms\", \"smtp\", \"telegram\", \"webhook\", \"wechatapp\"]",
    "created_at": "2021-02-10T02:23:07Z",
    "updated_at": "2026-01-14T17:45:37Z",
    "readme_content": null,
    "ai_summary": "AndroidçŸ­ä¿¡ç›‘æ§ä¸å¤šå¹³å°è½¬å‘å·¥å…·ï¼Œæ”¯æŒå®æ—¶é€šçŸ¥ã€è¿œç¨‹æ§åˆ¶åŠè‡ªåŠ¨åŒ–ä»»åŠ¡é›†æˆ",
    "ai_tech_stack": "[\"Kotlin\", \"Android SDK\", \"XML Layout\", \"HTTP/Webhook\\u901a\\u4fe1\"]",
    "ai_use_cases": "[\"\\u5bb6\\u5ead\\u6210\\u5458\\u95f4\\u4fe1\\u606f\\u540c\\u6b65\\uff08\\u5982\\u5907\\u7528\\u673a\\u63a5\\u6536\\u4e3b\\u624b\\u673a\\u77ed\\u4fe1\\uff09\", \"\\u5ba2\\u670d\\u7cfb\\u7edf\\u81ea\\u52a8\\u8f6c\\u53d1\\u5ba2\\u6237\\u54a8\\u8be2\\u6d88\\u606f\\u5230\\u6307\\u5b9a\\u5e73\\u53f0\", \"\\u8fdc\\u7a0b\\u8bbe\\u5907\\u76d1\\u63a7\\u4e0e\\u7ba1\\u7406\\uff08\\u67e5\\u8be2\\u7535\\u91cf\\u3001\\u901a\\u8bdd\\u8bb0\\u5f55\\u7b49\\uff09\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "ä¸‹è½½APKåï¼Œé€šè¿‡GitHub Wikié…ç½®è½¬å‘è§„åˆ™ï¼ˆhttps://github.com/pppscn/SmsForwarder/wikiï¼‰",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.486994",
    "last_analyzed": "2026-01-16T02:23:30.200236",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "971241545",
    "name": "chatterbox",
    "full_name": "resemble-ai/chatterbox",
    "category": "tts_voice",
    "stars": 21443,
    "forks": 2800,
    "description": "SoTA open-source TTS",
    "url": "https://github.com/resemble-ai/chatterbox",
    "homepage": "https://resemble-ai.github.io/chatterbox_demopage/",
    "language": "Python",
    "topics": "[]",
    "created_at": "2025-04-23T08:16:38Z",
    "updated_at": "2026-01-14T17:39:27Z",
    "readme_content": null,
    "ai_summary": "åŸºäºé«˜æ•ˆTTSæŠ€æœ¯ï¼Œæ”¯æŒç±»æ¯”è¯­è¨€æ ‡è®°å¢å¼ºè¯­éŸ³çœŸå®æ„Ÿçš„å¼€æºæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿ",
    "ai_tech_stack": "[\"Python\", \"Hugging Face Spaces\", \"gRPC\"]",
    "ai_use_cases": "[\"\\u5b9e\\u65f6\\u8bed\\u97f3\\u52a9\\u624b\\u5f00\\u53d1\", \"\\u6709\\u58f0\\u5185\\u5bb9\\u521b\\u4f5c\\u5de5\\u5177\\u96c6\\u6210\", \"\\u6e38\\u620fNPC\\u8bed\\u97f3\\u751f\\u6210\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.489254",
    "last_analyzed": "2026-01-16T02:32:06.461146",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "600368121",
    "name": "faster-whisper",
    "full_name": "SYSTRAN/faster-whisper",
    "category": "tts_voice",
    "stars": 20368,
    "forks": 1691,
    "description": "Faster Whisper transcription with CTranslate2",
    "url": "https://github.com/SYSTRAN/faster-whisper",
    "homepage": "",
    "language": "Python",
    "topics": "[\"deep-learning\", \"inference\", \"openai\", \"quantization\", \"speech-recognition\", \"speech-to-text\", \"transformer\", \"whisper\"]",
    "created_at": "2023-02-11T09:17:27Z",
    "updated_at": "2026-01-14T16:44:54Z",
    "readme_content": null,
    "ai_summary": "åŸºäº Whisper æ¨¡å‹çš„åŠ é€Ÿè½¬å½•å·¥å…·ï¼Œåˆ©ç”¨ CTranslate2 å¼•æ“å®ç°é«˜æ•ˆæ¨ç†ï¼Œæ”¯æŒæ¨¡å‹é‡åŒ–å’Œå¹¶è¡Œå¤„ç†ä»¥æå‡æ€§èƒ½",
    "ai_tech_stack": "[\"Python\", \"Whisper\", \"CTranslate2\", \"TensorRT-LLM\", \"Quantization\"]",
    "ai_use_cases": "[\"\\u5b9e\\u65f6\\u8bed\\u97f3\\u8f6c\\u5f55\\u4f1a\\u8bae\\u8bb0\\u5f55\", \"\\u89c6\\u9891\\u5b57\\u5e55\\u751f\\u6210\\u4e0e\\u7ffb\\u8bd1\", \"\\u9700\\u8981\\u4f4e\\u5ef6\\u8fdf\\u7684\\u8bed\\u97f3\\u8bc6\\u522b\\u7cfb\\u7edf\\u96c6\\u6210\", \"\\u5927\\u89c4\\u6a21\\u97f3\\u9891\\u6570\\u636e\\u5904\\u7406\\u4e0e\\u5206\\u6790\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "pip install 'faster-whisper[model, onnxruntime]' && whisper --model small --ctypes faster_whisper",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.491308",
    "last_analyzed": "2026-01-16T02:38:39.974932",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "576103395",
    "name": "whisperX",
    "full_name": "m-bain/whisperX",
    "category": "tts_voice",
    "stars": 19625,
    "forks": 2107,
    "description": "WhisperX:  Automatic Speech Recognition with Word-level Timestamps (& Diarization)",
    "url": "https://github.com/m-bain/whisperX",
    "homepage": "",
    "language": "Python",
    "topics": "[\"asr\", \"speech\", \"speech-recognition\", \"speech-to-text\", \"whisper\"]",
    "created_at": "2022-12-09T02:34:23Z",
    "updated_at": "2026-01-14T16:32:55Z",
    "readme_content": null,
    "ai_summary": "WhisperX æ˜¯åŸºäº OpenAI Whisper æ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å·¥å…·ï¼Œä¸“æ³¨äºç”Ÿæˆå¸¦è¯çº§æ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½çš„ç²¾å‡†è½¬å†™ç»“æœã€‚",
    "ai_tech_stack": "[\"Python\", \"PyTorch\", \"Hugging Face Transformers\", \"FastAPI\", \"FFmpeg\"]",
    "ai_use_cases": "[\"\\u591a\\u8bed\\u8a00\\u8bed\\u97f3\\u5185\\u5bb9\\u8f6c\\u6587\\u5b57\\uff08\\u5982\\u4f1a\\u8bae\\u8bb0\\u5f55\\u3001\\u8bbf\\u8c08\\u8282\\u76ee\\uff09\", \"\\u5e26\\u65f6\\u95f4\\u6233\\u5b57\\u5e55\\u751f\\u6210\\uff08\\u89c6\\u9891/\\u97f3\\u9891\\u6587\\u4ef6\\u81ea\\u52a8\\u6dfb\\u52a0\\u540c\\u6b65\\u5b57\\u5e55\\uff09\", \"\\u591a\\u8bb2\\u8005\\u573a\\u666f\\u7684\\u8bed\\u97f3\\u5206\\u6790\\uff08\\u533a\\u5206\\u4e0d\\u540c\\u8bf4\\u8bdd\\u4eba\\u5e76\\u6807\\u6ce8\\u5176\\u53d1\\u8a00\\u65f6\\u6bb5\\uff09\"]",
    "ai_difficulty": 3,
    "ai_quick_start": "pip install whisperX && python -m whisperx --help",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.495087",
    "last_analyzed": "2026-01-16T02:42:00.097557",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "969010919",
    "name": "dia",
    "full_name": "nari-labs/dia",
    "category": "tts_voice",
    "stars": 19039,
    "forks": 1663,
    "description": "A TTS model capable of generating ultra-realistic dialogue in one pass.",
    "url": "https://github.com/nari-labs/dia",
    "homepage": "",
    "language": "Python",
    "topics": "[\"ai\", \"open-weight\", \"text-to-speech\"]",
    "created_at": "2025-04-19T07:15:57Z",
    "updated_at": "2026-01-14T17:32:02Z",
    "readme_content": null,
    "ai_summary": "åŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œä¸“æ³¨äºç”Ÿæˆè‡ªç„¶æµç•…ã€è¶…é€¼çœŸçš„å¯¹è¯éŸ³é¢‘ï¼›åˆ©ç”¨å…ˆè¿›çš„å£°ç å™¨å’Œç¥ç»ç½‘ç»œæ¶æ„å®ç°é«˜è´¨é‡è¾“å‡ºã€‚",
    "ai_tech_stack": "[\"PyTorch\", \"Tacotron2\", \"WaveNet/Glow-TTS\", \"Mel-spectrogram\"]",
    "ai_use_cases": "[\"\\u865a\\u62df\\u52a9\\u624b\\u6216AI\\u5ba2\\u670d\\u7684\\u8bed\\u97f3\\u4ea4\\u4e92\", \"\\u6e38\\u620f\\u89d2\\u8272\\u5bf9\\u8bdd\\u751f\\u6210\\u4e0e\\u914d\\u97f3\", \"\\u65e0\\u969c\\u788d\\u9605\\u8bfb\\u5de5\\u5177\\uff08\\u4e3a\\u89c6\\u969c\\u7528\\u6237\\u63d0\\u4f9b\\u97f3\\u9891\\u5185\\u5bb9\\uff09\", \"\\u4e2a\\u6027\\u5316\\u6559\\u80b2\\u5e94\\u7528\\u4e2d\\u7684\\u8bfb\\u7269\\u6717\\u8bfb\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "python -m dia --input_text 'ä½ å¥½å—ï¼Ÿ' --output_file output.wav",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.497353",
    "last_analyzed": "2026-01-16T02:43:10.899758",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "928102730",
    "name": "index-tts",
    "full_name": "index-tts/index-tts",
    "category": "tts_voice",
    "stars": 17856,
    "forks": 2180,
    "description": "An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System",
    "url": "https://github.com/index-tts/index-tts",
    "homepage": "",
    "language": "Python",
    "topics": "[\"bigvgan\", \"cross-lingual\", \"indextts\", \"text-to-speech\", \"tts\", \"voice-clone\", \"zero-shot-tts\"]",
    "created_at": "2025-02-06T04:08:05Z",
    "updated_at": "2026-01-14T17:52:06Z",
    "readme_content": null,
    "ai_summary": "åŸºäºTransformeræ¨¡å‹çš„å·¥ä¸šçº§é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿï¼Œæä¾›é«˜è´¨é‡ã€å¯æ§ä¸”é«˜æ•ˆçš„è¯­éŸ³åˆæˆèƒ½åŠ›ï¼Œæ”¯æŒå¤šè¯­è¨€è¾“å‡º",
    "ai_tech_stack": "[\"Python\", \"PyTorch/TensorFlow\", \"\\u97f3\\u9891\\u5904\\u7406\\u5e93(SoundFile, NumPy\\u7b49)\", \"Web\\u670d\\u52a1(FastAPI/Flask)\"]",
    "ai_use_cases": "[\"\\u667a\\u80fd\\u5ba2\\u670d\\u7cfb\\u7edf\\u4e2d\\u7684\\u8bed\\u97f3\\u4ea4\\u4e92\\u529f\\u80fd\", \"\\u65e0\\u969c\\u788d\\u9605\\u8bfb\\u5e73\\u53f0\\u7684\\u6587\\u672c\\u6717\\u8bfb\\u670d\\u52a1\", \"\\u591a\\u8bed\\u8a00\\u5185\\u5bb9\\u521b\\u4f5c\\u5de5\\u5177\\uff08\\u5982\\u6709\\u58f0\\u4e66\\u751f\\u6210\\uff09\", \"\\u6e38\\u620fNPC\\u5bf9\\u8bdd\\u7cfb\\u7edf\"]",
    "ai_difficulty": 4,
    "ai_quick_start": "git clone https://github.com/index-tts/index-tts && cd index-tts && pip install -e . && python -m tts --text 'æµ‹è¯•æ–‡æœ¬' --output output.wav",
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.499397",
    "last_analyzed": "2026-01-16T02:47:19.207974",
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "540842713",
    "name": "buzz",
    "full_name": "chidiwilliams/buzz",
    "category": "tts_voice",
    "stars": 17070,
    "forks": 1267,
    "description": "Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.",
    "url": "https://github.com/chidiwilliams/buzz",
    "homepage": "https://chidiwilliams.github.io/buzz",
    "language": "Python",
    "topics": "[\"whisper\"]",
    "created_at": "2022-09-24T13:43:23Z",
    "updated_at": "2026-01-14T17:56:29Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.501744",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "746935181",
    "name": "ebook2audiobook",
    "full_name": "DrewThomasson/ebook2audiobook",
    "category": "tts_voice",
    "stars": 16954,
    "forks": 1370,
    "description": "Generate audiobooks from e-books, voice cloning & 1158+ languages!",
    "url": "https://github.com/DrewThomasson/ebook2audiobook",
    "homepage": "",
    "language": "Python",
    "topics": "[\"audiobook\", \"audiobooks\", \"chinese\", \"colab-notebook\", \"docker\", \"english\", \"epub\", \"gradio\", \"kaggle\", \"linux\", \"mac\", \"multilingual\", \"tts\", \"voice-cloning\", \"windows\", \"xtts\"]",
    "created_at": "2024-01-22T23:49:55Z",
    "updated_at": "2026-01-14T16:18:18Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.505402",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "452939314",
    "name": "tortoise-tts",
    "full_name": "neonbjb/tortoise-tts",
    "category": "tts_voice",
    "stars": 14766,
    "forks": 2051,
    "description": "A multi-voice TTS system trained with an emphasis on quality",
    "url": "https://github.com/neonbjb/tortoise-tts",
    "homepage": null,
    "language": "Jupyter Notebook",
    "topics": "[]",
    "created_at": "2022-01-28T04:33:15Z",
    "updated_at": "2026-01-14T13:15:41Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.508126",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "569959091",
    "name": "FunASR",
    "full_name": "modelscope/FunASR",
    "category": "tts_voice",
    "stars": 14474,
    "forks": 1512,
    "description": "A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.",
    "url": "https://github.com/modelscope/FunASR",
    "homepage": "https://www.funasr.com",
    "language": "Python",
    "topics": "[\"audio-visual-speech-recognition\", \"conformer\", \"dfsmn\", \"paraformer\", \"pretrained-model\", \"punctuation\", \"pytorch\", \"rnnt\", \"speaker-diarization\", \"speech-recognition\", \"speechgpt\", \"speechllm\", \"vad\", \"voice-activity-detection\", \"whisper\"]",
    "created_at": "2022-11-24T02:28:11Z",
    "updated_at": "2026-01-14T14:42:26Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.509736",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "869549554",
    "name": "F5-TTS",
    "full_name": "SWivid/F5-TTS",
    "category": "tts_voice",
    "stars": 13939,
    "forks": 2053,
    "description": "Official code for \"F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching\"",
    "url": "https://github.com/SWivid/F5-TTS",
    "homepage": "https://arxiv.org/abs/2410.06885",
    "language": "Python",
    "topics": "[]",
    "created_at": "2024-10-08T13:36:55Z",
    "updated_at": "2026-01-14T17:37:45Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.512552",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "110688239",
    "name": "PaddleSpeech",
    "full_name": "PaddlePaddle/PaddleSpeech",
    "category": "tts_voice",
    "stars": 12493,
    "forks": 1950,
    "description": "Easy-to-use Speech Toolkit including Self-Supervised Learning model, SOTA/Streaming ASR with punctuation, Streaming TTS with text frontend, Speaker Verification System, End-to-End Speech Translation and Keyword Spotting. Won NAACL2022 Best Demo Award.",
    "url": "https://github.com/PaddlePaddle/PaddleSpeech",
    "homepage": "https://paddlespeech.readthedocs.io",
    "language": "Python",
    "topics": "[\"asr\", \"code-switch\", \"conformer\", \"kws\", \"punctuation-restoration\", \"self-supervised-learning\", \"sound-classification\", \"speech-alignment\", \"speech-recognition\", \"speech-synthesis\", \"speech-translation\", \"streaming-asr\", \"streaming-tts\", \"transformer\", \"tts\", \"vocoder\", \"voice-cloning\", \"voice-recognition\", \"wav2vec2\", \"whisper\"]",
    "created_at": "2017-11-14T12:36:30Z",
    "updated_at": "2026-01-14T14:08:05Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.514583",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "938453471",
    "name": "Spark-TTS",
    "full_name": "SparkAudio/Spark-TTS",
    "category": "tts_voice",
    "stars": 10894,
    "forks": 1169,
    "description": "Spark-TTS Inference Code",
    "url": "https://github.com/SparkAudio/Spark-TTS",
    "homepage": null,
    "language": "Python",
    "topics": "[]",
    "created_at": "2025-02-25T01:29:36Z",
    "updated_at": "2026-01-14T15:35:37Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.516600",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "288482469",
    "name": "go-openai",
    "full_name": "sashabaranov/go-openai",
    "category": "tts_voice",
    "stars": 10504,
    "forks": 1691,
    "description": "OpenAI ChatGPT, GPT-5, GPT-Image-1, Whisper API clients for Go",
    "url": "https://github.com/sashabaranov/go-openai",
    "homepage": "",
    "language": "Go",
    "topics": "[\"chatgpt\", \"chatgpt-api\", \"dall-e\", \"go\", \"golang\", \"gpt-4\", \"gpt-5\", \"gpt-5-api\", \"openai\", \"openai-whisper\", \"streaming-api\"]",
    "created_at": "2020-08-18T14:42:54Z",
    "updated_at": "2026-01-14T11:40:37Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.519924",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "118620583",
    "name": "TTS",
    "full_name": "mozilla/TTS",
    "category": "tts_voice",
    "stars": 10097,
    "forks": 1326,
    "description": ":robot: :speech_balloon: Deep learning for Text to Speech  (Discussion forum: https://discourse.mozilla.org/c/tts)",
    "url": "https://github.com/mozilla/TTS",
    "homepage": "",
    "language": "Jupyter Notebook",
    "topics": "[\"dataset-analysis\", \"deep-learning\", \"gantts\", \"glow-tts\", \"melgan\", \"multiband-melgan\", \"python\", \"pytorch\", \"speaker-encoder\", \"speech\", \"tacotron\", \"tacotron2\", \"tensorflow2\", \"text-to-speech\", \"tts\", \"vocoder\"]",
    "created_at": "2018-01-23T14:22:06Z",
    "updated_at": "2026-01-13T17:50:48Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.522452",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "586310592",
    "name": "Whisper",
    "full_name": "Const-me/Whisper",
    "category": "tts_voice",
    "stars": 10063,
    "forks": 907,
    "description": "High-performance GPGPU inference of OpenAI's Whisper automatic speech recognition (ASR) model",
    "url": "https://github.com/Const-me/Whisper",
    "homepage": null,
    "language": "C++",
    "topics": "[]",
    "created_at": "2023-01-07T17:25:57Z",
    "updated_at": "2026-01-14T16:09:43Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.525689",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "251396732",
    "name": "RTranslator",
    "full_name": "niedev/RTranslator",
    "category": "tts_voice",
    "stars": 9539,
    "forks": 855,
    "description": "Open source real-time translation app for Android that runs locally",
    "url": "https://github.com/niedev/RTranslator",
    "homepage": "",
    "language": "C++",
    "topics": "[\"android\", \"android-app\", \"bluetooth-le\", \"mobile-app\", \"nllb\", \"offline\", \"onnx\", \"onnxruntime\", \"realtime-translator\", \"sentencepiece\", \"transformers\", \"translation\", \"translator\", \"whisper\"]",
    "created_at": "2020-03-30T18:40:55Z",
    "updated_at": "2026-01-14T11:29:34Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.528199",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "905697354",
    "name": "WhisperLiveKit",
    "full_name": "QuentinFuxa/WhisperLiveKit",
    "category": "tts_voice",
    "stars": 9492,
    "forks": 935,
    "description": "Simultaneous speech-to-text model",
    "url": "https://github.com/QuentinFuxa/WhisperLiveKit",
    "homepage": "",
    "language": "Python",
    "topics": "[]",
    "created_at": "2024-12-19T10:49:09Z",
    "updated_at": "2026-01-14T13:22:49Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.530206",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  },
  {
    "id": "1032248449",
    "name": "KittenTTS",
    "full_name": "KittenML/KittenTTS",
    "category": "tts_voice",
    "stars": 9453,
    "forks": 492,
    "description": " State-of-the-art TTS model under 25MB ğŸ˜» ",
    "url": "https://github.com/KittenML/KittenTTS",
    "homepage": "",
    "language": "Python",
    "topics": "[]",
    "created_at": "2025-08-05T03:30:24Z",
    "updated_at": "2026-01-14T13:26:18Z",
    "readme_content": null,
    "ai_summary": null,
    "ai_tech_stack": null,
    "ai_use_cases": null,
    "ai_difficulty": null,
    "ai_quick_start": null,
    "ai_tutorial": null,
    "last_scanned": "2026-01-15T02:01:30.532984",
    "last_analyzed": null,
    "screenshot": null,
    "ai_visual_summary": null,
    "ai_rag_summary": null
  }
]